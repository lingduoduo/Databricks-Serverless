{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc94837a-86f2-4ff8-b1bf-c40fbcbcd8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -q databricks-langchain langchain==0.3.7 faiss-cpu wikipedia langgraph==0.5.3  databricks_langchain transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c4878a-34a0-466d-8a77-34d802a685b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76216f23-819f-492c-806f-405888b6d542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build an AI Agent which can write code for a specified\n",
    "use case based on specified goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d719ec2d-68b3-4f97-b358-05f533ff3ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Optional, List\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool as langchain_tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "\n",
    "from databricks_langchain import ChatDatabricks \n",
    "from langgraph.graph import StateGraph, END  # pip install langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b61ab4-4a08-4cdd-8040-4b2a444bd77b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1) Environment / LLM\n",
    "# -----------------------\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\" # Model Serving endpoint name; other option see \"Serving\" under AI/ML tab (e.g. databricks-gpt-oss-20b)\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2ad5bd-3f9a-4ff9-afdc-72801840e2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Good lightweight defaults; change if you want a different model\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"   # try 0.5B/1.5B/3B depending on your cluster\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE == 0 else None,\n",
    ")\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde99a3e-060f-41a6-90f4-25bc19ea8f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def hf_chat(prompt: str, temperature: float = 0.3, max_new_tokens: int = 800) -> str:\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    except Exception:\n",
    "        chat_prompt = prompt\n",
    "\n",
    "    do_sample = temperature > 0\n",
    "\n",
    "    kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "\n",
    "    # Only pass sampling params if sampling is enabled\n",
    "    if do_sample:\n",
    "        kwargs.update(dict(\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "        ))\n",
    "\n",
    "    out = gen(chat_prompt, **kwargs)[0][\"generated_text\"]\n",
    "    return out.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a90ee6-7325-4a89-9e1f-4803c2269dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Utility Functions ----------\n",
    "def generate_prompt(use_case: str, goals: list[str], previous_code: str = \"\", feedback: str = \"\") -> str:\n",
    "    base_prompt = f\"\"\"\n",
    "You are an AI coding agent. Your job is to write Python code based on the following use case:\n",
    "\n",
    "Use Case: {use_case}\n",
    "\n",
    "Your goals are:\n",
    "{chr(10).join(f\"- {g.strip()}\" for g in goals)}\n",
    "\"\"\"\n",
    "    if previous_code:\n",
    "        base_prompt += f\"\\nPreviously generated code:\\n{previous_code}\\n\"\n",
    "    if feedback:\n",
    "        base_prompt += f\"\\nFeedback on previous version:\\n{feedback}\\n\"\n",
    "\n",
    "    base_prompt += \"\\nReturn ONLY valid Python code. No markdown fences. No explanations.\"\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8abb655-76c1-4971-9667-f1ae0649c507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_code_feedback(code: str, goals: list[str]) -> str:\n",
    "    feedback_prompt = f\"\"\"\n",
    "You are a Python code reviewer. A code snippet is shown below. Based on the following goals:\n",
    "\n",
    "{chr(10).join(f\"- {g.strip()}\" for g in goals)}\n",
    "\n",
    "Critique this code and identify if the goals are met. Mention improvements for clarity, simplicity,\n",
    "correctness, edge case handling, and test coverage (if requested by goals).\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\"\"\"\n",
    "    return hf_chat(feedback_prompt, temperature=0.2, max_new_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d471a3-3f56-4cbb-a80d-8389c858e409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def goals_met(feedback_text: str, goals: list[str]) -> bool:\n",
    "    review_prompt = f\"\"\"\n",
    "You are an AI reviewer.\n",
    "\n",
    "Goals:\n",
    "{chr(10).join(f\"- {g.strip()}\" for g in goals)}\n",
    "\n",
    "Feedback:\n",
    "\\\"\\\"\\\"\n",
    "{feedback_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Based on the feedback above, have the goals been met?\n",
    "\n",
    "Respond with only one word: True or False.\n",
    "\"\"\"\n",
    "    resp = hf_chat(review_prompt, temperature=0.0, max_new_tokens=5).strip().lower()\n",
    "    # be robust to extra tokens\n",
    "    return resp.startswith(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286fe17f-74f5-41b6-9d51-63c7fd097aab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_code_block(code: str) -> str:\n",
    "    # remove accidental markdown fences if the model outputs them\n",
    "    lines = code.strip().splitlines()\n",
    "    if lines and lines[0].strip().startswith(\"```\"):\n",
    "        lines = lines[1:]\n",
    "    if lines and lines[-1].strip().startswith(\"```\"):\n",
    "        lines = lines[:-1]\n",
    "    return \"\\n\".join(lines).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90dd760-9d6f-4919-8f8c-f96c711e788c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_comment_header(code: str, use_case: str) -> str:\n",
    "    comment = f\"# This Python program implements the following use case:\\n# {use_case.strip()}\\n\"\n",
    "    return comment + \"\\n\" + code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1351232-8f0a-4d11-ada2-8621a66aded5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "\n",
    "def save_code_local(code: str, use_case: str) -> str:\n",
    "    base_dir = Path(\"/tmp/mcp_code_agent\")\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    short = re.sub(r\"[^a-zA-Z0-9_]\", \"\", use_case.lower().replace(\" \", \"_\"))[:10]\n",
    "    short = short or \"code\"\n",
    "    filename = f\"{short}_{random.randint(1000,9999)}.py\"\n",
    "\n",
    "    path = base_dir / filename\n",
    "    path.write_text(code, encoding=\"utf-8\")\n",
    "\n",
    "    return str(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ce7372-360a-4b60-a31d-adcfe28fc028",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Main Agent ----------\n",
    "def run_code_agent(use_case: str, goals_input: str, max_iterations: int = 5) -> str:\n",
    "    goals = [g.strip() for g in goals_input.split(\",\") if g.strip()]\n",
    "\n",
    "    previous_code = \"\"\n",
    "    feedback_text = \"\"\n",
    "    code = \"\"\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        prompt = generate_prompt(use_case, goals, previous_code, feedback_text)\n",
    "        raw_code = hf_chat(prompt, temperature=0.3, max_new_tokens=900)\n",
    "        code = clean_code_block(raw_code)\n",
    "\n",
    "        feedback_text = get_code_feedback(code, goals)\n",
    "\n",
    "        if goals_met(feedback_text, goals):\n",
    "            break\n",
    "\n",
    "        previous_code = code\n",
    "\n",
    "    final_code = add_comment_header(code, use_case)\n",
    "    out_path = save_code_local(final_code, use_case)\n",
    "    print(\"Saved to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd9019b-7796-4624-b44f-0e020868f8c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "use_case_input = \"Write code to find BinaryGap of a given positive integer\"\n",
    "goals_input = \"Code simple to understand, Functionally correct, Handles comprehensive edge cases, Takes positive integer input only, prints the results with few examples\"\n",
    "\n",
    "out_path = run_code_agent(use_case_input, goals_input, max_iterations=5)\n",
    "print(\"Saved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c871168f-bb41-4721-9239-383d8a3152d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"/tmp/mcp_code_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a56a81-7490-4c49-ba46-e57bd7a9206b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/tmp/mcp_code_agent/write_code_7058.py\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d370613-488d-43ad-9ba1-6b69a0b66887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_goal_setting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
