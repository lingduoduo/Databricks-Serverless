{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c34d04d-11a4-40fc-b7d8-b6904cf6e1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U databricks-langchain==0.12.1 langchain==1.2.0 langchain-community==0.4.1 langchain-openai==1.1.6 faiss-cpu sentence-transformers langchain-classic rank_bm25 transformers scikit-learn spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c4878a-34a0-466d-8a77-34d802a685b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7057b129-723a-4dea-b3b6-d452091ceff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd59dec6-63bd-4173-8dfa-39d1e95d5c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Foundation Terminology\n",
    "\n",
    "Foundation\tTerminology glossary construction, term extraction, preprocessing standardization, term embeddings and vector indexing\n",
    "\n",
    "- NER, TF-IDF, KeyBERT\n",
    "- Term Normalization\n",
    "- Text Standardization\n",
    "- Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc94837a-86f2-4ff8-b1bf-c40fbcbcd8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define a terminology glossary for RAG variants (keep it updated, including context tags)\n",
    "GLOSSARY = [\n",
    "    {\n",
    "    \"term\": \"Retrieval-Augmented Generation\",\n",
    "    \"synonyms\": [\"RAG\", \"retrieval augmented generation\", \"retrieval-augmented generation\"],\n",
    "    \"definition\": \"A generation framework that retrieves external evidence and conditions an LLM on it.\",\n",
    "    \"context_tags\": [\"LLM\", \"search\", \"grounding\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Self-RAG\",\n",
    "        \"synonyms\": [\"Self RAG\", \"self-rag\", \"self-reflective RAG\", \"self-refining RAG\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach where the model explicitly self-checks (e.g., reflect, critique, verify) during generation, \"\n",
    "            \"deciding when to retrieve more evidence and how to revise its answer based on feedback signals.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"reflection\", \"self-critique\", \"verification\", \"iterative retrieval\", \"hallucination mitigation\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Corrective RAG\",\n",
    "        \"synonyms\": [\"C-RAG\", \"CRAG\", \"Corrective-RAG\", \"corrective rag\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach that detects low-quality retrieval or unsupported generations and applies corrective actions \"\n",
    "            \"(e.g., re-retrieve, rewrite queries, filter evidence, or re-rank) to improve factual grounding.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"retrieval quality\", \"re-ranking\", \"query rewriting\", \"evidence filtering\", \"robustness\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Knowledge Graph RAG\",\n",
    "        \"synonyms\": [\"KG-RAG\", \"KGRAG\", \"KG RAG\", \"knowledge-graph RAG\", \"graph RAG\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach that retrieves and reasons over a knowledge graph (entities/relations) as structured evidence, \"\n",
    "            \"often combining graph traversal with text retrieval to support multi-hop and relational questions.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"knowledge graph\", \"multi-hop reasoning\", \"entity linking\", \"graph traversal\", \"structured grounding\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Entity Linking\",\n",
    "        \"synonyms\": [\"EL\", \"entity resolution\", \"mention linking\", \"entity disambiguation\"],\n",
    "        \"definition\": (\n",
    "            \"The process of mapping a text mention (e.g., 'Apple') to a canonical entity (e.g., Apple Inc.) in a KB/KG \"\n",
    "            \"to support structured retrieval and reduce ambiguity.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"KG-RAG\", \"disambiguation\", \"knowledge base\", \"information extraction\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Evidence Grounding\",\n",
    "        \"synonyms\": [\"grounding\", \"source grounding\", \"evidence-based generation\"],\n",
    "        \"definition\": (\n",
    "            \"Constraining or evaluating generation based on retrieved evidence so claims are supported by sources; \"\n",
    "            \"commonly paired with citation, attribution, or entailment checks.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"factuality\", \"citations\", \"attribution\", \"hallucination mitigation\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1e02a5-3aaa-4574-aa36-cda5a7526de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class TerminologyProcessor:\n",
    "    def __init__(self, glossary: List[Dict[str, Any]]):\n",
    "        self.glossary = glossary\n",
    "        self.standard_term_map = {}\n",
    "        self.alias_to_entries_map = {}\n",
    "        self._build_mappings()\n",
    "\n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Build mappings; one alias may map to multiple terminology entries to handle ambiguity.\"\"\"\n",
    "        for entry in self.glossary:\n",
    "            standard_term = entry[\"term\"]\n",
    "            self.standard_term_map[standard_term.lower()] = standard_term\n",
    "\n",
    "            all_aliases = [standard_term] + entry.get(\"synonyms\", [])\n",
    "            for alias in all_aliases:\n",
    "                alias_lower = alias.lower()\n",
    "                if alias_lower not in self.alias_to_entries_map:\n",
    "                    self.alias_to_entries_map[alias_lower] = []\n",
    "                if entry not in self.alias_to_entries_map[alias_lower]:\n",
    "                    self.alias_to_entries_map[alias_lower].append(entry)\n",
    "\n",
    "    def standardize_text(self, text: str, context_window: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Context-aware terminology standardization using iteration + a replacement function.\n",
    "        Dynamically generates the correct regex for each term type.\n",
    "        \"\"\"\n",
    "        standardized_text = text\n",
    "        sorted_keys = sorted(self.alias_to_entries_map.keys(), key=len, reverse=True)\n",
    "\n",
    "        for key_lower in sorted_keys:\n",
    "            possible_entries = self.alias_to_entries_map[key_lower]\n",
    "\n",
    "            # --- Dynamically create the correct regex for each key ---\n",
    "            pattern_str = \"\"\n",
    "            # If key contains Latin letters, assume it's an abbreviation and enforce boundaries\n",
    "            if re.search(r\"[a-zA-Z]\", key_lower):\n",
    "                # Use lookarounds to avoid matching inside a larger word\n",
    "                pattern_str = r\"(?<![a-zA-Z])\" + re.escape(key_lower) + r\"(?![a-zA-Z])\"\n",
    "            else:\n",
    "                # For Chinese (or non-Latin) terms, match exactly\n",
    "                pattern_str = re.escape(key_lower)\n",
    "\n",
    "            pattern = re.compile(pattern_str, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replacement function called for each match\n",
    "            def replacer(match: re.Match) -> str:\n",
    "                if len(possible_entries) == 1:\n",
    "                    return possible_entries[0][\"term\"]\n",
    "                else:\n",
    "                    # --- Context-based disambiguation ---\n",
    "                    context_snippet = standardized_text[\n",
    "                        max(0, match.start() - context_window) : min(len(standardized_text), match.end() + context_window)\n",
    "                    ]\n",
    "                    for entry in possible_entries:\n",
    "                        clues = entry.get(\"context_tags\", []) + [entry[\"term\"]]\n",
    "                        if any(clue in context_snippet for clue in clues):\n",
    "                            return entry[\"term\"]\n",
    "                    # If no context clue is found, fall back to the first definition\n",
    "                    return possible_entries[0][\"term\"]\n",
    "\n",
    "            # Update text using the replacement function\n",
    "            standardized_text = pattern.sub(replacer, standardized_text)\n",
    "\n",
    "        return standardized_text\n",
    "\n",
    "    def extract_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract known standardized terms from text\n",
    "        \"\"\"\n",
    "        found_terms = set()\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for standard_term_lower, original_standard_term in self.standard_term_map.items():\n",
    "            # Direct substring search; do not use \\b\n",
    "            if re.search(re.escape(standard_term_lower), text_lower):\n",
    "                found_terms.add(original_standard_term)\n",
    "\n",
    "        return sorted(list(found_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536b7fdd-1949-44f0-96f5-84ab573841de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Initialize the terminology processor with the glossary.\n",
    "term_processor = TerminologyProcessor(GLOSSARY)\n",
    "print(\"--- 1. Initialize the terminology processor with the glossary ---\")\n",
    "\n",
    "# 2. Data preprocessing: terminology standardization\n",
    "print(\"--- 2. Data Preprocessing: Terminology Standardization ---\")\n",
    "user_query = \"I want to learn about applications of using RAG.\"\n",
    "processed_query = term_processor.standardize_text(user_query)\n",
    "print(f\"Original query: {user_query}\")\n",
    "print(f\"Standardized query: {processed_query}\")\n",
    "\n",
    "document_text = \"Recently I studied Self-RAG.\"\n",
    "processed_document = term_processor.standardize_text(document_text)\n",
    "print(f\"Original document: {document_text}\")\n",
    "print(f\"Standardized document: {processed_document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90cb5524-aa4f-4f61-a72e-86354d8b23be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Term extraction (for downstream vectorization or metadata tagging)\n",
    "print(\"\\n--- 3. Term Extraction ---\")\n",
    "extracted_terms_query = term_processor.extract_terms(processed_query)\n",
    "print(f\"Extracted terms from query: {extracted_terms_query}\")\n",
    "\n",
    "extracted_terms_document = term_processor.extract_terms(processed_document)\n",
    "print(f\"Extracted terms from document: {extracted_terms_document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb383f00-6742-44f8-8e86-93aa29176eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Simulated vector storage and retrieval augmentation (conceptual)\n",
    "print(\"\\n--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\")\n",
    "print(\"In a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\")\n",
    "print(\"These vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\")\n",
    "print(\"During retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acedac0-bd4b-4e04-9ed3-f98dafde0487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Simulated retrieval augmentation: query expansion\n",
    "def enhance_query_for_retrieval(query: str, processor: TerminologyProcessor) -> List[str]:\n",
    "    \"\"\"Expand query keywords using the terminology glossary to improve recall.\"\"\"\n",
    "    standardized_query = processor.standardize_text(query)\n",
    "    query_terms = processor.extract_terms(standardized_query)\n",
    "\n",
    "    expanded_keywords = set([standardized_query])\n",
    "    for term in query_terms:\n",
    "        expanded_keywords.add(term)\n",
    "        for entry in processor.glossary:\n",
    "            if entry[\"term\"] == term:\n",
    "                for synonym in entry.get(\"synonyms\", []):\n",
    "                    expanded_keywords.add(synonym)\n",
    "                break\n",
    "    return sorted(list(expanded_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fcd4d7d-5999-4454-aeb4-77fe3481ff18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 5. Simulated Retrieval Augmentation: Query Expansion ---\")\n",
    "original_query_for_retrieval = \"I want to know what a RAG does in an LLM?\"\n",
    "expanded_keywords = enhance_query_for_retrieval(original_query_for_retrieval, term_processor)\n",
    "print(f\"Original retrieval query: {original_query_for_retrieval}\")\n",
    "print(f\"Expanded retrieval keyword list: {expanded_keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d720cb0e-c3e8-4561-979b-3b34216da435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Detect Synonyms to Extend Terminology Glossary\n",
    "\n",
    "Detect Synonyms by Similarity\n",
    "\n",
    "- FAISS\n",
    "- Legal-BERT, ChatLaw-Text2Vec\n",
    "- Sentence Transformers + PEFT (LoRA) Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3845aba8-c509-43a7-9dc1-7f15aaf08c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "DASHES = r\"[-\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212]\"  # common dash chars\n",
    "\n",
    "def alias_to_pattern(alias: str):\n",
    "    # If alias contains a dash, match it as its own token with regex\n",
    "    if re.search(DASHES, alias):\n",
    "        # split on dash and keep the two sides\n",
    "        left, right = re.split(DASHES, alias, maxsplit=1)\n",
    "        left_tokens = left.strip().split()\n",
    "        right_tokens = right.strip().split()\n",
    "        return [{\"LOWER\": t.lower()} for t in left_tokens] + [{\"TEXT\": {\"REGEX\": DASHES}}] + [{\"LOWER\": t.lower()} for t in right_tokens]\n",
    "    else:\n",
    "        # normal phrase: token-by-token, case-insensitive\n",
    "        return [{\"LOWER\": t.lower()} for t in alias.strip().split()]\n",
    "\n",
    "def extract_terms_with_ruler(text, glossary):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # avoid adding duplicate pipes if you call this multiple times\n",
    "    if \"entity_ruler\" in nlp.pipe_names:\n",
    "        ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "        ruler.clear()\n",
    "    else:\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True})\n",
    "\n",
    "    patterns = []\n",
    "    for entry in glossary:\n",
    "        for alias in [entry.get(\"term\")] + entry.get(\"synonyms\", []):\n",
    "            if not alias:\n",
    "                continue\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": alias_to_pattern(alias)})\n",
    "\n",
    "    # prefer longer patterns first (helps with overlaps like KG-RAG vs RAG)\n",
    "    patterns.sort(key=lambda p: len(p[\"pattern\"]), reverse=True)\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ == \"TERM\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec190888-03e9-4a42-b658-1cb73d695f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example \n",
    "query = \"Examples of KG-RAG\" \n",
    "processed_query = term_processor.standardize_text(query)\n",
    "print(processed_query)\n",
    "# processed_query = \"Examples of Knowledge Graph Retrieval-Augmented Generation\"\n",
    "candidates = extract_terms_with_ruler(processed_query, GLOSSARY)\n",
    "print(f\"Automatically extracted term candidates: {candidates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76216f23-819f-492c-806f-405888b6d542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Term Glossary Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c0903a-4f79-43cd-910c-af9fbcff75e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# It is recommended to load the model once during project initialization\n",
    "# to avoid repeated loading overhead.\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def map_synonyms_by_similarity(main_terms: list, candidates: list, threshold: float = 0.8) -> dict:\n",
    "    \"\"\"\n",
    "    Map candidate terms to the closest standard terms by computing\n",
    "    cosine similarity between embeddings.\n",
    "\n",
    "    Args:\n",
    "        main_terms (list): List of standard (canonical) terms.\n",
    "        candidates (list): List of candidate synonyms to be matched.\n",
    "        threshold (float): Similarity threshold above which a candidate\n",
    "                           is considered a synonym.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each standard term to a list of\n",
    "              successfully matched synonyms.\n",
    "    \"\"\"\n",
    "    _matched_synonyms = {term: [] for term in main_terms}\n",
    "\n",
    "    if not main_terms or not candidates:\n",
    "        return _matched_synonyms\n",
    "    \n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode in batches for better efficiency\n",
    "    embeddings = model.encode(main_terms + candidates, convert_to_tensor=True)\n",
    "    term_embeddings = embeddings[:len(main_terms)]\n",
    "    candidate_embeddings = embeddings[len(main_terms):]\n",
    "\n",
    "    # Compute the cosine similarity matrix between standard terms and candidates\n",
    "    similarity_matrix = util.cos_sim(term_embeddings, candidate_embeddings)\n",
    "\n",
    "    for i, term in enumerate(main_terms):\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                _matched_synonyms[term].append(candidate)\n",
    "\n",
    "    return _matched_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d2ce6b-14ff-4cc3-b278-75e8ad475a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_terms_to_map = [\"RAG\"]\n",
    "all_possible_synonyms = [\n",
    "    entry[\"term\"]\n",
    "    for entry in GLOSSARY\n",
    "] + [\n",
    "    synonym\n",
    "    for entry in GLOSSARY\n",
    "    for synonym in entry.get(\"synonyms\", [])\n",
    "]\n",
    "optimized_mapped_synonyms = map_synonyms_by_similarity(\n",
    "    main_terms_to_map,\n",
    "    all_possible_synonyms\n",
    ")\n",
    "print(\"\\nOptimized matched synonyms:\", optimized_mapped_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c4bc8a-3433-4b03-973a-e701fe3b7de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "print(f\"\\nAttempting to load model '{model_name}'...\")\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988f01b0-9ae4-411f-ab42-ceaa81172673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "\n",
    "def build_term_vector_index(\n",
    "    term_glossary: Dict[str, dict],\n",
    "    model: SentenceTransformer,\n",
    "    use_cosine: bool = True\n",
    ") -> Tuple[faiss.Index, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert all terms and their synonyms into vector embeddings and build a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        term_glossary (dict):\n",
    "            A structured glossary where keys are canonical terms and values contain\n",
    "            a 'synonyms' list.\n",
    "        model (SentenceTransformer):\n",
    "            A loaded SentenceTransformer model.\n",
    "        use_cosine (bool):\n",
    "            Whether to use cosine similarity (recommended for sentence embeddings).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            (faiss_index, indexed_terms)\n",
    "            - faiss_index: FAISS index containing all embeddings\n",
    "            - indexed_terms: list of terms aligned with index rows\n",
    "    \"\"\"\n",
    "    terms_to_index: List[str] = []\n",
    "\n",
    "    # Collect canonical terms and synonyms\n",
    "    for canonical_term, info in term_glossary.items():\n",
    "        terms_to_index.append(canonical_term)\n",
    "        synonyms = info.get(\"synonyms\", [])\n",
    "        if isinstance(synonyms, list):\n",
    "            terms_to_index.extend(synonyms)\n",
    "\n",
    "    # Deduplicate while keeping deterministic order\n",
    "    indexed_terms = sorted(set(terms_to_index))\n",
    "    if not indexed_terms:\n",
    "        raise ValueError(\"No terms found in glossary.\")\n",
    "\n",
    "    print(\"Generating term embeddings...\")\n",
    "    embeddings = model.encode(\n",
    "        indexed_terms,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine,\n",
    "        show_progress_bar=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    # Choose FAISS index type\n",
    "    if use_cosine:\n",
    "        # Cosine similarity = inner product on normalized vectors\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    metric = \"cosine similarity\" if use_cosine else \"L2 distance\"\n",
    "    print(f\"FAISS index built successfully. \"\n",
    "          f\"Vectors: {index.ntotal}, Dimension: {dim}, Metric: {metric}\")\n",
    "\n",
    "    return index, indexed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998f2c82-af56-447d-a842-203d4df24253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def convert_glossary_to_term_dict(\n",
    "    glossary: List[Dict[str, Any]],\n",
    "    deduplicate: bool = True,\n",
    "    keep_term_as_synonym: bool = False\n",
    ") -> Dict[str, Dict[str, list]]:\n",
    "    \"\"\"\n",
    "    Convert list-based glossary into FAISS-friendly dict format.\n",
    "\n",
    "    Args:\n",
    "        glossary: original GLOSSARY list\n",
    "        deduplicate: remove duplicate synonyms\n",
    "        keep_term_as_synonym: include canonical term in synonyms or not\n",
    "\n",
    "    Returns:\n",
    "        dict: {canonical_term: {\"synonyms\": [...]}}\n",
    "    \"\"\"\n",
    "    term_dict = {}\n",
    "\n",
    "    for entry in glossary:\n",
    "        term = entry.get(\"term\")\n",
    "        if not term:\n",
    "            continue\n",
    "\n",
    "        synonyms = entry.get(\"synonyms\", [])\n",
    "        if not isinstance(synonyms, list):\n",
    "            synonyms = []\n",
    "\n",
    "        if keep_term_as_synonym:\n",
    "            synonyms = [term] + synonyms\n",
    "\n",
    "        if deduplicate:\n",
    "            # preserve order, remove duplicates\n",
    "            seen = set()\n",
    "            synonyms = [s for s in synonyms if not (s in seen or seen.add(s))]\n",
    "\n",
    "        term_dict[term] = {\"synonyms\": synonyms}\n",
    "\n",
    "    return term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2e6c74-2602-49c3-800d-75623061283c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "term_glossary = convert_glossary_to_term_dict(GLOSSARY)\n",
    "print(term_glossary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a447ede-dd84-4e09-9f3f-aedad8fe577e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "index, indexed_terms = build_term_vector_index(\n",
    "    term_glossary=term_glossary,\n",
    "    model=model,\n",
    "    use_cosine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e6ec73-8944-44fb-ba7b-6737d9d052f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def search_terms(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    indexed_terms: List[str],\n",
    "    top_k: int = 5,\n",
    "    use_cosine: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the most similar terms to a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query text.\n",
    "        model (SentenceTransformer): SentenceTransformer model.\n",
    "        index (faiss.Index): FAISS index.\n",
    "        indexed_terms (list): Terms aligned with index rows.\n",
    "        top_k (int): Number of results to return.\n",
    "        use_cosine (bool): Whether cosine similarity is used.\n",
    "\n",
    "    Returns:\n",
    "        list of (term, score) tuples.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        results.append((indexed_terms[idx], float(score)))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f8772c-e834-432b-8902-b23c13e6e864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Index Build Successful ---\")\n",
    "print(\"Number of vectors in FAISS index:\", index.ntotal)\n",
    "print(\"Indexed terms:\", indexed_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a65a4ab-3299-40ee-ae10-0d3ac861ec7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Part 2: Define our core retrieval function ---\n",
    "\n",
    "def search_similar_terms(\n",
    "    query_text: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    term_list: list,\n",
    "    k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar terms to a query text from a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The user input query term/text.\n",
    "        model (SentenceTransformer): The embedding model used to encode the query.\n",
    "        index (faiss.Index): The FAISS index object.\n",
    "        term_list (list): The term list aligned with the order of vectors in the FAISS index.\n",
    "        k (int): The number of most similar results to return.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Retrieval ---\")\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "\n",
    "    # 1) Encode the query text into an embedding vector\n",
    "    query_vector = model.encode([query_text])\n",
    "    query_vector = query_vector.astype(\"float32\")\n",
    "\n",
    "    # 2) Search in the FAISS index\n",
    "    # index.search returns two arrays: D (distances/scores) and I (indices)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    # 3) Parse and print results\n",
    "    print(\"Results:\")\n",
    "    for i in range(k):\n",
    "        idx = int(indices[0][i])\n",
    "        dist = float(distances[0][i])\n",
    "        term = term_list[idx]\n",
    "\n",
    "        # For IndexFlatL2, distance is squared Euclidean distance:\n",
    "        # smaller distance => more similar\n",
    "        print(f\"  Top {i+1}: term='{term}', distance={dist:.4f} (smaller = more similar)\")\n",
    "\n",
    "\n",
    "# 4) === Demo ===\n",
    "\n",
    "# Case 1: Query using a synonym for the canonical term\n",
    "search_similar_terms(query_text=\"RAG\", model=model, index=index, term_list=indexed_terms, k=3)\n",
    "\n",
    "# Case 2: Semantically similar query (core advantage)\n",
    "search_similar_terms(query_text=\"Vector Store\", model=model, index=index, term_list=indexed_terms, k=3)\n",
    "\n",
    "# Case 3: Query with a broader term\n",
    "# Goal: Query \"Language Model\" and see whether it matches more specific related terms (if present).\n",
    "search_similar_terms(query_text=\"Language Model\", model=model, index=index, term_list=indexed_terms, k=3)\n",
    "\n",
    "# Case 4: Tolerance to minor noise / paraphrases\n",
    "# Goal: Query \"Transformer model\" (a paraphrase) and see if it matches \"Transformer\" / \"transformer\".\n",
    "search_similar_terms(query_text=\"Transformer model\", model=model, index=index, term_list=indexed_terms, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15192026-6cc3-4adb-89dd-53f5cc22c391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enhancement Query and Answer based on Glossary\n",
    "\n",
    "Hybrid retrieval (BM25 + vectors), query expansion (MultiQuery), hypothetical document embeddings (HyDE), cross-encoder re-ranking\n",
    "\n",
    "- Multi-Query Retriever\n",
    "- HyDE\n",
    "- Hybrid Search\n",
    "- BGE-Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5faab0a-9555-4447-ba0c-4915cd605620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Query Expansion and Rewriting\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51b6605-986a-46ae-9a33-fb3b086354c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. Prepare sample documents\n",
    "# We create some example text containing technical terminology\n",
    "doc_text = \"\"\"\n",
    "Convolutional Neural Networks (CNNs) are a key model in deep learning,\n",
    "especially effective in the field of image recognition.\n",
    "Their core idea is to automatically extract local image features\n",
    "through convolutional layers and pooling layers.\n",
    "\n",
    "Unlike CNNs, Transformer models were originally applied to\n",
    "natural language processing (NLP) tasks such as machine translation.\n",
    "Today, they have also been successfully applied to computer vision,\n",
    "known as Vision Transformers.\n",
    "\n",
    "Large Language Models (LLMs) are a major focus of current AI research.\n",
    "Based on the Transformer architecture, they are capable of\n",
    "understanding and generating human-like text,\n",
    "demonstrating strong reasoning capabilities.\n",
    "\"\"\".strip()\n",
    "\n",
    "documents = [Document(page_content=doc_text, metadata={\"source\": \"sample_tech_doc\"})]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b3057b-3ea9-4df6-a29c-a28c20b1f7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks, DatabricksEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL = \"databricks-bge-large-en\"\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_MODEL)\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41083eac-a5dc-4cec-a1dd-708c5bb68257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.multi_query import MultiQueryRetriever\n",
    "from databricks_langchain import ChatDatabricks, DatabricksEmbeddings\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.2)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436baa00-2b96-45d4-bf1b-86bf7b562fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is a RAG?\"\n",
    "retrieved_docs = retriever_from_llm.invoke(query)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dbf9d32-0860-40a8-a503-43c499cae8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hybrid retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4bafa7-b69f-4e07-b622-c3ade280f102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 3\n",
    "print(\"BM25 retriever built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1c9025-0da9-4346-b003-c9eb55c82319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nInitializing MergerRetriever...\")\n",
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever_list = [bm25_retriever, faiss_retriever]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ede895-57c3-42f8-9579-04e736b174ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import MergerRetriever\n",
    "# MergerRetriever handles parallel retrieval and deduplication\n",
    "merged_retriever = MergerRetriever(retrievers=retriever_list)\n",
    "print(\"MergerRetriever initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49aa55ac-738b-4218-bb05-5458dc558483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nInitializing MergerRetriever...\")\n",
    "retriever_list = [bm25_retriever, faiss_retriever]\n",
    "# MergerRetriever handles parallel retrieval and deduplication\n",
    "merged_retriever = MergerRetriever(retrievers=retriever_list)\n",
    "print(\"MergerRetriever initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c128bd-633a-4888-9004-75118a02926e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. Run a query and compare results ---\n",
    "query = \"Self-RAG\"\n",
    "print(f\"\\n\\n--- Running Hybrid Retrieval ---\")\n",
    "print(f\"Query: '{query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687bcaf5-02bc-4024-8047-f7f5d96ca848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For comparison, inspect each retrieverâ€™s results individually first\n",
    "print(\"\\n--- Individual Retriever Results ---\")\n",
    "\n",
    "bm25_results = bm25_retriever.invoke(query)\n",
    "print(f\"[BM25 Keyword Retrieval Results] (total {len(bm25_results)}):\")\n",
    "for doc in bm25_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "faiss_results = faiss_retriever.invoke(query)\n",
    "print(f\"\\n[FAISS Vector Retrieval Results] (total {len(faiss_results)}):\")\n",
    "for doc in faiss_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b849a97-9dbc-4511-b347-41a83f424b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now inspect the merged (hybrid) results\n",
    "print(\"\\n--- MergerRetriever Hybrid Results ---\")\n",
    "merged_results = merged_retriever.invoke(query)\n",
    "print(f\"[Final Hybrid Results] (total {len(merged_results)}, deduplicated):\")\n",
    "for doc in merged_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "845fb0e4-ec85-4f86-9297-9f03a028903b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Response Generation and Evaluation\n",
    "\n",
    "- Prompt Engineering\n",
    "- Structured Output\n",
    "- Output Parser\n",
    "- Post-processing \n",
    "- LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37b5a3c-dc39-4255-bc8b-effe10f2cdf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "\n",
    "# --- 1) Preparation ---\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.2)\n",
    "\n",
    "\n",
    "# --- 2) Define the expected output structure ---\n",
    "class TerminologyInAnswer(BaseModel):\n",
    "    \"\"\"A structured model containing the main answer and the technical terms used.\"\"\"\n",
    "    answer: str = Field(description=\"A detailed and accurate answer to the user's question.\")\n",
    "    standard_terms_used: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Standard technical terms from the official glossary explicitly used in the answer.\",\n",
    "        examples=[[\"RAG\", \"Self-RAG\"]],\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 3) Structured output chain (parameterized) ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI expert with deep technical knowledge. Provide a structured answer.\"),\n",
    "    (\"human\", \"Please explain: {question}\")\n",
    "])\n",
    "\n",
    "structured_llm_chain = prompt | llm.with_structured_output(TerminologyInAnswer)\n",
    "\n",
    "\n",
    "def glossary_list_to_def_dict(glossary_list: List[Dict[str, Any]], include_synonyms: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert list-style glossary into dict: {term_or_synonym: definition}.\n",
    "    Skips empty terms and empty definitions.\n",
    "    \"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for entry in glossary_list or []:\n",
    "        term = (entry.get(\"term\") or \"\").strip()\n",
    "        definition = (entry.get(\"definition\") or \"\").strip()\n",
    "        if term and definition:\n",
    "            out[term] = definition\n",
    "\n",
    "        if include_synonyms and definition:\n",
    "            for syn in entry.get(\"synonyms\", []) or []:\n",
    "                syn = (syn or \"\").strip()\n",
    "                if syn and syn not in out:\n",
    "                    out[syn] = definition\n",
    "    return out\n",
    "\n",
    "\n",
    "class TermEnhancer:\n",
    "    \"\"\"\n",
    "    Efficiently wraps known terms with <abbr title=\"...\">term</abbr> in one pass.\n",
    "    - Single compiled regex for all terms\n",
    "    - Avoids touching text inside HTML tags\n",
    "    - Avoids re-wrapping if text already contains <abbr ...>term</abbr> (best-effort)\n",
    "    \"\"\"\n",
    "    def __init__(self, term_defs: Dict[str, str], *, case_sensitive: bool = True):\n",
    "        self.term_defs = {k: v for k, v in (term_defs or {}).items() if k and v}\n",
    "        if not self.term_defs:\n",
    "            self._regex = None\n",
    "            return\n",
    "\n",
    "        # Prefer longer matches first so \"KG-RAG\" beats \"RAG\" when both exist.\n",
    "        terms_sorted = sorted(self.term_defs.keys(), key=len, reverse=True)\n",
    "\n",
    "        # Build a single alternation regex.\n",
    "        # NOTE: if you want strict word boundaries, you can wrap each term with \\b,\n",
    "        # but that breaks on hyphenated terms (Self-RAG). So we keep it flexible.\n",
    "        escaped = [re.escape(t) for t in terms_sorted]\n",
    "        flags = 0 if case_sensitive else re.IGNORECASE\n",
    "        self._regex = re.compile(\"|\".join(escaped), flags=flags)\n",
    "\n",
    "    def enhance(self, text: str) -> str:\n",
    "        if not text or not self._regex:\n",
    "            return text\n",
    "\n",
    "        # Split on HTML tags; only substitute in non-tag parts.\n",
    "        # This prevents replacing inside attributes like title=\"...\".\n",
    "        parts = re.split(r\"(<[^>]+>)\", text)\n",
    "\n",
    "        def repl(match: re.Match) -> str:\n",
    "            term = match.group(0)\n",
    "            definition = self.term_defs.get(term)\n",
    "            # If case-insensitive mode is desired, you can map via a normalized dict instead.\n",
    "            if not definition:\n",
    "                return term\n",
    "            safe_def = html.escape(definition, quote=True)\n",
    "            return f'<abbr title=\"{safe_def}\">{term}</abbr>'\n",
    "\n",
    "        for i in range(0, len(parts), 2):  # even indices are outside tags\n",
    "            # Best-effort: avoid wrapping inside existing <abbr>...</abbr>\n",
    "            # If your text can contain nested HTML, you may want a real HTML parser.\n",
    "            parts[i] = self._regex.sub(repl, parts[i])\n",
    "\n",
    "        return \"\".join(parts)\n",
    "\n",
    "\n",
    "# --- 4) Execute ---\n",
    "question = \"what a self-RAG is\"\n",
    "structured_response: TerminologyInAnswer = structured_llm_chain.invoke({\"question\": question})\n",
    "\n",
    "print(\"--- Structured object returned by the LLM ---\")\n",
    "print(structured_response)\n",
    "print(\"\\nAnswer content:\", structured_response.answer)\n",
    "print(\"Standard terms:\", structured_response.standard_terms_used)\n",
    "\n",
    "# --- 5) Enhance output with glossary definitions ---\n",
    "term_to_definition = glossary_list_to_def_dict(GLOSSARY, include_synonyms=True)\n",
    "enhancer = TermEnhancer(term_to_definition, case_sensitive=True)\n",
    "\n",
    "final_output = enhancer.enhance(structured_response.answer)\n",
    "\n",
    "print(\"\\n--- Final enhanced output ---\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca1afca-dcdc-49e9-bb46-845d35a3eb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def enhance_text_with_definitions(text: str, term_glossary: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Wrap glossary terms with <abbr title=\"...\">term</abbr> in a single pass.\n",
    "\n",
    "    Improvements over naive .replace loop:\n",
    "    - One compiled regex (faster than O(#terms * text_len))\n",
    "    - Longest term first to avoid partial matches\n",
    "    - Escapes HTML in definitions\n",
    "    - Avoids modifying inside HTML tags (best-effort)\n",
    "    \"\"\"\n",
    "    if not text or not term_glossary:\n",
    "        return text\n",
    "\n",
    "    # Keep only non-empty terms with definitions\n",
    "    term_glossary = {k: v for k, v in term_glossary.items() if k and v}\n",
    "    if not term_glossary:\n",
    "        return text\n",
    "\n",
    "    # Prefer longer terms first (\"Transformer Architecture\" before \"Transformer\")\n",
    "    terms_sorted = sorted(term_glossary.keys(), key=len, reverse=True)\n",
    "    pattern = re.compile(\"|\".join(re.escape(t) for t in terms_sorted))\n",
    "\n",
    "    # Split on HTML tags so we only modify visible text (not attributes)\n",
    "    parts = re.split(r\"(<[^>]+>)\", text)\n",
    "\n",
    "    def repl(m: re.Match) -> str:\n",
    "        term = m.group(0)\n",
    "        definition = term_glossary.get(term, \"\")\n",
    "        safe_def = html.escape(definition, quote=True)\n",
    "        return f'<abbr title=\"{safe_def}\">{term}</abbr>'\n",
    "\n",
    "    # Replace only outside tags: even indices\n",
    "    for i in range(0, len(parts), 2):\n",
    "        parts[i] = pattern.sub(repl, parts[i])\n",
    "\n",
    "    return \"\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3bed46-6167-415e-a132-f9743662eb36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "term_to_definition = {\n",
    "    \"Large Language Models\": \"Neural networks trained on massive text corpora to understand and generate language.\",\n",
    "    \"Transformer Architecture\": \"A neural network architecture based on self-attention mechanisms.\",\n",
    "    \"RAG\": \"Retrieval-Augmented Generation, which combines retrieval with text generation.\"\n",
    "}\n",
    "\n",
    "llm_answer_text = (\n",
    "    \"Large Language Models are typically based on the Transformer Architecture, \"\n",
    "    \"while RAG is a dominant approach for retrieving and generating knowledge.\"\n",
    ")\n",
    "\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, term_to_definition)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202dbd04-e295-48dc-a987-d69a86dec61e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce41c0f-987f-4b46-8f70-844e736e859b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1) Databricks LLM setup\n",
    "# -----------------------\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "# Use a separate evaluator LLM (lower temperature for consistency)\n",
    "evaluator_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.0)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Define a structured evaluation model\n",
    "# -----------------------------------------\n",
    "class TerminologyEvaluation(BaseModel):\n",
    "    \"\"\"A structured model for evaluating terminology consistency.\"\"\"\n",
    "    consistency_score: int = Field(\n",
    "        description=\"Score from 1 to 5 (5=fully consistent, 1=severely inconsistent).\"\n",
    "    )\n",
    "    is_consistent: bool = Field(\n",
    "        description=\"Whether the answer is overall compliant with terminology standards.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Explanation of the score, highlighting strengths and issues.\"\n",
    "    )\n",
    "    suggestions_for_improvement: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Concrete suggestions to improve terminology usage.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3) Glossary (data-driven)\n",
    "# -----------------------\n",
    "# You can expand this anytime; prompt remains unchanged.\n",
    "GLOSSARY: List[Dict[str, object]] = [\n",
    "    {\"term\": \"Convolutional Neural Network\", \"aliases\": [\"CNN\"]},\n",
    "    {\"term\": \"Transformer Model\", \"aliases\": [\"Transformer\", \"Transformer Architecture\"]},\n",
    "    {\"term\": \"Large Language Model\", \"aliases\": [\"LLM\", \"Large Language Models\"]},\n",
    "]\n",
    "\n",
    "def format_glossary(glossary: List[Dict[str, object]]) -> str:\n",
    "    \"\"\"Render glossary into a compact, evaluator-friendly block.\"\"\"\n",
    "    lines = []\n",
    "    for entry in glossary:\n",
    "        term = entry[\"term\"]\n",
    "        aliases = entry.get(\"aliases\", []) or []\n",
    "        if aliases:\n",
    "            lines.append(f\"- {term} (aliases: {', '.join(aliases)})\")\n",
    "        else:\n",
    "            lines.append(f\"- {term}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "glossary_text = format_glossary(GLOSSARY)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 4) Build evaluation chain\n",
    "# -----------------------\n",
    "evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a strict technical documentation quality evaluator. \"\n",
    "     \"You ONLY evaluate terminology usage based on the provided glossary and criteria.\"),\n",
    "    (\"human\",\n",
    "     \"\"\"Evaluate the terminology consistency and correctness in the answer.\n",
    "\n",
    "Evaluation Criteria:\n",
    "1) Accuracy: Are standard terms used correctly?\n",
    "2) Compliance: Does the answer avoid unofficial or ambiguous aliases when a standard term should be used?\n",
    "3) Completeness: Does the answer use the most appropriate standard terms when needed?\n",
    "\n",
    "Authoritative Terminology Glossary:\n",
    "{glossary_text}\n",
    "\n",
    "Answer to Evaluate:\n",
    "{answer_text}\n",
    "\n",
    "Rules:\n",
    "- Score must be an integer 1..5.\n",
    "- Set is_consistent=true only if terminology is largely compliant (minor issues ok).\n",
    "- Give concrete rewrite suggestions (phrases to replace), not vague advice.\n",
    "\n",
    "Return your result as structured output.\n",
    "\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "evaluation_chain = evaluation_prompt | evaluator_llm.with_structured_output(TerminologyEvaluation)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 5) Run evaluations\n",
    "# -----------------------\n",
    "good_answer = (\n",
    "    \"A Large Language Model (LLM) is built on a Transformer Model, while a \"\n",
    "    \"Convolutional Neural Network (CNN) is widely used in image-related domains.\"\n",
    ")\n",
    "\n",
    "bad_answer = (\n",
    "    \"A big model is based on a transformer-style architecture, and a conv net \"\n",
    "    \"is very strong at picture processing.\"\n",
    ")\n",
    "\n",
    "print(\"--- Evaluating [Good Answer] ---\")\n",
    "good_eval: TerminologyEvaluation = evaluation_chain.invoke({\n",
    "    \"glossary_text\": glossary_text,\n",
    "    \"answer_text\": good_answer\n",
    "})\n",
    "print(good_eval)\n",
    "print(\"\\nScore:\", good_eval.consistency_score)\n",
    "print(\"Consistent:\", good_eval.is_consistent)\n",
    "print(\"Suggestions:\", good_eval.suggestions_for_improvement)\n",
    "\n",
    "print(\"\\n--- Evaluating [Needs Improvement Answer] ---\")\n",
    "bad_eval: TerminologyEvaluation = evaluation_chain.invoke({\n",
    "    \"glossary_text\": glossary_text,\n",
    "    \"answer_text\": bad_answer\n",
    "})\n",
    "print(bad_eval)\n",
    "print(\"\\nScore:\", bad_eval.consistency_score)\n",
    "print(\"Consistent:\", bad_eval.is_consistent)\n",
    "print(\"Suggestions:\", bad_eval.suggestions_for_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c8a7e9-20f2-4793-a967-d554d92ce3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_RAG",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
