{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c34d04d-11a4-40fc-b7d8-b6904cf6e1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -U -qqqq \n",
    "# backoff \n",
    "# databricks-langchain \n",
    "# langgraph==0.5.3 \n",
    "# uv \n",
    "# databricks-agents \n",
    "# mlflow-skinny[databricks] \n",
    "# chromadb \n",
    "# sentence-transformers \n",
    "# langchain-huggingface\n",
    "# langchain-chroma \n",
    "# wikipedia \n",
    "# faiss-cpu\n",
    "\n",
    "\n",
    "%pip install -U transformers sentence-transformers faiss-cpu scikit-learn spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c4878a-34a0-466d-8a77-34d802a685b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7057b129-723a-4dea-b3b6-d452091ceff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc94837a-86f2-4ff8-b1bf-c40fbcbcd8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define a terminology glossary for RAG variants (keep it updated, including context tags)\n",
    "GLOSSARY = [\n",
    "    {\n",
    "    \"term\": \"Retrieval-Augmented Generation\",\n",
    "    \"synonyms\": [\"RAG\", \"retrieval augmented generation\", \"retrieval-augmented generation\"],\n",
    "    \"definition\": \"A generation framework that retrieves external evidence and conditions an LLM on it.\",\n",
    "    \"context_tags\": [\"LLM\", \"search\", \"grounding\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Self-RAG\",\n",
    "        \"synonyms\": [\"Self RAG\", \"self-rag\", \"self-reflective RAG\", \"self-refining RAG\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach where the model explicitly self-checks (e.g., reflect, critique, verify) during generation, \"\n",
    "            \"deciding when to retrieve more evidence and how to revise its answer based on feedback signals.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"reflection\", \"self-critique\", \"verification\", \"iterative retrieval\", \"hallucination mitigation\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Corrective RAG\",\n",
    "        \"synonyms\": [\"C-RAG\", \"CRAG\", \"Corrective-RAG\", \"corrective rag\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach that detects low-quality retrieval or unsupported generations and applies corrective actions \"\n",
    "            \"(e.g., re-retrieve, rewrite queries, filter evidence, or re-rank) to improve factual grounding.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"retrieval quality\", \"re-ranking\", \"query rewriting\", \"evidence filtering\", \"robustness\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Knowledge Graph RAG\",\n",
    "        \"synonyms\": [\"KG-RAG\", \"KGRAG\", \"KG RAG\", \"knowledge-graph RAG\", \"graph RAG\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach that retrieves and reasons over a knowledge graph (entities/relations) as structured evidence, \"\n",
    "            \"often combining graph traversal with text retrieval to support multi-hop and relational questions.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"knowledge graph\", \"multi-hop reasoning\", \"entity linking\", \"graph traversal\", \"structured grounding\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Entity Linking\",\n",
    "        \"synonyms\": [\"EL\", \"entity resolution\", \"mention linking\", \"entity disambiguation\"],\n",
    "        \"definition\": (\n",
    "            \"The process of mapping a text mention (e.g., 'Apple') to a canonical entity (e.g., Apple Inc.) in a KB/KG \"\n",
    "            \"to support structured retrieval and reduce ambiguity.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"KG-RAG\", \"disambiguation\", \"knowledge base\", \"information extraction\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Evidence Grounding\",\n",
    "        \"synonyms\": [\"grounding\", \"source grounding\", \"evidence-based generation\"],\n",
    "        \"definition\": (\n",
    "            \"Constraining or evaluating generation based on retrieved evidence so claims are supported by sources; \"\n",
    "            \"commonly paired with citation, attribution, or entailment checks.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"factuality\", \"citations\", \"attribution\", \"hallucination mitigation\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1e02a5-3aaa-4574-aa36-cda5a7526de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class TerminologyProcessor:\n",
    "    def __init__(self, glossary: List[Dict[str, Any]]):\n",
    "        self.glossary = glossary\n",
    "        self.standard_term_map = {}\n",
    "        self.alias_to_entries_map = {}\n",
    "        self._build_mappings()\n",
    "\n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Build mappings; one alias may map to multiple terminology entries to handle ambiguity.\"\"\"\n",
    "        for entry in self.glossary:\n",
    "            standard_term = entry[\"term\"]\n",
    "            self.standard_term_map[standard_term.lower()] = standard_term\n",
    "\n",
    "            all_aliases = [standard_term] + entry.get(\"synonyms\", [])\n",
    "            for alias in all_aliases:\n",
    "                alias_lower = alias.lower()\n",
    "                if alias_lower not in self.alias_to_entries_map:\n",
    "                    self.alias_to_entries_map[alias_lower] = []\n",
    "                if entry not in self.alias_to_entries_map[alias_lower]:\n",
    "                    self.alias_to_entries_map[alias_lower].append(entry)\n",
    "\n",
    "    def standardize_text(self, text: str, context_window: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Context-aware terminology standardization using iteration + a replacement function.\n",
    "        Dynamically generates the correct regex for each term type.\n",
    "        \"\"\"\n",
    "        standardized_text = text\n",
    "        sorted_keys = sorted(self.alias_to_entries_map.keys(), key=len, reverse=True)\n",
    "\n",
    "        for key_lower in sorted_keys:\n",
    "            possible_entries = self.alias_to_entries_map[key_lower]\n",
    "\n",
    "            # --- Dynamically create the correct regex for each key ---\n",
    "            pattern_str = \"\"\n",
    "            # If key contains Latin letters, assume it's an abbreviation and enforce boundaries\n",
    "            if re.search(r\"[a-zA-Z]\", key_lower):\n",
    "                # Use lookarounds to avoid matching inside a larger word\n",
    "                pattern_str = r\"(?<![a-zA-Z])\" + re.escape(key_lower) + r\"(?![a-zA-Z])\"\n",
    "            else:\n",
    "                # For Chinese (or non-Latin) terms, match exactly\n",
    "                pattern_str = re.escape(key_lower)\n",
    "\n",
    "            pattern = re.compile(pattern_str, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replacement function called for each match\n",
    "            def replacer(match: re.Match) -> str:\n",
    "                if len(possible_entries) == 1:\n",
    "                    return possible_entries[0][\"term\"]\n",
    "                else:\n",
    "                    # --- Context-based disambiguation ---\n",
    "                    context_snippet = standardized_text[\n",
    "                        max(0, match.start() - context_window) : min(len(standardized_text), match.end() + context_window)\n",
    "                    ]\n",
    "                    for entry in possible_entries:\n",
    "                        clues = entry.get(\"context_tags\", []) + [entry[\"term\"]]\n",
    "                        if any(clue in context_snippet for clue in clues):\n",
    "                            return entry[\"term\"]\n",
    "                    # If no context clue is found, fall back to the first definition\n",
    "                    return possible_entries[0][\"term\"]\n",
    "\n",
    "            # Update text using the replacement function\n",
    "            standardized_text = pattern.sub(replacer, standardized_text)\n",
    "\n",
    "        return standardized_text\n",
    "\n",
    "    def extract_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract known standardized terms from text\n",
    "        \"\"\"\n",
    "        found_terms = set()\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for standard_term_lower, original_standard_term in self.standard_term_map.items():\n",
    "            # Direct substring search; do not use \\b\n",
    "            if re.search(re.escape(standard_term_lower), text_lower):\n",
    "                found_terms.add(original_standard_term)\n",
    "\n",
    "        return sorted(list(found_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536b7fdd-1949-44f0-96f5-84ab573841de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Initialize the terminology processor with the glossary.\n",
    "term_processor = TerminologyProcessor(GLOSSARY)\n",
    "print(\"--- 1. Initialize the terminology processor with the glossary ---\")\n",
    "\n",
    "# 2. Data preprocessing: terminology standardization\n",
    "print(\"--- 2. Data Preprocessing: Terminology Standardization ---\")\n",
    "user_query = \"I want to learn about applications of using RAG.\"\n",
    "processed_query = term_processor.standardize_text(user_query)\n",
    "print(f\"Original query: {user_query}\")\n",
    "print(f\"Standardized query: {processed_query}\")\n",
    "\n",
    "document_text = \"Recently I studied Self-RAG.\"\n",
    "processed_document = term_processor.standardize_text(document_text)\n",
    "print(f\"Original document: {document_text}\")\n",
    "print(f\"Standardized document: {processed_document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90cb5524-aa4f-4f61-a72e-86354d8b23be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Term extraction (for downstream vectorization or metadata tagging)\n",
    "print(\"\\n--- 3. Term Extraction ---\")\n",
    "extracted_terms_query = term_processor.extract_terms(processed_query)\n",
    "print(f\"Extracted terms from query: {extracted_terms_query}\")\n",
    "\n",
    "extracted_terms_document = term_processor.extract_terms(processed_document)\n",
    "print(f\"Extracted terms from document: {extracted_terms_document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb383f00-6742-44f8-8e86-93aa29176eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Simulated vector storage and retrieval augmentation (conceptual)\n",
    "print(\"\\n--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\")\n",
    "print(\"In a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\")\n",
    "print(\"These vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\")\n",
    "print(\"During retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acedac0-bd4b-4e04-9ed3-f98dafde0487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Simulated retrieval augmentation: query expansion\n",
    "def enhance_query_for_retrieval(query: str, processor: TerminologyProcessor) -> List[str]:\n",
    "    \"\"\"Expand query keywords using the terminology glossary to improve recall.\"\"\"\n",
    "    standardized_query = processor.standardize_text(query)\n",
    "    query_terms = processor.extract_terms(standardized_query)\n",
    "\n",
    "    expanded_keywords = set([standardized_query])\n",
    "    for term in query_terms:\n",
    "        expanded_keywords.add(term)\n",
    "        for entry in processor.glossary:\n",
    "            if entry[\"term\"] == term:\n",
    "                for synonym in entry.get(\"synonyms\", []):\n",
    "                    expanded_keywords.add(synonym)\n",
    "                break\n",
    "    return sorted(list(expanded_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fcd4d7d-5999-4454-aeb4-77fe3481ff18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 5. Simulated Retrieval Augmentation: Query Expansion ---\")\n",
    "original_query_for_retrieval = \"I want to know what a RAG does in an LLM?\"\n",
    "expanded_keywords = enhance_query_for_retrieval(original_query_for_retrieval, term_processor)\n",
    "print(f\"Original retrieval query: {original_query_for_retrieval}\")\n",
    "print(f\"Expanded retrieval keyword list: {expanded_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3845aba8-c509-43a7-9dc1-7f15aaf08c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "DASHES = r\"[-\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212]\"  # common dash chars\n",
    "\n",
    "def alias_to_pattern(alias: str):\n",
    "    # If alias contains a dash, match it as its own token with regex\n",
    "    if re.search(DASHES, alias):\n",
    "        # split on dash and keep the two sides\n",
    "        left, right = re.split(DASHES, alias, maxsplit=1)\n",
    "        left_tokens = left.strip().split()\n",
    "        right_tokens = right.strip().split()\n",
    "        return [{\"LOWER\": t.lower()} for t in left_tokens] + [{\"TEXT\": {\"REGEX\": DASHES}}] + [{\"LOWER\": t.lower()} for t in right_tokens]\n",
    "    else:\n",
    "        # normal phrase: token-by-token, case-insensitive\n",
    "        return [{\"LOWER\": t.lower()} for t in alias.strip().split()]\n",
    "\n",
    "def extract_terms_with_ruler(text, glossary):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # avoid adding duplicate pipes if you call this multiple times\n",
    "    if \"entity_ruler\" in nlp.pipe_names:\n",
    "        ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "        ruler.clear()\n",
    "    else:\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True})\n",
    "\n",
    "    patterns = []\n",
    "    for entry in glossary:\n",
    "        for alias in [entry.get(\"term\")] + entry.get(\"synonyms\", []):\n",
    "            if not alias:\n",
    "                continue\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": alias_to_pattern(alias)})\n",
    "\n",
    "    # prefer longer patterns first (helps with overlaps like KG-RAG vs RAG)\n",
    "    patterns.sort(key=lambda p: len(p[\"pattern\"]), reverse=True)\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ == \"TERM\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec190888-03e9-4a42-b658-1cb73d695f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example \n",
    "query = \"Examples of KG-RAG\" \n",
    "processed_query = term_processor.standardize_text(query)\n",
    "print(processed_query)\n",
    "# processed_query = \"Examples of Knowledge Graph Retrieval-Augmented Generation\"\n",
    "candidates = extract_terms_with_ruler(processed_query, GLOSSARY)\n",
    "print(f\"Automatically extracted term candidates: {candidates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76216f23-819f-492c-806f-405888b6d542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure LLM  and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c0903a-4f79-43cd-910c-af9fbcff75e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# It is recommended to load the model once during project initialization\n",
    "# to avoid repeated loading overhead.\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def map_synonyms_by_similarity(main_terms: list, candidates: list, threshold: float = 0.8) -> dict:\n",
    "    \"\"\"\n",
    "    Map candidate terms to the closest standard terms by computing\n",
    "    cosine similarity between embeddings.\n",
    "\n",
    "    Args:\n",
    "        main_terms (list): List of standard (canonical) terms.\n",
    "        candidates (list): List of candidate synonyms to be matched.\n",
    "        threshold (float): Similarity threshold above which a candidate\n",
    "                           is considered a synonym.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each standard term to a list of\n",
    "              successfully matched synonyms.\n",
    "    \"\"\"\n",
    "    _matched_synonyms = {term: [] for term in main_terms}\n",
    "\n",
    "    if not main_terms or not candidates:\n",
    "        return _matched_synonyms\n",
    "    \n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode in batches for better efficiency\n",
    "    embeddings = model.encode(main_terms + candidates, convert_to_tensor=True)\n",
    "    term_embeddings = embeddings[:len(main_terms)]\n",
    "    candidate_embeddings = embeddings[len(main_terms):]\n",
    "\n",
    "    # Compute the cosine similarity matrix between standard terms and candidates\n",
    "    similarity_matrix = util.cos_sim(term_embeddings, candidate_embeddings)\n",
    "\n",
    "    for i, term in enumerate(main_terms):\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                _matched_synonyms[term].append(candidate)\n",
    "\n",
    "    return _matched_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d2ce6b-14ff-4cc3-b278-75e8ad475a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_terms_to_map = [\"RAG\"]\n",
    "all_possible_synonyms = [\n",
    "    entry[\"term\"]\n",
    "    for entry in GLOSSARY\n",
    "] + [\n",
    "    synonym\n",
    "    for entry in GLOSSARY\n",
    "    for synonym in entry.get(\"synonyms\", [])\n",
    "]\n",
    "optimized_mapped_synonyms = map_synonyms_by_similarity(\n",
    "    main_terms_to_map,\n",
    "    all_possible_synonyms\n",
    ")\n",
    "print(\"\\nOptimized matched synonyms:\", optimized_mapped_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c4bc8a-3433-4b03-973a-e701fe3b7de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "print(f\"\\nAttempting to load model '{model_name}'...\")\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988f01b0-9ae4-411f-ab42-ceaa81172673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "\n",
    "def build_term_vector_index(\n",
    "    term_glossary: Dict[str, dict],\n",
    "    model: SentenceTransformer,\n",
    "    use_cosine: bool = True\n",
    ") -> Tuple[faiss.Index, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert all terms and their synonyms into vector embeddings and build a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        term_glossary (dict):\n",
    "            A structured glossary where keys are canonical terms and values contain\n",
    "            a 'synonyms' list.\n",
    "        model (SentenceTransformer):\n",
    "            A loaded SentenceTransformer model.\n",
    "        use_cosine (bool):\n",
    "            Whether to use cosine similarity (recommended for sentence embeddings).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            (faiss_index, indexed_terms)\n",
    "            - faiss_index: FAISS index containing all embeddings\n",
    "            - indexed_terms: list of terms aligned with index rows\n",
    "    \"\"\"\n",
    "    terms_to_index: List[str] = []\n",
    "\n",
    "    # Collect canonical terms and synonyms\n",
    "    for canonical_term, info in term_glossary.items():\n",
    "        terms_to_index.append(canonical_term)\n",
    "        synonyms = info.get(\"synonyms\", [])\n",
    "        if isinstance(synonyms, list):\n",
    "            terms_to_index.extend(synonyms)\n",
    "\n",
    "    # Deduplicate while keeping deterministic order\n",
    "    indexed_terms = sorted(set(terms_to_index))\n",
    "    if not indexed_terms:\n",
    "        raise ValueError(\"No terms found in glossary.\")\n",
    "\n",
    "    print(\"Generating term embeddings...\")\n",
    "    embeddings = model.encode(\n",
    "        indexed_terms,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine,\n",
    "        show_progress_bar=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    # Choose FAISS index type\n",
    "    if use_cosine:\n",
    "        # Cosine similarity = inner product on normalized vectors\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    metric = \"cosine similarity\" if use_cosine else \"L2 distance\"\n",
    "    print(f\"FAISS index built successfully. \"\n",
    "          f\"Vectors: {index.ntotal}, Dimension: {dim}, Metric: {metric}\")\n",
    "\n",
    "    return index, indexed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998f2c82-af56-447d-a842-203d4df24253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def convert_glossary_to_term_dict(\n",
    "    glossary: List[Dict[str, Any]],\n",
    "    deduplicate: bool = True,\n",
    "    keep_term_as_synonym: bool = False\n",
    ") -> Dict[str, Dict[str, list]]:\n",
    "    \"\"\"\n",
    "    Convert list-based glossary into FAISS-friendly dict format.\n",
    "\n",
    "    Args:\n",
    "        glossary: original GLOSSARY list\n",
    "        deduplicate: remove duplicate synonyms\n",
    "        keep_term_as_synonym: include canonical term in synonyms or not\n",
    "\n",
    "    Returns:\n",
    "        dict: {canonical_term: {\"synonyms\": [...]}}\n",
    "    \"\"\"\n",
    "    term_dict = {}\n",
    "\n",
    "    for entry in glossary:\n",
    "        term = entry.get(\"term\")\n",
    "        if not term:\n",
    "            continue\n",
    "\n",
    "        synonyms = entry.get(\"synonyms\", [])\n",
    "        if not isinstance(synonyms, list):\n",
    "            synonyms = []\n",
    "\n",
    "        if keep_term_as_synonym:\n",
    "            synonyms = [term] + synonyms\n",
    "\n",
    "        if deduplicate:\n",
    "            # preserve order, remove duplicates\n",
    "            seen = set()\n",
    "            synonyms = [s for s in synonyms if not (s in seen or seen.add(s))]\n",
    "\n",
    "        term_dict[term] = {\"synonyms\": synonyms}\n",
    "\n",
    "    return term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2e6c74-2602-49c3-800d-75623061283c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "term_glossary = convert_glossary_to_term_dict(GLOSSARY)\n",
    "print(term_glossary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a447ede-dd84-4e09-9f3f-aedad8fe577e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "index, indexed_terms = build_term_vector_index(\n",
    "    term_glossary=term_glossary,\n",
    "    model=model,\n",
    "    use_cosine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e6ec73-8944-44fb-ba7b-6737d9d052f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def search_terms(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    indexed_terms: List[str],\n",
    "    top_k: int = 5,\n",
    "    use_cosine: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the most similar terms to a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query text.\n",
    "        model (SentenceTransformer): SentenceTransformer model.\n",
    "        index (faiss.Index): FAISS index.\n",
    "        indexed_terms (list): Terms aligned with index rows.\n",
    "        top_k (int): Number of results to return.\n",
    "        use_cosine (bool): Whether cosine similarity is used.\n",
    "\n",
    "    Returns:\n",
    "        list of (term, score) tuples.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        results.append((indexed_terms[idx], float(score)))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f8772c-e834-432b-8902-b23c13e6e864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Index Build Successful ---\")\n",
    "print(\"Number of vectors in FAISS index:\", index.ntotal)\n",
    "print(\"Indexed terms:\", indexed_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a65a4ab-3299-40ee-ae10-0d3ac861ec7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_RAG",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
