{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8901e340-f23c-4263-8dcc-8e887f407be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
<<<<<<< HEAD
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n  Downloading langchain-1.2.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-databricks\n  Downloading langchain_databricks-0.1.2-py3-none-any.whl.metadata (3.3 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (7.6 kB)\nCollecting tiktoken\n  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (6.7 kB)\nCollecting langchain-core<2.0.0,>=1.2.1 (from langchain)\n  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\nCollecting langgraph<1.1.0,>=1.0.2 (from langchain)\n  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.10.6)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (9.5 kB)\nCollecting requests<3.0.0,>=2.32.5 (from langchain-community)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (6.0.2)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (8.1 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (9.0.0)\nCollecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n  Downloading langsmith-0.5.2-py3-none-any.whl.metadata (15 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (2.1.3)\nCollecting databricks-vectorsearch<0.41,>=0.40 (from langchain-databricks)\n  Downloading databricks_vectorsearch-0.40-py3-none-any.whl.metadata (2.8 kB)\nINFO: pip is looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-databricks\n  Downloading langchain_databricks-0.1.1-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_databricks-0.1.0-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\nINFO: pip is still looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting langchain\n  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting langchain-community\n  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\nCollecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n  Downloading SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.6-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\nCollecting langchain\n  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n  Downloading langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.19-py3-none-any.whl.metadata (7.9 kB)\n  Downloading langchain_community-0.0.18-py3-none-any.whl.metadata (7.9 kB)\n  Downloading langchain_community-0.0.17-py3-none-any.whl.metadata (7.9 kB)\n  Downloading langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\n  Downloading langchain_community-0.0.15-py3-none-any.whl.metadata (7.6 kB)\n  Downloading langchain_community-0.0.14-py3-none-any.whl.metadata (7.5 kB)\n  Downloading langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\n  Downloading langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n  Downloading langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.6-py3-none-any.whl.metadata (7.2 kB)\n  Downloading langchain_community-0.0.5-py3-none-any.whl.metadata (7.1 kB)\n  Downloading langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n  Downloading langchain_community-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n  Downloading langchain_community-0.0.2-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain\n  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\nCollecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (24.1)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.12.2)\nCollecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n  Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (1.1 kB)\nCollecting langchain\n  Downloading langchain-1.1.2-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.1.1-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.7-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.4-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n  Downloading langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n  Downloading langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n  Downloading langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n  Downloading langchain_core-0.3.81-py3-none-any.whl.metadata (3.2 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.32.3)\nCollecting mlflow>=2.16.0 (from langchain-databricks)\n  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: scipy>=1.11 in /databricks/python3/lib/python3.12/site-packages (from langchain-databricks) (1.15.1)\nCollecting regex>=2022.1.18 (from tiktoken)\n  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (75 kB)\nRequirement already satisfied: mlflow-skinny<3,>=2.11.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.22.0)\nCollecting protobuf<5,>=3.12.0 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\nCollecting deprecation>=2 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\nCollecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (41 kB)\nCollecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nINFO: pip is looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\nCollecting mlflow>=2.16.0 (from langchain-databricks)\n  Downloading mlflow-3.8.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.7.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.5.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.5.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.2-py3-none-any.whl.metadata (30 kB)\nINFO: pip is still looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\n  Downloading mlflow-3.3.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.3-py3-none-any.whl.metadata (29 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-2.22.4-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-skinny<3,>=2.11.3 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading mlflow_skinny-2.22.4-py3-none-any.whl.metadata (31 kB)\nCollecting Flask<4 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.1.5)\nCollecting alembic!=1.10.0,<2 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\nCollecting docker<8,>=4.0.0 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting markdown<4,>=3.3 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.10.0)\nRequirement already satisfied: pandas!=2.3.0,<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (2.2.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (1.6.1)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.49.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.5.3)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.34.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.1.31)\nCollecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n  Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (4.1 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow>=2.16.0->langchain-databricks)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow>=2.16.0->langchain-databricks) (3.0.2)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow>=2.16.0->langchain-databricks) (2.9.0.post0)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.6.2)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (3.2.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (3.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.40.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.21.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.53b1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.16.0->langchain-databricks) (1.16.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.9.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.4.8)\nDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m23.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m80.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_core-0.3.81-py3-none-any.whl (457 kB)\nDownloading langchain_databricks-0.1.2-py3-none-any.whl (21 kB)\nDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (11.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/11.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.5/11.5 MB\u001B[0m \u001B[31m155.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m50.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m77.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_vectorsearch-0.40-py3-none-any.whl (12 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\nDownloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\nDownloading langsmith-0.5.2-py3-none-any.whl (283 kB)\nDownloading mlflow-2.22.4-py3-none-any.whl (29.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/29.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.0/29.0 MB\u001B[0m \u001B[31m188.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-2.22.4-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m150.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\nDownloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (798 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/798.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m798.6/798.6 kB\u001B[0m \u001B[31m37.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m83.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading alembic-1.17.2-py3-none-any.whl (248 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (243 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (597 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/597.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m597.3/597.3 kB\u001B[0m \u001B[31m25.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading markdown-3.10-py3-none-any.whl (107 kB)\nDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\nDownloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (258 kB)\nDownloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (132 kB)\nDownloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (225 kB)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl (294 kB)\nDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nDownloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (340 kB)\nDownloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (372 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nInstalling collected packages: werkzeug, uuid-utils, typing-inspection, typing-inspect, requests, regex, python-dotenv, protobuf, propcache, orjson, multidict, marshmallow, markdown, Mako, jsonpatch, itsdangerous, httpx-sse, gunicorn, greenlet, graphql-core, frozenlist, faiss-cpu, deprecation, blinker, aiohappyeyeballs, yarl, tiktoken, SQLAlchemy, requests-toolbelt, graphql-relay, Flask, docker, dataclasses-json, aiosignal, pydantic-settings, langsmith, graphene, alembic, aiohttp, langchain-core, mlflow-skinny, langchain-text-splitters, mlflow, langchain, databricks-vectorsearch, langchain-databricks, langchain-community\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.7.0\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.22.0\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngrpcio-status 1.67.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed Flask-3.1.2 Mako-1.3.10 SQLAlchemy-2.0.45 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 alembic-1.17.2 blinker-1.9.0 databricks-vectorsearch-0.40 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 faiss-cpu-1.13.2 frozenlist-1.8.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.0 gunicorn-23.0.0 httpx-sse-0.4.3 itsdangerous-2.2.0 jsonpatch-1.33 langchain-0.3.27 langchain-community-0.3.31 langchain-core-0.3.81 langchain-databricks-0.1.2 langchain-text-splitters-0.3.11 langsmith-0.5.2 markdown-3.10 marshmallow-3.26.2 mlflow-2.22.4 mlflow-skinny-2.22.4 multidict-6.7.0 orjson-3.11.5 propcache-0.4.1 protobuf-4.25.8 pydantic-settings-2.12.0 python-dotenv-1.2.1 regex-2025.11.3 requests-2.32.5 requests-toolbelt-1.0.0 tiktoken-0.12.0 typing-inspect-0.9.0 typing-inspection-0.4.2 uuid-utils-0.12.0 werkzeug-3.1.4 yarl-1.22.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> origin/main
   "source": [
    "%pip install -U langchain langchain-community langchain-databricks faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b641c7-0eb8-4e2f-8d17-20c47d967d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef54e8-4f3d-42ab-b96f-b60ca42326f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b1f725-60ba-4c28-b19e-21107de2b062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
<<<<<<< HEAD
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-0c4b8dbc-4a66-4826-9d56-20/.ipykernel/3944/command-5829496254952718-1259243950:8: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n/home/spark-0c4b8dbc-4a66-4826-9d56-20/.ipykernel/3944/command-5829496254952718-1259243950:9: LangChainDeprecationWarning: Use databricks_langchain.DatabricksEmbeddings\n  embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> origin/main
   "source": [
    "# -----------------------\n",
    "# 1) Databricks LLM + Embeddings\n",
    "# -----------------------\n",
    "# Make sure your Databricks auth is configured (e.g., DATABRICKS_HOST + DATABRICKS_TOKEN)\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"  # <-- change to your embedding endpoint name\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b03608-cbeb-476e-8ee0-28e4db57563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Sentence split + sentence-window splitter (LlamaIndex-like)\n",
    "# -----------------------\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[。！？!?])\\s+|\\n+\")\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def sentence_window_splitter(\n",
    "    documents: List[Document],\n",
    "    window_size: int = 2,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Mimics LlamaIndex SentenceWindowNodeParser:\n",
    "    - Each chunk is one core sentence\n",
    "    - metadata includes:\n",
    "        - original_text: core sentence\n",
    "        - window: context window (core +/- window_size sentences)\n",
    "    \"\"\"\n",
    "    out: List[Document] = []\n",
    "    for doc in documents:\n",
    "        sents = split_sentences(doc.page_content)\n",
    "        for i, core in enumerate(sents):\n",
    "            lo = max(0, i - window_size)\n",
    "            hi = min(len(sents), i + window_size + 1)\n",
    "            window_text = \" \".join(sents[lo:hi])\n",
    "\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=core,\n",
    "                    metadata={\n",
    "                        **(doc.metadata or {}),\n",
    "                        \"original_text\": core,\n",
    "                        \"window\": window_text,\n",
    "                        \"sent_index\": i,\n",
    "                        \"window_size\": window_size,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a46cdd-8ca8-421c-acc2-b26138b51c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Prompt + QA\n",
    "# -----------------------\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful technical assistant. Answer using ONLY the provided context. \"\n",
    "     \"If the context is insufficient, say what is missing.\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nAnswer in English:\")\n",
    "])\n",
    "\n",
    "def answer_with_retrieval(\n",
    "    docs: List[Document],\n",
    "    question: str,\n",
    "    top_k: int = 5,\n",
    "    use_window_metadata: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build FAISS index, retrieve top_k chunks, print retrieved chunks, then ask the Databricks LLM.\n",
    "    If use_window_metadata=True, feed metadata['window'] as context (Sentence Window style).\n",
    "    \"\"\"\n",
    "    vs = FAISS.from_documents(docs, embeddings)\n",
    "    retrieved = vs.similarity_search_with_score(question, k=top_k)\n",
    "\n",
    "    print(\"\\n--- Top retrieved chunks ---\")\n",
    "    context_blocks = []\n",
    "    for rank, (d, score) in enumerate(retrieved, 1):\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            window = d.metadata.get(\"window\", d.page_content)\n",
    "            core = d.metadata.get(\"original_text\", d.page_content)\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(f\"Core: {core}\")\n",
    "            print(f\"Window:\\n{window}\")\n",
    "            context_blocks.append(window)\n",
    "        else:\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(d.page_content)\n",
    "            context_blocks.append(d.page_content)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    msg = QA_PROMPT.format_messages(question=question, context=context)\n",
    "    resp = llm.invoke(msg)\n",
    "\n",
    "    print(\"\\n--- LLM Answer ---\")\n",
    "    print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65d9762-ddc9-48d9-9d54-09f8a1931f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4) Runner (prints raw chunks + retrieval behavior)\n",
    "# -----------------------\n",
    "def evaluate_splitter(\n",
    "    splitter_name: str,\n",
    "    split_fn: Callable[[], List[Document]],\n",
    "    question: str,\n",
    "    use_window_metadata: bool = False,\n",
    "    top_k: int = 5,\n",
    "    max_print_chunks: int = 50,\n",
    ") -> None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing splitter: {splitter_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    chunks = split_fn()\n",
    "\n",
    "    print(f\"\\n[Raw chunks generated] total={len(chunks)}\")\n",
    "    for i, d in enumerate(chunks[:max_print_chunks], 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            print(f\"Core: {d.metadata.get('original_text')}\")\n",
    "            print(f\"Window: {d.metadata.get('window')}\")\n",
    "        else:\n",
    "            print(d.page_content)\n",
    "\n",
    "    if len(chunks) > max_print_chunks:\n",
    "        print(f\"\\n... (only printed first {max_print_chunks} chunks)\")\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer_with_retrieval(\n",
    "        docs=chunks,\n",
    "        question=question,\n",
    "        top_k=top_k,\n",
    "        use_window_metadata=use_window_metadata,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{splitter_name} done.\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1906460-b1a6-4bb1-b7a8-b294fd615fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
<<<<<<< HEAD
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nTesting splitter: Token Split (chunk_size=30, overlap=0)\n============================================================\n\n[Raw chunks generated] total=20\n\n--- Chunk 1 ---\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n--- Chunk 2 ---\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and\n\n--- Chunk 3 ---\n use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific\n\n--- Chunk 4 ---\n search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector index\n\n--- Chunk 5 ---\ning,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 6 ---\n\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise\n\n--- Chunk 7 ---\n and efficient.\n\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference,\n\n--- Chunk 8 ---\n a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language\n\n--- Chunk 9 ---\n Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is\n\n--- Chunk 10 ---\n less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the\n\n--- Chunk 11 ---\n AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful\n\n--- Chunk 12 ---\n capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression\n\n--- Chunk 13 ---\n, and clustering.\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of\n\n--- Chunk 14 ---\n complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking\n\n--- Chunk 15 ---\n strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n\n\n--- Chunk 16 ---\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\n\n\n--- Chunk 17 ---\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries\n\n--- Chunk 18 ---\n.\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth\n\n--- Chunk 19 ---\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n--- Chunk 20 ---\n characteristics\nof the data and the expected query types.\n\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3183\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n[2] score=0.3336\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n[3] score=0.3830\n search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector index\n\n[4] score=0.4250\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and\n\n[5] score=0.4327\n complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector index\n5. Text generation\n\nSentence window chunking and semantic chunking differ in that sentence window chunking is an advanced chunking strategy that is not explicitly described in the provided context, whereas semantic chunking is not mentioned at all. However, based on the general concept of chunking, it can be inferred that semantic chunking likely involves dividing text into meaningful units based on their semantic relationships or meanings.\n\nToken Split (chunk_size=30, overlap=0) done.\n============================================================\n\n\n============================================================\nTesting splitter: Token Split (chunk_size=30, overlap=10)\n============================================================\n\n[Raw chunks generated] total=29\n\n--- Chunk 1 ---\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n--- Chunk 2 ---\n-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\n\n--- Chunk 3 ---\n model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\n\n\n--- Chunk 4 ---\n use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific\n\n--- Chunk 5 ---\n answering,\nknowledge assistants, and domain-specific search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion,\n\n--- Chunk 6 ---\n of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis\n\n--- Chunk 7 ---\ning,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 8 ---\ns, databases, APIs, or web pages—\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred\n\n--- Chunk 9 ---\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n\nEach chunk is converted into\n\n--- Chunk 10 ---\n and efficient.\n\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference,\n\n--- Chunk 11 ---\n stored in a vector index.\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\n\n--- Chunk 12 ---\n\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nall\n\n--- Chunk 13 ---\n Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is\n\n--- Chunk 14 ---\n external knowledge.\n\n--- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-\n\n--- Chunk 15 ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem.\n\n--- Chunk 16 ---\n AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful\n\n--- Chunk 17 ---\n foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine\n\n--- Chunk 18 ---\ncikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering.\nTogether, these tools\n\n--- Chunk 19 ---\n, and clustering.\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of\n\n--- Chunk 20 ---\n practitioners,\nenabling efficient development and deployment of complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\n\n\n--- Chunk 21 ---\n another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target\n\n--- Chunk 22 ---\n strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n\n\n--- Chunk 23 ---\n“window” sentences as context.\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the\n\n--- Chunk 24 ---\n LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand,\n\n--- Chunk 25 ---\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries\n\n--- Chunk 26 ---\n than relying solely on fixed character counts or sentence boundaries.\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural\n\n--- Chunk 27 ---\n semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth advanced methods can significantly improve retrieval quality and\ndown\n\n--- Chunk 28 ---\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n--- Chunk 29 ---\nosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3183\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n[2] score=0.3336\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n[3] score=0.3912\n LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand,\n\n[4] score=0.4275\n-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\n\n[5] score=0.4342\n another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. External knowledge retrieval\n2. Language Model (LLM)\n3. Chunking strategy (which can be either sentence window chunking or semantic chunking)\n\nSentence window chunking and semantic chunking differ in that:\n\n* Sentence window chunking involves dividing the input text into chunks, where each chunk contains a target sentence and its surrounding context.\n* Semantic chunking, on the other hand, involves dividing the input text into chunks based on semantic meaning, rather than just syntactic structure.\n\nToken Split (chunk_size=30, overlap=10) done.\n============================================================\n\n\n============================================================\nTesting splitter: Sentence Window Split (window_size=2)\n============================================================\n\n[Raw chunks generated] total=44\n\n--- Chunk 1 ---\nCore: Retrieval-Augmented Generation (RAG) is a common architecture for building\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n\n--- Chunk 2 ---\nCore: LLM-powered applications that combine external knowledge retrieval with\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems\n\n--- Chunk 3 ---\nCore: text generation.\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as\n\n--- Chunk 4 ---\nCore: Instead of relying solely on a model’s internal parameters, RAG systems\nWindow: LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation.\n\n--- Chunk 5 ---\nCore: retrieve relevant information from external data sources and use it as\nWindow: text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering,\n\n--- Chunk 6 ---\nCore: context during answer generation.\nWindow: Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search.\n\n--- Chunk 7 ---\nCore: This approach is widely used for applications such as question answering,\nWindow: retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\n\n--- Chunk 8 ---\nCore: knowledge assistants, and domain-specific search.\nWindow: context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing,\n\n--- Chunk 9 ---\nCore: A typical RAG pipeline consists of several core components, including\nWindow: This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis.\n\n--- Chunk 10 ---\nCore: document ingestion, text chunking, embedding generation, vector indexing,\nWindow: knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 11 ---\nCore: retrieval, and response synthesis.\nWindow: A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed.\n\n--- Chunk 12 ---\nCore: Documents from various sources—such as PDFs, databases, APIs, or web pages—\nWindow: document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\n\n--- Chunk 13 ---\nCore: are first ingested and preprocessed.\nWindow: retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient.\n\n--- Chunk 14 ---\nCore: They are then split into smaller units, often referred to as chunks,\nWindow: Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\n\n--- Chunk 15 ---\nCore: to make retrieval more precise and efficient.\nWindow: are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index.\n\n--- Chunk 16 ---\nCore: Each chunk is converted into a vector representation using an embedding model\nWindow: They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed\n\n--- Chunk 17 ---\nCore: and stored in a vector index.\nWindow: to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks.\n\n--- Chunk 18 ---\nCore: During inference, a user query is embedded and compared against the indexed\nWindow: Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\n\n--- Chunk 19 ---\nCore: vectors to retrieve the most relevant chunks.\nWindow: and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\n--- Chunk 20 ---\nCore: These retrieved chunks are provided to a Large Language Model (LLM) as context,\nWindow: During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\n--- Chunk 21 ---\nCore: allowing the model to generate answers that are grounded in external knowledge.\nWindow: vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used\n\n--- Chunk 22 ---\nCore: --- The following content is less directly related to the RAG topic ---\nWindow: These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem.\n\n--- Chunk 23 ---\nCore: In addition, Python, as a general-purpose programming language, is widely used\nWindow: allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\n\n--- Chunk 24 ---\nCore: in the AI field due to its simplicity and rich ecosystem.\nWindow: --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data.\n\n--- Chunk 25 ---\nCore: For example, NumPy and Pandas are foundational tools for data processing,\nWindow: In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\n\n--- Chunk 26 ---\nCore: providing powerful capabilities for numerical computation and structured data.\nWindow: in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering.\n\n--- Chunk 27 ---\nCore: Scikit-learn offers a comprehensive suite of machine learning algorithms\nWindow: For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n\n--- Chunk 28 ---\nCore: for tasks such as classification, regression, and clustering.\nWindow: providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models.\n\n--- Chunk 29 ---\nCore: Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nWindow: Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\n--- Chunk 30 ---\nCore: enabling efficient development and deployment of complex AI models.\nWindow: for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk\n\n--- Chunk 31 ---\nCore: --- The following is another related but conceptually independent section ---\nWindow: Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding\n\n--- Chunk 32 ---\nCore: Sentence window chunking is an advanced chunking strategy in which each chunk\nWindow: enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n\n--- Chunk 33 ---\nCore: contains a target sentence along with a configurable number of surrounding\nWindow: --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\n\n--- Chunk 34 ---\nCore: “window” sentences as context.\nWindow: Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers.\n\n--- Chunk 35 ---\nCore: This approach aims to provide rich local context to the LLM during retrieval,\nWindow: contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\n\n--- Chunk 36 ---\nCore: thereby improving the coherence and factual consistency of generated answers.\nWindow: “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries.\n\n--- Chunk 37 ---\nCore: Semantic chunking, on the other hand, attempts to split text based on semantic\nWindow: This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n\n--- Chunk 38 ---\nCore: content rather than relying solely on fixed character counts or sentence boundaries.\nWindow: thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\n--- Chunk 39 ---\nCore: It leverages embedding models to compute semantic similarity between sentences\nWindow: Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\n\n--- Chunk 40 ---\nCore: or phrases and identify natural breakpoints where topics or meanings shift.\nWindow: content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications.\n\n--- Chunk 41 ---\nCore: Both advanced methods can significantly improve retrieval quality and\nWindow: It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\n\n--- Chunk 42 ---\nCore: downstream generation performance in RAG applications.\nWindow: or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\n--- Chunk 43 ---\nCore: Choosing the right chunking strategy typically depends on the characteristics\nWindow: Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\n--- Chunk 44 ---\nCore: of the data and the expected query types.\nWindow: downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3029\nCore: Retrieval-Augmented Generation (RAG) is a common architecture for building\nWindow:\nRetrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n\n[2] score=0.3936\nCore: Sentence window chunking is an advanced chunking strategy in which each chunk\nWindow:\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n\n[3] score=0.4610\nCore: Semantic chunking, on the other hand, attempts to split text based on semantic\nWindow:\nThis approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n\n[4] score=0.4851\nCore: document ingestion, text chunking, embedding generation, vector indexing,\nWindow:\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n\n[5] score=0.5718\nCore: vectors to retrieve the most relevant chunks.\nWindow:\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion: This involves collecting and processing documents from various sources.\n2. Text chunking: This is the process of splitting text into smaller chunks, which can be done using different strategies such as sentence window chunking or semantic chunking.\n3. Embedding generation: This involves converting the text chunks into numerical vectors that can be used for comparison.\n4. Vector indexing: This involves storing the embedded vectors in an index for efficient retrieval.\n5. Retrieval: This involves comparing the user query with the indexed vectors to retrieve the most relevant chunks.\n6. Response synthesis: This involves using the retrieved chunks as context to generate answers with a Large Language Model (LLM).\n\nSentence window chunking and semantic chunking differ in their approach to splitting text into chunks. Sentence window chunking involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding sentences as context. Semantic chunking, on the other hand, involves splitting text based on semantic content, using embedding models to compute semantic similarity between sentences.\n\nSentence Window Split (window_size=2) done.\n============================================================\n\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> origin/main
   "source": [
    "# -----------------------\n",
    "# 5) Example document + question (English version)\n",
    "# -----------------------\n",
    "documents = [\n",
    "    Document(page_content=\"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
    "LLM-powered applications that combine external knowledge retrieval with\n",
    "text generation.\n",
    "Instead of relying solely on a model’s internal parameters, RAG systems\n",
    "retrieve relevant information from external data sources and use it as\n",
    "context during answer generation.\n",
    "This approach is widely used for applications such as question answering,\n",
    "knowledge assistants, and domain-specific search.\n",
    "\n",
    "A typical RAG pipeline consists of several core components, including\n",
    "document ingestion, text chunking, embedding generation, vector indexing,\n",
    "retrieval, and response synthesis.\n",
    "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
    "are first ingested and preprocessed.\n",
    "They are then split into smaller units, often referred to as chunks,\n",
    "to make retrieval more precise and efficient.\n",
    "\n",
    "Each chunk is converted into a vector representation using an embedding model\n",
    "and stored in a vector index.\n",
    "During inference, a user query is embedded and compared against the indexed\n",
    "vectors to retrieve the most relevant chunks.\n",
    "These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
    "allowing the model to generate answers that are grounded in external knowledge.\n",
    "\n",
    "--- The following content is less directly related to the RAG topic ---\n",
    "\n",
    "In addition, Python, as a general-purpose programming language, is widely used\n",
    "in the AI field due to its simplicity and rich ecosystem.\n",
    "For example, NumPy and Pandas are foundational tools for data processing,\n",
    "providing powerful capabilities for numerical computation and structured data.\n",
    "Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
    "for tasks such as classification, regression, and clustering.\n",
    "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
    "enabling efficient development and deployment of complex AI models.\n",
    "\n",
    "--- The following is another related but conceptually independent section ---\n",
    "\n",
    "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
    "contains a target sentence along with a configurable number of surrounding\n",
    "“window” sentences as context.\n",
    "This approach aims to provide rich local context to the LLM during retrieval,\n",
    "thereby improving the coherence and factual consistency of generated answers.\n",
    "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
    "content rather than relying solely on fixed character counts or sentence boundaries.\n",
    "It leverages embedding models to compute semantic similarity between sentences\n",
    "or phrases and identify natural breakpoints where topics or meanings shift.\n",
    "Both advanced methods can significantly improve retrieval quality and\n",
    "downstream generation performance in RAG applications.\n",
    "Choosing the right chunking strategy typically depends on the characteristics\n",
    "of the data and the expected query types.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "question = (\n",
    "    \"What are the main components of a Retrieval-Augmented Generation (RAG) system, \"\n",
    "    \"and how do sentence window chunking and semantic chunking differ?\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Run splitters\n",
    "# -----------------------\n",
    "# Token-based split (chunk_size=30, overlap=0)\n",
    "splitter_a = TokenTextSplitter(chunk_size=30, chunk_overlap=0)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=0)\",\n",
    "    split_fn=lambda: splitter_a.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Token-based split (chunk_size=30, overlap=10)\n",
    "splitter_b = TokenTextSplitter(chunk_size=30, chunk_overlap=10)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=10)\",\n",
    "    split_fn=lambda: splitter_b.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Sentence-window split (window_size=2)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Sentence Window Split (window_size=2)\",\n",
    "    split_fn=lambda: sentence_window_splitter(documents, window_size=2),\n",
    "    question=question,\n",
    "    use_window_metadata=True,   # feed metadata['window'] into LLM context\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d162956-a99a-4eb3-abcb-8ebe80c177f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Tokens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
<<<<<<< HEAD
}
=======
}
>>>>>>> origin/main
