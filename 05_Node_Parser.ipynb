{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8901e340-f23c-4263-8dcc-8e887f407be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain langchain-community langchain-databricks faiss-cpu tiktoken langchain_text_splitters langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b641c7-0eb8-4e2f-8d17-20c47d967d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef54e8-4f3d-42ab-b96f-b60ca42326f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcda38ce-d178-418a-9f3e-66c0df7ecdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b1f725-60ba-4c28-b19e-21107de2b062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1) Databricks LLM + Embeddings\n",
    "# -----------------------\n",
    "# Make sure your Databricks auth is configured (e.g., DATABRICKS_HOST + DATABRICKS_TOKEN)\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"  # <-- change to your embedding endpoint name\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b03608-cbeb-476e-8ee0-28e4db57563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\", \".\", \"\"],\n",
    ")\n",
    "\n",
    "text = \"\"\"\n",
    "Why does chunking cause context fragmentation?\n",
    "\n",
    "1. Loss of Coherence\n",
    "When a complete semantic unit is forcibly split, the information becomes incomplete\n",
    "(the meaning is broken). For example, if an argument is distributed across two chunks,\n",
    "neither chunk alone can accurately convey the original meaning. This interferes with\n",
    "the language model’s understanding and generation, leading to incomplete or even\n",
    "misleading outputs.\n",
    "\n",
    "2. Diluted Relevance\n",
    "If a chunk mixes relevant and irrelevant content, the key information becomes diluted.\n",
    "This negatively affects the accuracy of vector representations and, in turn, lowers\n",
    "retrieval ranking performance.\n",
    "\n",
    "3. Scattered Information\n",
    "For complex questions that require multi-hop reasoning, relevant information may be\n",
    "scattered across multiple chunks. If not all of them are retrieved, a RAG system cannot\n",
    "produce a complete answer.\n",
    "\n",
    "When these issues compound, they directly lead to a “garbage in, garbage out” effect,\n",
    "and may even increase the risk of model hallucinations.\n",
    "\"\"\"\n",
    "\n",
    "# Text to be processed\n",
    "documents = text_splitter.create_documents([text])\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d48160-7008-4c0b-81e6-1c26924fe02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a46cdd-8ca8-421c-acc2-b26138b51c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LangChain equivalent of LlamaIndex SemanticSplitterNodeParser\n",
    "# pip install -U langchain-text-splitters langchain-openai\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Example text containing multiple topics\n",
    "multi_theme_text = (\n",
    "    \"Artificial intelligence (AI) is fundamentally transforming the healthcare industry. \"\n",
    "    \"AI algorithms can analyze medical images and diagnose diseases such as cancer earlier \"\n",
    "    \"and more accurately than human radiologists. \"\n",
    "    \"In addition, AI plays a critical role in drug discovery by predicting the effectiveness \"\n",
    "    \"of chemical compounds, significantly reducing the time required to bring new drugs to market. \"\n",
    "    \"Shifting gears, let us talk about financial technology (FinTech). \"\n",
    "    \"Mobile payments have become mainstream worldwide, with digital wallets and contactless \"\n",
    "    \"payments reshaping consumer behavior. \"\n",
    "    \"Blockchain technology provides decentralized solutions for cross-border payments and \"\n",
    "    \"asset tokenization, with the potential to reshape the underlying infrastructure of the \"\n",
    "    \"entire financial system.\"\n",
    ")\n",
    "\n",
    "# Create a LangChain Document\n",
    "docs = [Document(page_content=multi_theme_text)]\n",
    "\n",
    "# Initialize semantic chunker (closest LangChain equivalent)\n",
    "splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=90,  # roughly analogous to LlamaIndex's percentile threshold\n",
    ")\n",
    "\n",
    "# Perform semantic splitting\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Print results\n",
    "print(f\"Semantic splitting produced {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(chunk.page_content)\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e6d25a-c31d-4321-87f4-2e758647c66b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DenseXRetrievalPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5023ba3b-0b5b-4e9e-a4cc-223b418ec58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Prompt (force JSON-only)\n",
    "# -----------------------------\n",
    "PROPOSITIONS_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"You are a precise information extraction system.\n",
    "\n",
    "Task: Decompose the given Content into clear, simple propositions that are interpretable out of context.\n",
    "\n",
    "Rules:\n",
    "1) Split compound sentences into simple sentences. Keep original phrasing when possible.\n",
    "2) If a named entity has extra descriptive info, put that info into its own proposition.\n",
    "3) Decontextualize: replace pronouns with the full entity names they refer to.\n",
    "4) Output MUST be a JSON array of strings ONLY (no markdown, no extra text).\n",
    "\n",
    "Content:\n",
    "{node_text}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Robust parsing helpers\n",
    "# -----------------------------\n",
    "def _strip_code_fences(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    # Remove opening fence like ```json or ``` (any tag)\n",
    "    text = re.sub(r\"^\\s*```[a-zA-Z0-9_-]*\\s*\", \"\", text)\n",
    "    # Remove trailing fence\n",
    "    text = re.sub(r\"\\s*```\\s*$\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def _extract_first_json_array(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the first JSON array from a possibly messy response like:\n",
    "      - Output: [...]\n",
    "      - ```json [...] ```\n",
    "      - Explanation ... [...] trailing text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    text = _strip_code_fences(text)\n",
    "\n",
    "    start = text.find(\"[\")\n",
    "    end = text.rfind(\"]\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return None\n",
    "\n",
    "    return text[start : end + 1].strip()\n",
    "\n",
    "\n",
    "def _message_to_text(msg: Any) -> str:\n",
    "    \"\"\"\n",
    "    Databricks/LangChain messages can sometimes be:\n",
    "      - content: str\n",
    "      - content: list[dict] (blocks)\n",
    "    Convert robustly to plain text.\n",
    "    \"\"\"\n",
    "    if msg is None:\n",
    "        return \"\"\n",
    "\n",
    "    content = getattr(msg, \"content\", \"\")\n",
    "\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "\n",
    "    if isinstance(content, list):\n",
    "        parts: List[str] = []\n",
    "        for block in content:\n",
    "            if isinstance(block, str):\n",
    "                parts.append(block)\n",
    "            elif isinstance(block, dict):\n",
    "                # common patterns\n",
    "                if isinstance(block.get(\"text\"), str):\n",
    "                    parts.append(block[\"text\"])\n",
    "                elif isinstance(block.get(\"content\"), str):\n",
    "                    parts.append(block[\"content\"])\n",
    "        return \"\\n\".join(parts).strip()\n",
    "\n",
    "    return str(content).strip()\n",
    "\n",
    "\n",
    "def safe_json_list(text: str, *, debug: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a list[str] from model output.\n",
    "    Returns [] only if it truly can't parse a JSON array.\n",
    "    \"\"\"\n",
    "    candidate = _extract_first_json_array(text)\n",
    "    if not candidate:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] No JSON array found in output.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        data = json.loads(candidate)\n",
    "    except json.JSONDecodeError as e:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] JSONDecodeError:\", e)\n",
    "            print(\"[DEBUG] Candidate snippet:\", candidate[:800])\n",
    "        return []\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Parsed JSON is not a list:\", type(data))\n",
    "        return []\n",
    "\n",
    "    return [x for x in data if isinstance(x, str)]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Core extraction function\n",
    "# -----------------------------\n",
    "def extract_propositions(node_text: str, llm: Any, *, debug: bool = True) -> List[str]:\n",
    "    prompt = PROPOSITIONS_PROMPT.format(node_text=node_text)\n",
    "\n",
    "    msg = llm.invoke(prompt)\n",
    "    raw_text = _message_to_text(msg).strip()\n",
    "\n",
    "    props = safe_json_list(raw_text, debug=debug)\n",
    "\n",
    "    if debug and not props:\n",
    "        print(\"\\n[DEBUG] Raw model output (first 1200 chars):\")\n",
    "        print(raw_text[:1200])\n",
    "        ak = getattr(msg, \"additional_kwargs\", None)\n",
    "        if isinstance(ak, dict) and ak:\n",
    "            print(\"\\n[DEBUG] additional_kwargs keys:\", list(ak.keys()))\n",
    "\n",
    "    return props\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Test\n",
    "# -----------------------------\n",
    "test_text = \"The Eiffel Tower is located in Paris and was built in 1889.\"\n",
    "\n",
    "propositions = extract_propositions(test_text, llm, debug=True)\n",
    "print(\"\\nExtracted propositions:\")\n",
    "print(propositions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab9f3f0-f350-423a-a8a4-3d2989da7158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) In-memory sample documents (replace dir_path)\n",
    "# -----------------------------\n",
    "raw_docs = [\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"The Eiffel Tower is located in Paris, France. \"\n",
    "            \"It was constructed between 1887 and 1889 and officially opened in 1889 \"\n",
    "            \"for the World's Fair (Exposition Universelle). \"\n",
    "            \"The tower was designed by Gustave Eiffel's engineering company.\"\n",
    "        ),\n",
    "        metadata={\"source\": \"eiffel_tower\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"The Statue of Liberty is located in New York Harbor. \"\n",
    "            \"It was a gift from France to the United States and was dedicated in 1886. \"\n",
    "            \"The statue was designed by Frédéric Auguste Bartholdi.\"\n",
    "        ),\n",
    "        metadata={\"source\": \"statue_of_liberty\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} in-memory documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3179526e-4a93-4bd0-b850-1a5403ebcbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build proposition documents (DenseX-style indexing step)\n",
    "# -----------------------------\n",
    "prop_docs: List[Document] = []\n",
    "\n",
    "for d in raw_docs:\n",
    "    text = d.page_content or \"\"\n",
    "    props = extract_propositions(text, llm, debug=False)\n",
    "\n",
    "    # Fallback if the model returns non-JSON or empty\n",
    "    if not props:\n",
    "        props = [text]\n",
    "\n",
    "    for p in props:\n",
    "        prop_docs.append(\n",
    "            Document(\n",
    "                page_content=p,\n",
    "                metadata={**(d.metadata or {}), \"source_type\": \"proposition\"},\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Built {len(prop_docs)} proposition documents.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Vector index (FAISS local) using Databricks embeddings\n",
    "# -----------------------------\n",
    "vectorstore = FAISS.from_documents(prop_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Retrieval + answer using Databricks chat LLM\n",
    "# -----------------------------\n",
    "ANSWER_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the question using ONLY the context below.\n",
    "If the context is insufficient, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(f\"- {d.page_content}\" for d in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | ANSWER_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"When was the Eiffel Tower built?\"\n",
    "answer = qa_chain.invoke(query)\n",
    "\n",
    "print(\"\\n=== Answer ===\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d02df87-b3d3-4437-b2e4-8652ffa99d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Node_Parser",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
