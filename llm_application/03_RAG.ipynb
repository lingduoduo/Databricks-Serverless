{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c34d04d-11a4-40fc-b7d8-b6904cf6e1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-langchain==0.12.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (0.12.1)\nRequirement already satisfied: langchain==1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (1.2.0)\nRequirement already satisfied: langchain-community==0.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (0.4.1)\nRequirement already satisfied: langchain-openai==1.1.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (1.1.6)\nRequirement already satisfied: faiss-cpu in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (1.13.2)\nRequirement already satisfied: sentence-transformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (5.2.0)\nRequirement already satisfied: langchain-classic in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (1.0.1)\nRequirement already satisfied: rank_bm25 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (0.2.2)\nRequirement already satisfied: transformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (4.57.3)\nRequirement already satisfied: scikit-learn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (1.8.0)\nRequirement already satisfied: spacy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (3.8.11)\nRequirement already satisfied: databricks-ai-bridge>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (0.11.0)\nRequirement already satisfied: databricks-mcp>=0.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (0.5.1)\nRequirement already satisfied: databricks-sdk>=0.65.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (0.76.0)\nRequirement already satisfied: databricks-vectorsearch>=0.50 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (0.63)\nRequirement already satisfied: langchain-mcp-adapters>=0.1.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (0.2.1)\nRequirement already satisfied: mlflow>=2.20.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (3.8.1)\nRequirement already satisfied: openai>=1.99.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (2.14.0)\nRequirement already satisfied: pydantic>2.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-langchain==0.12.1) (2.12.5)\nRequirement already satisfied: unitycatalog-langchain>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (0.3.0)\nRequirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain==1.2.0) (1.2.5)\nRequirement already satisfied: langgraph<1.1.0,>=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain==1.2.0) (1.0.5)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (2.0.45)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.11/site-packages (from langchain-community==0.4.1) (6.0)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (3.13.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.11/site-packages (from langchain-community==0.4.1) (8.2.2)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (2.12.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (0.5.2)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (0.4.3)\nRequirement already satisfied: numpy>=1.26.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-community==0.4.1) (1.26.4)\nRequirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-openai==1.1.6) (0.12.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from faiss-cpu) (23.2)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from sentence-transformers) (2.9.1)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-classic) (1.1.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from transformers) (3.13.4)\nRequirement already satisfied: regex!=2019.12.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: joblib>=1.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (1.0.15)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (2.0.13)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (8.3.10)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (2.5.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (0.4.3)\nRequirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (0.21.0)\nRequirement already satisfied: jinja2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.11/site-packages (from spacy) (75.1.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1) (1.22.0)\nRequirement already satisfied: mlflow-skinny>=2.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (3.8.1)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (1.5.3)\nRequirement already satisfied: tabulate>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (0.9.0)\nRequirement already satisfied: mcp>=1.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (1.25.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk>=0.65.0->databricks-langchain==0.12.1) (2.35.0)\nRequirement already satisfied: protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-sdk>=0.65.0->databricks-langchain==0.12.1) (5.29.5)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from databricks-vectorsearch>=0.50->databricks-langchain==0.12.1) (2.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (3.26.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (0.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (1.33)\nRequirement already satisfied: uuid-utils<1.0,>=0.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (0.12.0)\nRequirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (3.0.1)\nRequirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (1.0.5)\nRequirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (0.3.1)\nRequirement already satisfied: xxhash>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (3.6.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (3.11.5)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (0.23.0)\nRequirement already satisfied: mlflow-tracing==3.8.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (3.8.1)\nRequirement already satisfied: Flask-CORS<7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (6.0.2)\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (3.1.2)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (1.17.2)\nRequirement already satisfied: cryptography<47,>=43.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (46.0.3)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (7.1.0)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (23.0.0)\nRequirement already satisfied: huey<3,>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (2.5.5)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (3.7.2)\nRequirement already satisfied: pyarrow<23,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow>=2.20.1->databricks-langchain==0.12.1) (14.0.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (8.3.1)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (3.0.0)\nRequirement already satisfied: fastapi<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (0.128.0)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (1.39.1)\nRequirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (1.39.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (1.39.1)\nRequirement already satisfied: python-dotenv<2,>=0.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (1.2.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (0.5.1)\nRequirement already satisfied: uvicorn<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (0.40.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from openai>=1.99.9->databricks-langchain==0.12.1) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain==0.12.1) (1.7.0)\nRequirement already satisfied: jiter<1,>=0.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from openai>=1.99.9->databricks-langchain==0.12.1) (0.12.0)\nRequirement already satisfied: sniffio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from openai>=1.99.9->databricks-langchain==0.12.1) (1.3.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from pydantic>2.10.0->databricks-langchain==0.12.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from pydantic>2.10.0->databricks-langchain==0.12.1) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from pydantic>2.10.0->databricks-langchain==0.12.1) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4.1) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4.1) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4.1) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.5->langchain-community==0.4.1) (2023.7.22)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community==0.4.1) (3.3.0)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nRequirement already satisfied: sympy>=1.13.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\nRequirement already satisfied: unitycatalog-ai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (0.3.2)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from jinja2->spacy) (3.0.3)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow>=2.20.1->databricks-langchain==0.12.1) (1.3.10)\nRequirement already satisfied: cffi>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from cryptography<47,>=43.0.0->mlflow>=2.20.1->databricks-langchain==0.12.1) (2.0.0)\nRequirement already satisfied: blinker>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (2.2.0)\nRequirement already satisfied: werkzeug>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from Flask<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (3.1.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain==0.12.1) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain==0.12.1) (4.9)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from graphene<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (3.2.7)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from graphene<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (3.2.0)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.11/site-packages (from graphene<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (2.8.2)\nRequirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community==0.4.1) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain==1.2.0) (3.0.0)\nRequirement already satisfied: ormsgpack>=1.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain==1.2.0) (1.12.1)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (10.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.20.1->databricks-langchain==0.12.1) (3.0.9)\nRequirement already satisfied: jsonschema>=4.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (4.25.1)\nRequirement already satisfied: pyjwt>=2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (2.10.1)\nRequirement already satisfied: python-multipart>=0.0.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (0.0.21)\nRequirement already satisfied: sse-starlette>=1.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (3.1.1)\nRequirement already satisfied: starlette>=0.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (0.50.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (2022.7)\nRequirement already satisfied: wrapt in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1) (0.4.3)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.11/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (1.5.6)\nRequirement already satisfied: unitycatalog-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (0.3.1)\nRequirement already satisfied: databricks-connect<17.1,>=15.1.0 in /databricks/python3/lib/python3.11/site-packages (from unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (15.4.15)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow>=2.20.1->databricks-langchain==0.12.1) (2.21)\nRequirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (1.69.0)\nRequirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (1.69.0)\nRequirement already satisfied: py4j==0.10.9.7 in /databricks/python3/lib/python3.11/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (0.10.9.7)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (1.16.0)\nRequirement already satisfied: annotated-doc>=0.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from fastapi<1->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (0.0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (3.11.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain==0.12.1) (0.30.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (0.60b1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain==0.12.1) (0.4.8)\nRequirement already satisfied: aiohttp-retry>=2.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-622728fd-db06-44b1-b7e4-f7c2d5ad8faf/lib/python3.11/site-packages (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain==0.12.1) (2.9.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny>=2.19.0->databricks-ai-bridge>=0.4.2->databricks-langchain==0.12.1) (5.0.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U databricks-langchain==0.12.1 langchain==1.2.0 langchain-community==0.4.1 langchain-openai==1.1.6 faiss-cpu sentence-transformers langchain-classic rank_bm25 transformers scikit-learn spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c4878a-34a0-466d-8a77-34d802a685b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7057b129-723a-4dea-b3b6-d452091ceff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\r\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/12.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m134.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25h\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\nYou can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd59dec6-63bd-4173-8dfa-39d1e95d5c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Foundation Terminology\n",
    "\n",
    "Foundation\tTerminology glossary construction, term extraction, preprocessing standardization, term embeddings and vector indexing\n",
    "\n",
    "- NER, TF-IDF, KeyBERT\n",
    "- Term Normalization\n",
    "- Text Standardization\n",
    "- Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc94837a-86f2-4ff8-b1bf-c40fbcbcd8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define a terminology glossary for RAG variants (keep it updated, including context tags)\n",
    "GLOSSARY = [\n",
    "    {\n",
    "    \"term\": \"Retrieval-Augmented Generation\",\n",
    "    \"synonyms\": [\"RAG\", \"retrieval augmented generation\", \"retrieval-augmented generation\"],\n",
    "    \"definition\": \"A generation framework that retrieves external evidence and conditions an LLM on it.\",\n",
    "    \"context_tags\": [\"LLM\", \"search\", \"grounding\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Self-RAG\",\n",
    "        \"synonyms\": [\"Self RAG\", \"self-rag\", \"self-reflective RAG\", \"self-refining RAG\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach where the model explicitly self-checks (e.g., reflect, critique, verify) during generation, \"\n",
    "            \"deciding when to retrieve more evidence and how to revise its answer based on feedback signals.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"reflection\", \"self-critique\", \"verification\", \"iterative retrieval\", \"hallucination mitigation\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Corrective RAG\",\n",
    "        \"synonyms\": [\"C-RAG\", \"CRAG\", \"Corrective-RAG\", \"corrective rag\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach that detects low-quality retrieval or unsupported generations and applies corrective actions \"\n",
    "            \"(e.g., re-retrieve, rewrite queries, filter evidence, or re-rank) to improve factual grounding.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"retrieval quality\", \"re-ranking\", \"query rewriting\", \"evidence filtering\", \"robustness\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Knowledge Graph RAG\",\n",
    "        \"synonyms\": [\"KG-RAG\", \"KGRAG\", \"KG RAG\", \"knowledge-graph RAG\", \"graph RAG\"],\n",
    "        \"definition\": (\n",
    "            \"A RAG approach that retrieves and reasons over a knowledge graph (entities/relations) as structured evidence, \"\n",
    "            \"often combining graph traversal with text retrieval to support multi-hop and relational questions.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"knowledge graph\", \"multi-hop reasoning\", \"entity linking\", \"graph traversal\", \"structured grounding\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Entity Linking\",\n",
    "        \"synonyms\": [\"EL\", \"entity resolution\", \"mention linking\", \"entity disambiguation\"],\n",
    "        \"definition\": (\n",
    "            \"The process of mapping a text mention (e.g., 'Apple') to a canonical entity (e.g., Apple Inc.) in a KB/KG \"\n",
    "            \"to support structured retrieval and reduce ambiguity.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"KG-RAG\", \"disambiguation\", \"knowledge base\", \"information extraction\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Evidence Grounding\",\n",
    "        \"synonyms\": [\"grounding\", \"source grounding\", \"evidence-based generation\"],\n",
    "        \"definition\": (\n",
    "            \"Constraining or evaluating generation based on retrieved evidence so claims are supported by sources; \"\n",
    "            \"commonly paired with citation, attribution, or entailment checks.\"\n",
    "        ),\n",
    "        \"context_tags\": [\"factuality\", \"citations\", \"attribution\", \"hallucination mitigation\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1e02a5-3aaa-4574-aa36-cda5a7526de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class TerminologyProcessor:\n",
    "    def __init__(self, glossary: List[Dict[str, Any]]):\n",
    "        self.glossary = glossary\n",
    "        self.standard_term_map = {}\n",
    "        self.alias_to_entries_map = {}\n",
    "        self._build_mappings()\n",
    "\n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Build mappings; one alias may map to multiple terminology entries to handle ambiguity.\"\"\"\n",
    "        for entry in self.glossary:\n",
    "            standard_term = entry[\"term\"]\n",
    "            self.standard_term_map[standard_term.lower()] = standard_term\n",
    "\n",
    "            all_aliases = [standard_term] + entry.get(\"synonyms\", [])\n",
    "            for alias in all_aliases:\n",
    "                alias_lower = alias.lower()\n",
    "                if alias_lower not in self.alias_to_entries_map:\n",
    "                    self.alias_to_entries_map[alias_lower] = []\n",
    "                if entry not in self.alias_to_entries_map[alias_lower]:\n",
    "                    self.alias_to_entries_map[alias_lower].append(entry)\n",
    "\n",
    "    def standardize_text(self, text: str, context_window: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Context-aware terminology standardization using iteration + a replacement function.\n",
    "        Dynamically generates the correct regex for each term type.\n",
    "        \"\"\"\n",
    "        standardized_text = text\n",
    "        sorted_keys = sorted(self.alias_to_entries_map.keys(), key=len, reverse=True)\n",
    "\n",
    "        for key_lower in sorted_keys:\n",
    "            possible_entries = self.alias_to_entries_map[key_lower]\n",
    "\n",
    "            # --- Dynamically create the correct regex for each key ---\n",
    "            pattern_str = \"\"\n",
    "            # If key contains Latin letters, assume it's an abbreviation and enforce boundaries\n",
    "            if re.search(r\"[a-zA-Z]\", key_lower):\n",
    "                # Use lookarounds to avoid matching inside a larger word\n",
    "                pattern_str = r\"(?<![a-zA-Z])\" + re.escape(key_lower) + r\"(?![a-zA-Z])\"\n",
    "            else:\n",
    "                # For Chinese (or non-Latin) terms, match exactly\n",
    "                pattern_str = re.escape(key_lower)\n",
    "\n",
    "            pattern = re.compile(pattern_str, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replacement function called for each match\n",
    "            def replacer(match: re.Match) -> str:\n",
    "                if len(possible_entries) == 1:\n",
    "                    return possible_entries[0][\"term\"]\n",
    "                else:\n",
    "                    # --- Context-based disambiguation ---\n",
    "                    context_snippet = standardized_text[\n",
    "                        max(0, match.start() - context_window) : min(len(standardized_text), match.end() + context_window)\n",
    "                    ]\n",
    "                    for entry in possible_entries:\n",
    "                        clues = entry.get(\"context_tags\", []) + [entry[\"term\"]]\n",
    "                        if any(clue in context_snippet for clue in clues):\n",
    "                            return entry[\"term\"]\n",
    "                    # If no context clue is found, fall back to the first definition\n",
    "                    return possible_entries[0][\"term\"]\n",
    "\n",
    "            # Update text using the replacement function\n",
    "            standardized_text = pattern.sub(replacer, standardized_text)\n",
    "\n",
    "        return standardized_text\n",
    "\n",
    "    def extract_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract known standardized terms from text\n",
    "        \"\"\"\n",
    "        found_terms = set()\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for standard_term_lower, original_standard_term in self.standard_term_map.items():\n",
    "            # Direct substring search; do not use \\b\n",
    "            if re.search(re.escape(standard_term_lower), text_lower):\n",
    "                found_terms.add(original_standard_term)\n",
    "\n",
    "        return sorted(list(found_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536b7fdd-1949-44f0-96f5-84ab573841de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Initialize the terminology processor with the glossary ---\n--- 2. Data Preprocessing: Terminology Standardization ---\nOriginal query: I want to learn about applications of using RAG.\nStandardized query: I want to learn about applications of using Retrieval-Augmented Generation.\nOriginal document: Recently I studied Self-RAG.\nStandardized document: Recently I studied Self-Retrieval-Augmented Generation.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the terminology processor with the glossary.\n",
    "term_processor = TerminologyProcessor(GLOSSARY)\n",
    "print(\"--- 1. Initialize the terminology processor with the glossary ---\")\n",
    "\n",
    "# 2. Data preprocessing: terminology standardization\n",
    "print(\"--- 2. Data Preprocessing: Terminology Standardization ---\")\n",
    "user_query = \"I want to learn about applications of using RAG.\"\n",
    "processed_query = term_processor.standardize_text(user_query)\n",
    "print(f\"Original query: {user_query}\")\n",
    "print(f\"Standardized query: {processed_query}\")\n",
    "\n",
    "document_text = \"Recently I studied Self-RAG.\"\n",
    "processed_document = term_processor.standardize_text(document_text)\n",
    "print(f\"Original document: {document_text}\")\n",
    "print(f\"Standardized document: {processed_document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90cb5524-aa4f-4f61-a72e-86354d8b23be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- 3. Term Extraction ---\nExtracted terms from query: ['Retrieval-Augmented Generation']\nExtracted terms from document: ['Retrieval-Augmented Generation']\n"
     ]
    }
   ],
   "source": [
    "# 3. Term extraction (for downstream vectorization or metadata tagging)\n",
    "print(\"\\n--- 3. Term Extraction ---\")\n",
    "extracted_terms_query = term_processor.extract_terms(processed_query)\n",
    "print(f\"Extracted terms from query: {extracted_terms_query}\")\n",
    "\n",
    "extracted_terms_document = term_processor.extract_terms(processed_document)\n",
    "print(f\"Extracted terms from document: {extracted_terms_document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb383f00-6742-44f8-8e86-93aa29176eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\nIn a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\nThese vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\nDuring retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\n"
     ]
    }
   ],
   "source": [
    "# 4. Simulated vector storage and retrieval augmentation (conceptual)\n",
    "print(\"\\n--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\")\n",
    "print(\"In a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\")\n",
    "print(\"These vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\")\n",
    "print(\"During retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acedac0-bd4b-4e04-9ed3-f98dafde0487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Simulated retrieval augmentation: query expansion\n",
    "def enhance_query_for_retrieval(query: str, processor: TerminologyProcessor) -> List[str]:\n",
    "    \"\"\"Expand query keywords using the terminology glossary to improve recall.\"\"\"\n",
    "    standardized_query = processor.standardize_text(query)\n",
    "    query_terms = processor.extract_terms(standardized_query)\n",
    "\n",
    "    expanded_keywords = set([standardized_query])\n",
    "    for term in query_terms:\n",
    "        expanded_keywords.add(term)\n",
    "        for entry in processor.glossary:\n",
    "            if entry[\"term\"] == term:\n",
    "                for synonym in entry.get(\"synonyms\", []):\n",
    "                    expanded_keywords.add(synonym)\n",
    "                break\n",
    "    return sorted(list(expanded_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fcd4d7d-5999-4454-aeb4-77fe3481ff18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- 5. Simulated Retrieval Augmentation: Query Expansion ---\nOriginal retrieval query: I want to know what a RAG does in an LLM?\nExpanded retrieval keyword list: ['I want to know what a Retrieval-Augmented Generation does in an LLM?', 'RAG', 'Retrieval-Augmented Generation', 'retrieval augmented generation', 'retrieval-augmented generation']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 5. Simulated Retrieval Augmentation: Query Expansion ---\")\n",
    "original_query_for_retrieval = \"I want to know what a RAG does in an LLM?\"\n",
    "expanded_keywords = enhance_query_for_retrieval(original_query_for_retrieval, term_processor)\n",
    "print(f\"Original retrieval query: {original_query_for_retrieval}\")\n",
    "print(f\"Expanded retrieval keyword list: {expanded_keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d720cb0e-c3e8-4561-979b-3b34216da435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Detect Synonyms to Extend Terminology Glossary\n",
    "\n",
    "Detect Synonyms by Similarity\n",
    "\n",
    "- FAISS\n",
    "- Legal-BERT, ChatLaw-Text2Vec\n",
    "- Sentence Transformers + PEFT (LoRA) Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3845aba8-c509-43a7-9dc1-7f15aaf08c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "DASHES = r\"[-\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212]\"  # common dash chars\n",
    "\n",
    "def alias_to_pattern(alias: str):\n",
    "    # If alias contains a dash, match it as its own token with regex\n",
    "    if re.search(DASHES, alias):\n",
    "        # split on dash and keep the two sides\n",
    "        left, right = re.split(DASHES, alias, maxsplit=1)\n",
    "        left_tokens = left.strip().split()\n",
    "        right_tokens = right.strip().split()\n",
    "        return [{\"LOWER\": t.lower()} for t in left_tokens] + [{\"TEXT\": {\"REGEX\": DASHES}}] + [{\"LOWER\": t.lower()} for t in right_tokens]\n",
    "    else:\n",
    "        # normal phrase: token-by-token, case-insensitive\n",
    "        return [{\"LOWER\": t.lower()} for t in alias.strip().split()]\n",
    "\n",
    "def extract_terms_with_ruler(text, glossary):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # avoid adding duplicate pipes if you call this multiple times\n",
    "    if \"entity_ruler\" in nlp.pipe_names:\n",
    "        ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "        ruler.clear()\n",
    "    else:\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True})\n",
    "\n",
    "    patterns = []\n",
    "    for entry in glossary:\n",
    "        for alias in [entry.get(\"term\")] + entry.get(\"synonyms\", []):\n",
    "            if not alias:\n",
    "                continue\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": alias_to_pattern(alias)})\n",
    "\n",
    "    # prefer longer patterns first (helps with overlaps like KG-RAG vs RAG)\n",
    "    patterns.sort(key=lambda p: len(p[\"pattern\"]), reverse=True)\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ == \"TERM\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec190888-03e9-4a42-b658-1cb73d695f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Knowledge Graph Retrieval-Augmented Generation\nAutomatically extracted term candidates: {'Retrieval-Augmented Generation'}\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "query = \"Examples of KG-RAG\" \n",
    "processed_query = term_processor.standardize_text(query)\n",
    "print(processed_query)\n",
    "# processed_query = \"Examples of Knowledge Graph Retrieval-Augmented Generation\"\n",
    "candidates = extract_terms_with_ruler(processed_query, GLOSSARY)\n",
    "print(f\"Automatically extracted term candidates: {candidates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76216f23-819f-492c-806f-405888b6d542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Term Glossary Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c0903a-4f79-43cd-910c-af9fbcff75e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# It is recommended to load the model once during project initialization\n",
    "# to avoid repeated loading overhead.\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def map_synonyms_by_similarity(main_terms: list, candidates: list, threshold: float = 0.8) -> dict:\n",
    "    \"\"\"\n",
    "    Map candidate terms to the closest standard terms by computing\n",
    "    cosine similarity between embeddings.\n",
    "\n",
    "    Args:\n",
    "        main_terms (list): List of standard (canonical) terms.\n",
    "        candidates (list): List of candidate synonyms to be matched.\n",
    "        threshold (float): Similarity threshold above which a candidate\n",
    "                           is considered a synonym.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each standard term to a list of\n",
    "              successfully matched synonyms.\n",
    "    \"\"\"\n",
    "    _matched_synonyms = {term: [] for term in main_terms}\n",
    "\n",
    "    if not main_terms or not candidates:\n",
    "        return _matched_synonyms\n",
    "    \n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode in batches for better efficiency\n",
    "    embeddings = model.encode(main_terms + candidates, convert_to_tensor=True)\n",
    "    term_embeddings = embeddings[:len(main_terms)]\n",
    "    candidate_embeddings = embeddings[len(main_terms):]\n",
    "\n",
    "    # Compute the cosine similarity matrix between standard terms and candidates\n",
    "    similarity_matrix = util.cos_sim(term_embeddings, candidate_embeddings)\n",
    "\n",
    "    for i, term in enumerate(main_terms):\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                _matched_synonyms[term].append(candidate)\n",
    "\n",
    "    return _matched_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d2ce6b-14ff-4cc3-b278-75e8ad475a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nOptimized matched synonyms: {'RAG': ['Corrective RAG', 'RAG', 'Corrective-RAG', 'corrective rag']}\n"
     ]
    }
   ],
   "source": [
    "main_terms_to_map = [\"RAG\"]\n",
    "all_possible_synonyms = [\n",
    "    entry[\"term\"]\n",
    "    for entry in GLOSSARY\n",
    "] + [\n",
    "    synonym\n",
    "    for entry in GLOSSARY\n",
    "    for synonym in entry.get(\"synonyms\", [])\n",
    "]\n",
    "optimized_mapped_synonyms = map_synonyms_by_similarity(\n",
    "    main_terms_to_map,\n",
    "    all_possible_synonyms\n",
    ")\n",
    "print(\"\\nOptimized matched synonyms:\", optimized_mapped_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c4bc8a-3433-4b03-973a-e701fe3b7de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAttempting to load model 'paraphrase-MiniLM-L6-v2'...\n"
     ]
    }
   ],
   "source": [
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "print(f\"\\nAttempting to load model '{model_name}'...\")\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988f01b0-9ae4-411f-ab42-ceaa81172673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "\n",
    "def build_term_vector_index(\n",
    "    term_glossary: Dict[str, dict],\n",
    "    model: SentenceTransformer,\n",
    "    use_cosine: bool = True\n",
    ") -> Tuple[faiss.Index, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert all terms and their synonyms into vector embeddings and build a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        term_glossary (dict):\n",
    "            A structured glossary where keys are canonical terms and values contain\n",
    "            a 'synonyms' list.\n",
    "        model (SentenceTransformer):\n",
    "            A loaded SentenceTransformer model.\n",
    "        use_cosine (bool):\n",
    "            Whether to use cosine similarity (recommended for sentence embeddings).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            (faiss_index, indexed_terms)\n",
    "            - faiss_index: FAISS index containing all embeddings\n",
    "            - indexed_terms: list of terms aligned with index rows\n",
    "    \"\"\"\n",
    "    terms_to_index: List[str] = []\n",
    "\n",
    "    # Collect canonical terms and synonyms\n",
    "    for canonical_term, info in term_glossary.items():\n",
    "        terms_to_index.append(canonical_term)\n",
    "        synonyms = info.get(\"synonyms\", [])\n",
    "        if isinstance(synonyms, list):\n",
    "            terms_to_index.extend(synonyms)\n",
    "\n",
    "    # Deduplicate while keeping deterministic order\n",
    "    indexed_terms = sorted(set(terms_to_index))\n",
    "    if not indexed_terms:\n",
    "        raise ValueError(\"No terms found in glossary.\")\n",
    "\n",
    "    print(\"Generating term embeddings...\")\n",
    "    embeddings = model.encode(\n",
    "        indexed_terms,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine,\n",
    "        show_progress_bar=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    # Choose FAISS index type\n",
    "    if use_cosine:\n",
    "        # Cosine similarity = inner product on normalized vectors\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    metric = \"cosine similarity\" if use_cosine else \"L2 distance\"\n",
    "    print(f\"FAISS index built successfully. \"\n",
    "          f\"Vectors: {index.ntotal}, Dimension: {dim}, Metric: {metric}\")\n",
    "\n",
    "    return index, indexed_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998f2c82-af56-447d-a842-203d4df24253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def convert_glossary_to_term_dict(\n",
    "    glossary: List[Dict[str, Any]],\n",
    "    deduplicate: bool = True,\n",
    "    keep_term_as_synonym: bool = False\n",
    ") -> Dict[str, Dict[str, list]]:\n",
    "    \"\"\"\n",
    "    Convert list-based glossary into FAISS-friendly dict format.\n",
    "\n",
    "    Args:\n",
    "        glossary: original GLOSSARY list\n",
    "        deduplicate: remove duplicate synonyms\n",
    "        keep_term_as_synonym: include canonical term in synonyms or not\n",
    "\n",
    "    Returns:\n",
    "        dict: {canonical_term: {\"synonyms\": [...]}}\n",
    "    \"\"\"\n",
    "    term_dict = {}\n",
    "\n",
    "    for entry in glossary:\n",
    "        term = entry.get(\"term\")\n",
    "        if not term:\n",
    "            continue\n",
    "\n",
    "        synonyms = entry.get(\"synonyms\", [])\n",
    "        if not isinstance(synonyms, list):\n",
    "            synonyms = []\n",
    "\n",
    "        if keep_term_as_synonym:\n",
    "            synonyms = [term] + synonyms\n",
    "\n",
    "        if deduplicate:\n",
    "            # preserve order, remove duplicates\n",
    "            seen = set()\n",
    "            synonyms = [s for s in synonyms if not (s in seen or seen.add(s))]\n",
    "\n",
    "        term_dict[term] = {\"synonyms\": synonyms}\n",
    "\n",
    "    return term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2e6c74-2602-49c3-800d-75623061283c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Retrieval-Augmented Generation': {'synonyms': ['RAG', 'retrieval augmented generation', 'retrieval-augmented generation']}, 'Self-RAG': {'synonyms': ['Self RAG', 'self-rag', 'self-reflective RAG', 'self-refining RAG']}, 'Corrective RAG': {'synonyms': ['C-RAG', 'CRAG', 'Corrective-RAG', 'corrective rag']}, 'Knowledge Graph RAG': {'synonyms': ['KG-RAG', 'KGRAG', 'KG RAG', 'knowledge-graph RAG', 'graph RAG']}, 'Entity Linking': {'synonyms': ['EL', 'entity resolution', 'mention linking', 'entity disambiguation']}, 'Evidence Grounding': {'synonyms': ['grounding', 'source grounding', 'evidence-based generation']}}\n"
     ]
    }
   ],
   "source": [
    "term_glossary = convert_glossary_to_term_dict(GLOSSARY)\n",
    "print(term_glossary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a447ede-dd84-4e09-9f3f-aedad8fe577e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating term embeddings...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6b3140336e413fafea1d97a4817dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built successfully. Vectors: 29, Dimension: 384, Metric: cosine similarity\n"
     ]
    }
   ],
   "source": [
    "index, indexed_terms = build_term_vector_index(\n",
    "    term_glossary=term_glossary,\n",
    "    model=model,\n",
    "    use_cosine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e6ec73-8944-44fb-ba7b-6737d9d052f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def search_terms(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    indexed_terms: List[str],\n",
    "    top_k: int = 5,\n",
    "    use_cosine: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the most similar terms to a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query text.\n",
    "        model (SentenceTransformer): SentenceTransformer model.\n",
    "        index (faiss.Index): FAISS index.\n",
    "        indexed_terms (list): Terms aligned with index rows.\n",
    "        top_k (int): Number of results to return.\n",
    "        use_cosine (bool): Whether cosine similarity is used.\n",
    "\n",
    "    Returns:\n",
    "        list of (term, score) tuples.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        results.append((indexed_terms[idx], float(score)))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f8772c-e834-432b-8902-b23c13e6e864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Index Build Successful ---\nNumber of vectors in FAISS index: 29\nIndexed terms: ['C-RAG', 'CRAG', 'Corrective RAG', 'Corrective-RAG', 'EL', 'Entity Linking', 'Evidence Grounding', 'KG RAG', 'KG-RAG', 'KGRAG', 'Knowledge Graph RAG', 'RAG', 'Retrieval-Augmented Generation', 'Self RAG', 'Self-RAG', 'corrective rag', 'entity disambiguation', 'entity resolution', 'evidence-based generation', 'graph RAG', 'grounding', 'knowledge-graph RAG', 'mention linking', 'retrieval augmented generation', 'retrieval-augmented generation', 'self-rag', 'self-refining RAG', 'self-reflective RAG', 'source grounding']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Index Build Successful ---\")\n",
    "print(\"Number of vectors in FAISS index:\", index.ntotal)\n",
    "print(\"Indexed terms:\", indexed_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a65a4ab-3299-40ee-ae10-0d3ac861ec7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Running Retrieval ---\nQuery: 'RAG'\nResults:\n  Top 1: term='RAG', distance=8.1937 (smaller = more similar)\n  Top 2: term='corrective rag', distance=6.9808 (smaller = more similar)\n  Top 3: term='Corrective RAG', distance=6.9808 (smaller = more similar)\n\n--- Running Retrieval ---\nQuery: 'Vector Store'\nResults:\n  Top 1: term='retrieval augmented generation', distance=2.2956 (smaller = more similar)\n  Top 2: term='retrieval-augmented generation', distance=2.2796 (smaller = more similar)\n  Top 3: term='Retrieval-Augmented Generation', distance=2.2796 (smaller = more similar)\n\n--- Running Retrieval ---\nQuery: 'Language Model'\nResults:\n  Top 1: term='knowledge-graph RAG', distance=2.4766 (smaller = more similar)\n  Top 2: term='Knowledge Graph RAG', distance=2.4148 (smaller = more similar)\n  Top 3: term='evidence-based generation', distance=2.1641 (smaller = more similar)\n\n--- Running Retrieval ---\nQuery: 'Transformer model'\nResults:\n  Top 1: term='entity resolution', distance=2.0483 (smaller = more similar)\n  Top 2: term='retrieval-augmented generation', distance=1.7749 (smaller = more similar)\n  Top 3: term='Retrieval-Augmented Generation', distance=1.7749 (smaller = more similar)\n"
     ]
    }
   ],
   "source": [
    "# --- Part 2: Define our core retrieval function ---\n",
    "\n",
    "def search_similar_terms(\n",
    "    query_text: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    term_list: list,\n",
    "    k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar terms to a query text from a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The user input query term/text.\n",
    "        model (SentenceTransformer): The embedding model used to encode the query.\n",
    "        index (faiss.Index): The FAISS index object.\n",
    "        term_list (list): The term list aligned with the order of vectors in the FAISS index.\n",
    "        k (int): The number of most similar results to return.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Retrieval ---\")\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "\n",
    "    # 1) Encode the query text into an embedding vector\n",
    "    query_vector = model.encode([query_text])\n",
    "    query_vector = query_vector.astype(\"float32\")\n",
    "\n",
    "    # 2) Search in the FAISS index\n",
    "    # index.search returns two arrays: D (distances/scores) and I (indices)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    # 3) Parse and print results\n",
    "    print(\"Results:\")\n",
    "    for i in range(k):\n",
    "        idx = int(indices[0][i])\n",
    "        dist = float(distances[0][i])\n",
    "        term = term_list[idx]\n",
    "\n",
    "        # For IndexFlatL2, distance is squared Euclidean distance:\n",
    "        # smaller distance => more similar\n",
    "        print(f\"  Top {i+1}: term='{term}', distance={dist:.4f} (smaller = more similar)\")\n",
    "\n",
    "\n",
    "# 4) === Demo ===\n",
    "\n",
    "# Case 1: Query using a synonym for the canonical term\n",
    "search_similar_terms(query_text=\"RAG\", model=model, index=index, term_list=indexed_terms, k=3)\n",
    "\n",
    "# Case 2: Semantically similar query (core advantage)\n",
    "search_similar_terms(query_text=\"Vector Store\", model=model, index=index, term_list=indexed_terms, k=3)\n",
    "\n",
    "# Case 3: Query with a broader term\n",
    "# Goal: Query \"Language Model\" and see whether it matches more specific related terms (if present).\n",
    "search_similar_terms(query_text=\"Language Model\", model=model, index=index, term_list=indexed_terms, k=3)\n",
    "\n",
    "# Case 4: Tolerance to minor noise / paraphrases\n",
    "# Goal: Query \"Transformer model\" (a paraphrase) and see if it matches \"Transformer\" / \"transformer\".\n",
    "search_similar_terms(query_text=\"Transformer model\", model=model, index=index, term_list=indexed_terms, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15192026-6cc3-4adb-89dd-53f5cc22c391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Enhancement Query and Answer based on Glossary\n",
    "\n",
    "Hybrid retrieval (BM25 + vectors), query expansion (MultiQuery), hypothetical document embeddings (HyDE), cross-encoder re-ranking\n",
    "\n",
    "- Multi-Query Retriever\n",
    "- HyDE\n",
    "- Hybrid Search\n",
    "- BGE-Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5faab0a-9555-4447-ba0c-4915cd605620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Query Expansion and Rewriting\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51b6605-986a-46ae-9a33-fb3b086354c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. Prepare sample documents\n",
    "# We create some example text containing technical terminology\n",
    "doc_text = \"\"\"\n",
    "Convolutional Neural Networks (CNNs) are a key model in deep learning,\n",
    "especially effective in the field of image recognition.\n",
    "Their core idea is to automatically extract local image features\n",
    "through convolutional layers and pooling layers.\n",
    "\n",
    "Unlike CNNs, Transformer models were originally applied to\n",
    "natural language processing (NLP) tasks such as machine translation.\n",
    "Today, they have also been successfully applied to computer vision,\n",
    "known as Vision Transformers.\n",
    "\n",
    "Large Language Models (LLMs) are a major focus of current AI research.\n",
    "Based on the Transformer architecture, they are capable of\n",
    "understanding and generating human-like text,\n",
    "demonstrating strong reasoning capabilities.\n",
    "\"\"\".strip()\n",
    "\n",
    "documents = [Document(page_content=doc_text, metadata={\"source\": \"sample_tech_doc\"})]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b3057b-3ea9-4df6-a29c-a28c20b1f7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks, DatabricksEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL = \"databricks-bge-large-en\"\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_MODEL)\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41083eac-a5dc-4cec-a1dd-708c5bb68257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.multi_query import MultiQueryRetriever\n",
    "from databricks_langchain import ChatDatabricks, DatabricksEmbeddings\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.2)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436baa00-2b96-45d4-bf1b-86bf7b562fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding and generating human-like text,\ndemonstrating strong reasoning capabilities.\nnatural language processing (NLP) tasks such as machine translation.\nTheir core idea is to automatically extract local image features\nthrough convolutional layers and pooling layers.\nespecially effective in the field of image recognition.\nConvolutional Neural Networks (CNNs) are a key model in deep learning,\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a RAG?\"\n",
    "retrieved_docs = retriever_from_llm.invoke(query)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dbf9d32-0860-40a8-a503-43c499cae8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hybrid retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4bafa7-b69f-4e07-b622-c3ade280f102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 retriever built successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 3\n",
    "print(\"BM25 retriever built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1c9025-0da9-4346-b003-c9eb55c82319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nInitializing MergerRetriever...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing MergerRetriever...\")\n",
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever_list = [bm25_retriever, faiss_retriever]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ede895-57c3-42f8-9579-04e736b174ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MergerRetriever initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.retrievers import MergerRetriever\n",
    "# MergerRetriever handles parallel retrieval and deduplication\n",
    "merged_retriever = MergerRetriever(retrievers=retriever_list)\n",
    "print(\"MergerRetriever initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49aa55ac-738b-4218-bb05-5458dc558483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nInitializing MergerRetriever...\nMergerRetriever initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing MergerRetriever...\")\n",
    "retriever_list = [bm25_retriever, faiss_retriever]\n",
    "# MergerRetriever handles parallel retrieval and deduplication\n",
    "merged_retriever = MergerRetriever(retrievers=retriever_list)\n",
    "print(\"MergerRetriever initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c128bd-633a-4888-9004-75118a02926e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n--- Running Hybrid Retrieval ---\nQuery: 'Self-RAG'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Run a query and compare results ---\n",
    "query = \"Self-RAG\"\n",
    "print(f\"\\n\\n--- Running Hybrid Retrieval ---\")\n",
    "print(f\"Query: '{query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687bcaf5-02bc-4024-8047-f7f5d96ca848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Individual Retriever Results ---\n[BM25 Keyword Retrieval Results] (total 3):\n  - understanding and generating human-like text,\ndemo...\n  - Based on the Transformer architecture, they are ca...\n  - Large Language Models (LLMs) are a major focus of ...\n\n[FAISS Vector Retrieval Results] (total 3):\n  - understanding and generating human-like text,\ndemo...\n  - through convolutional layers and pooling layers....\n  - natural language processing (NLP) tasks such as ma...\n"
     ]
    }
   ],
   "source": [
    "# For comparison, inspect each retriever’s results individually first\n",
    "print(\"\\n--- Individual Retriever Results ---\")\n",
    "\n",
    "bm25_results = bm25_retriever.invoke(query)\n",
    "print(f\"[BM25 Keyword Retrieval Results] (total {len(bm25_results)}):\")\n",
    "for doc in bm25_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "faiss_results = faiss_retriever.invoke(query)\n",
    "print(f\"\\n[FAISS Vector Retrieval Results] (total {len(faiss_results)}):\")\n",
    "for doc in faiss_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b849a97-9dbc-4511-b347-41a83f424b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- MergerRetriever Hybrid Results ---\n[Final Hybrid Results] (total 6, deduplicated):\n  - understanding and generating human-like text,\ndemo...\n  - understanding and generating human-like text,\ndemo...\n  - Based on the Transformer architecture, they are ca...\n  - through convolutional layers and pooling layers....\n  - Large Language Models (LLMs) are a major focus of ...\n  - natural language processing (NLP) tasks such as ma...\n"
     ]
    }
   ],
   "source": [
    "# Now inspect the merged (hybrid) results\n",
    "print(\"\\n--- MergerRetriever Hybrid Results ---\")\n",
    "merged_results = merged_retriever.invoke(query)\n",
    "print(f\"[Final Hybrid Results] (total {len(merged_results)}, deduplicated):\")\n",
    "for doc in merged_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "845fb0e4-ec85-4f86-9297-9f03a028903b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Response Generation and Evaluation\n",
    "\n",
    "- Prompt Engineering\n",
    "- Structured Output\n",
    "- Output Parser\n",
    "- Post-processing \n",
    "- LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37b5a3c-dc39-4255-bc8b-effe10f2cdf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-622728fd-db06-44b1-b7e4-f7/.ipykernel/3025259/command-8604636319512275-762136011:12: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Structured object returned by the LLM ---\nanswer=\"A Self-RAG is a type of Return on Assets Growth that is calculated by taking the current year's return on assets and subtracting the previous year's return on assets. It is a measure of a company's ability to generate earnings from its assets over time.\" standard_terms_used=['RAG', 'Self-RAG']\n\nAnswer content: A Self-RAG is a type of Return on Assets Growth that is calculated by taking the current year's return on assets and subtracting the previous year's return on assets. It is a measure of a company's ability to generate earnings from its assets over time.\nStandard terms: ['RAG', 'Self-RAG']\n\n--- Final enhanced output ---\nA <abbr title=\"A RAG approach where the model explicitly self-checks (e.g., reflect, critique, verify) during generation, deciding when to retrieve more evidence and how to revise its answer based on feedback signals.\">Self-RAG</abbr> is a type of Return on Assets Growth that is calculated by taking the current year's return on assets and subtracting the previous year's return on assets. It is a measure of a company's ability to generate earnings from its assets over time.\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "\n",
    "# --- 1) Preparation ---\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.2)\n",
    "\n",
    "\n",
    "# --- 2) Define the expected output structure ---\n",
    "class TerminologyInAnswer(BaseModel):\n",
    "    \"\"\"A structured model containing the main answer and the technical terms used.\"\"\"\n",
    "    answer: str = Field(description=\"A detailed and accurate answer to the user's question.\")\n",
    "    standard_terms_used: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Standard technical terms from the official glossary explicitly used in the answer.\",\n",
    "        examples=[[\"RAG\", \"Self-RAG\"]],\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 3) Structured output chain (parameterized) ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI expert with deep technical knowledge. Provide a structured answer.\"),\n",
    "    (\"human\", \"Please explain: {question}\")\n",
    "])\n",
    "\n",
    "structured_llm_chain = prompt | llm.with_structured_output(TerminologyInAnswer)\n",
    "\n",
    "\n",
    "def glossary_list_to_def_dict(glossary_list: List[Dict[str, Any]], include_synonyms: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert list-style glossary into dict: {term_or_synonym: definition}.\n",
    "    Skips empty terms and empty definitions.\n",
    "    \"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for entry in glossary_list or []:\n",
    "        term = (entry.get(\"term\") or \"\").strip()\n",
    "        definition = (entry.get(\"definition\") or \"\").strip()\n",
    "        if term and definition:\n",
    "            out[term] = definition\n",
    "\n",
    "        if include_synonyms and definition:\n",
    "            for syn in entry.get(\"synonyms\", []) or []:\n",
    "                syn = (syn or \"\").strip()\n",
    "                if syn and syn not in out:\n",
    "                    out[syn] = definition\n",
    "    return out\n",
    "\n",
    "\n",
    "class TermEnhancer:\n",
    "    \"\"\"\n",
    "    Efficiently wraps known terms with <abbr title=\"...\">term</abbr> in one pass.\n",
    "    - Single compiled regex for all terms\n",
    "    - Avoids touching text inside HTML tags\n",
    "    - Avoids re-wrapping if text already contains <abbr ...>term</abbr> (best-effort)\n",
    "    \"\"\"\n",
    "    def __init__(self, term_defs: Dict[str, str], *, case_sensitive: bool = True):\n",
    "        self.term_defs = {k: v for k, v in (term_defs or {}).items() if k and v}\n",
    "        if not self.term_defs:\n",
    "            self._regex = None\n",
    "            return\n",
    "\n",
    "        # Prefer longer matches first so \"KG-RAG\" beats \"RAG\" when both exist.\n",
    "        terms_sorted = sorted(self.term_defs.keys(), key=len, reverse=True)\n",
    "\n",
    "        # Build a single alternation regex.\n",
    "        # NOTE: if you want strict word boundaries, you can wrap each term with \\b,\n",
    "        # but that breaks on hyphenated terms (Self-RAG). So we keep it flexible.\n",
    "        escaped = [re.escape(t) for t in terms_sorted]\n",
    "        flags = 0 if case_sensitive else re.IGNORECASE\n",
    "        self._regex = re.compile(\"|\".join(escaped), flags=flags)\n",
    "\n",
    "    def enhance(self, text: str) -> str:\n",
    "        if not text or not self._regex:\n",
    "            return text\n",
    "\n",
    "        # Split on HTML tags; only substitute in non-tag parts.\n",
    "        # This prevents replacing inside attributes like title=\"...\".\n",
    "        parts = re.split(r\"(<[^>]+>)\", text)\n",
    "\n",
    "        def repl(match: re.Match) -> str:\n",
    "            term = match.group(0)\n",
    "            definition = self.term_defs.get(term)\n",
    "            # If case-insensitive mode is desired, you can map via a normalized dict instead.\n",
    "            if not definition:\n",
    "                return term\n",
    "            safe_def = html.escape(definition, quote=True)\n",
    "            return f'<abbr title=\"{safe_def}\">{term}</abbr>'\n",
    "\n",
    "        for i in range(0, len(parts), 2):  # even indices are outside tags\n",
    "            # Best-effort: avoid wrapping inside existing <abbr>...</abbr>\n",
    "            # If your text can contain nested HTML, you may want a real HTML parser.\n",
    "            parts[i] = self._regex.sub(repl, parts[i])\n",
    "\n",
    "        return \"\".join(parts)\n",
    "\n",
    "\n",
    "# --- 4) Execute ---\n",
    "question = \"what a self-RAG is\"\n",
    "structured_response: TerminologyInAnswer = structured_llm_chain.invoke({\"question\": question})\n",
    "\n",
    "print(\"--- Structured object returned by the LLM ---\")\n",
    "print(structured_response)\n",
    "print(\"\\nAnswer content:\", structured_response.answer)\n",
    "print(\"Standard terms:\", structured_response.standard_terms_used)\n",
    "\n",
    "# --- 5) Enhance output with glossary definitions ---\n",
    "term_to_definition = glossary_list_to_def_dict(GLOSSARY, include_synonyms=True)\n",
    "enhancer = TermEnhancer(term_to_definition, case_sensitive=True)\n",
    "\n",
    "final_output = enhancer.enhance(structured_response.answer)\n",
    "\n",
    "print(\"\\n--- Final enhanced output ---\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca1afca-dcdc-49e9-bb46-845d35a3eb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def enhance_text_with_definitions(text: str, term_glossary: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Wrap glossary terms with <abbr title=\"...\">term</abbr> in a single pass.\n",
    "\n",
    "    Improvements over naive .replace loop:\n",
    "    - One compiled regex (faster than O(#terms * text_len))\n",
    "    - Longest term first to avoid partial matches\n",
    "    - Escapes HTML in definitions\n",
    "    - Avoids modifying inside HTML tags (best-effort)\n",
    "    \"\"\"\n",
    "    if not text or not term_glossary:\n",
    "        return text\n",
    "\n",
    "    # Keep only non-empty terms with definitions\n",
    "    term_glossary = {k: v for k, v in term_glossary.items() if k and v}\n",
    "    if not term_glossary:\n",
    "        return text\n",
    "\n",
    "    # Prefer longer terms first (\"Transformer Architecture\" before \"Transformer\")\n",
    "    terms_sorted = sorted(term_glossary.keys(), key=len, reverse=True)\n",
    "    pattern = re.compile(\"|\".join(re.escape(t) for t in terms_sorted))\n",
    "\n",
    "    # Split on HTML tags so we only modify visible text (not attributes)\n",
    "    parts = re.split(r\"(<[^>]+>)\", text)\n",
    "\n",
    "    def repl(m: re.Match) -> str:\n",
    "        term = m.group(0)\n",
    "        definition = term_glossary.get(term, \"\")\n",
    "        safe_def = html.escape(definition, quote=True)\n",
    "        return f'<abbr title=\"{safe_def}\">{term}</abbr>'\n",
    "\n",
    "    # Replace only outside tags: even indices\n",
    "    for i in range(0, len(parts), 2):\n",
    "        parts[i] = pattern.sub(repl, parts[i])\n",
    "\n",
    "    return \"\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3bed46-6167-415e-a132-f9743662eb36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<abbr title=\"Neural networks trained on massive text corpora to understand and generate language.\">Large Language Models</abbr> are typically based on the <abbr title=\"A neural network architecture based on self-attention mechanisms.\">Transformer Architecture</abbr>, while <abbr title=\"Retrieval-Augmented Generation, which combines retrieval with text generation.\">RAG</abbr> is a dominant approach for retrieving and generating knowledge.\n"
     ]
    }
   ],
   "source": [
    "term_to_definition = {\n",
    "    \"Large Language Models\": \"Neural networks trained on massive text corpora to understand and generate language.\",\n",
    "    \"Transformer Architecture\": \"A neural network architecture based on self-attention mechanisms.\",\n",
    "    \"RAG\": \"Retrieval-Augmented Generation, which combines retrieval with text generation.\"\n",
    "}\n",
    "\n",
    "llm_answer_text = (\n",
    "    \"Large Language Models are typically based on the Transformer Architecture, \"\n",
    "    \"while RAG is a dominant approach for retrieving and generating knowledge.\"\n",
    ")\n",
    "\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, term_to_definition)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202dbd04-e295-48dc-a987-d69a86dec61e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce41c0f-987f-4b46-8f70-844e736e859b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating [Good Answer] ---\nconsistency_score=4 is_consistent=True reasoning=\"The answer uses standard terms correctly, but there is a minor issue with the use of the alias 'Large Language Models' instead of the preferred term 'Large Language Model'.\" suggestions_for_improvement=[\"Replace 'Large Language Models' with 'Large Language Model'.\"]\n\nScore: 4\nConsistent: True\nSuggestions: [\"Replace 'Large Language Models' with 'Large Language Model'.\"]\n\n--- Evaluating [Needs Improvement Answer] ---\nconsistency_score=2 is_consistent=False reasoning=\"The answer uses unofficial aliases for standard terms, such as 'conv net' instead of 'Convolutional Neural Network' and 'transformer-style architecture' instead of 'Transformer Model'. Additionally, the answer does not use the most appropriate standard term for 'a big model' based on the provided terminology glossary.\" suggestions_for_improvement=[\"Replace 'conv net' with 'Convolutional Neural Network'\", \"Replace 'transformer-style architecture' with 'Transformer Model'\", \"Use the most appropriate standard term for 'a big model' based on the provided terminology glossary\"]\n\nScore: 2\nConsistent: False\nSuggestions: [\"Replace 'conv net' with 'Convolutional Neural Network'\", \"Replace 'transformer-style architecture' with 'Transformer Model'\", \"Use the most appropriate standard term for 'a big model' based on the provided terminology glossary\"]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1) Databricks LLM setup\n",
    "# -----------------------\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "# Use a separate evaluator LLM (lower temperature for consistency)\n",
    "evaluator_llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.0)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Define a structured evaluation model\n",
    "# -----------------------------------------\n",
    "class TerminologyEvaluation(BaseModel):\n",
    "    \"\"\"A structured model for evaluating terminology consistency.\"\"\"\n",
    "    consistency_score: int = Field(\n",
    "        description=\"Score from 1 to 5 (5=fully consistent, 1=severely inconsistent).\"\n",
    "    )\n",
    "    is_consistent: bool = Field(\n",
    "        description=\"Whether the answer is overall compliant with terminology standards.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Explanation of the score, highlighting strengths and issues.\"\n",
    "    )\n",
    "    suggestions_for_improvement: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Concrete suggestions to improve terminology usage.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3) Glossary (data-driven)\n",
    "# -----------------------\n",
    "# You can expand this anytime; prompt remains unchanged.\n",
    "GLOSSARY: List[Dict[str, object]] = [\n",
    "    {\"term\": \"Convolutional Neural Network\", \"aliases\": [\"CNN\"]},\n",
    "    {\"term\": \"Transformer Model\", \"aliases\": [\"Transformer\", \"Transformer Architecture\"]},\n",
    "    {\"term\": \"Large Language Model\", \"aliases\": [\"LLM\", \"Large Language Models\"]},\n",
    "]\n",
    "\n",
    "def format_glossary(glossary: List[Dict[str, object]]) -> str:\n",
    "    \"\"\"Render glossary into a compact, evaluator-friendly block.\"\"\"\n",
    "    lines = []\n",
    "    for entry in glossary:\n",
    "        term = entry[\"term\"]\n",
    "        aliases = entry.get(\"aliases\", []) or []\n",
    "        if aliases:\n",
    "            lines.append(f\"- {term} (aliases: {', '.join(aliases)})\")\n",
    "        else:\n",
    "            lines.append(f\"- {term}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "glossary_text = format_glossary(GLOSSARY)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 4) Build evaluation chain\n",
    "# -----------------------\n",
    "evaluation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a strict technical documentation quality evaluator. \"\n",
    "     \"You ONLY evaluate terminology usage based on the provided glossary and criteria.\"),\n",
    "    (\"human\",\n",
    "     \"\"\"Evaluate the terminology consistency and correctness in the answer.\n",
    "\n",
    "Evaluation Criteria:\n",
    "1) Accuracy: Are standard terms used correctly?\n",
    "2) Compliance: Does the answer avoid unofficial or ambiguous aliases when a standard term should be used?\n",
    "3) Completeness: Does the answer use the most appropriate standard terms when needed?\n",
    "\n",
    "Authoritative Terminology Glossary:\n",
    "{glossary_text}\n",
    "\n",
    "Answer to Evaluate:\n",
    "{answer_text}\n",
    "\n",
    "Rules:\n",
    "- Score must be an integer 1..5.\n",
    "- Set is_consistent=true only if terminology is largely compliant (minor issues ok).\n",
    "- Give concrete rewrite suggestions (phrases to replace), not vague advice.\n",
    "\n",
    "Return your result as structured output.\n",
    "\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "evaluation_chain = evaluation_prompt | evaluator_llm.with_structured_output(TerminologyEvaluation)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 5) Run evaluations\n",
    "# -----------------------\n",
    "good_answer = (\n",
    "    \"A Large Language Model (LLM) is built on a Transformer Model, while a \"\n",
    "    \"Convolutional Neural Network (CNN) is widely used in image-related domains.\"\n",
    ")\n",
    "\n",
    "bad_answer = (\n",
    "    \"A big model is based on a transformer-style architecture, and a conv net \"\n",
    "    \"is very strong at picture processing.\"\n",
    ")\n",
    "\n",
    "print(\"--- Evaluating [Good Answer] ---\")\n",
    "good_eval: TerminologyEvaluation = evaluation_chain.invoke({\n",
    "    \"glossary_text\": glossary_text,\n",
    "    \"answer_text\": good_answer\n",
    "})\n",
    "print(good_eval)\n",
    "print(\"\\nScore:\", good_eval.consistency_score)\n",
    "print(\"Consistent:\", good_eval.is_consistent)\n",
    "print(\"Suggestions:\", good_eval.suggestions_for_improvement)\n",
    "\n",
    "print(\"\\n--- Evaluating [Needs Improvement Answer] ---\")\n",
    "bad_eval: TerminologyEvaluation = evaluation_chain.invoke({\n",
    "    \"glossary_text\": glossary_text,\n",
    "    \"answer_text\": bad_answer\n",
    "})\n",
    "print(bad_eval)\n",
    "print(\"\\nScore:\", bad_eval.consistency_score)\n",
    "print(\"Consistent:\", bad_eval.is_consistent)\n",
    "print(\"Suggestions:\", bad_eval.suggestions_for_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c8a7e9-20f2-4793-a967-d554d92ce3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_RAG",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}