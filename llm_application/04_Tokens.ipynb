{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8901e340-f23c-4263-8dcc-8e887f407be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain langchain-community langchain-databricks faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b641c7-0eb8-4e2f-8d17-20c47d967d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef54e8-4f3d-42ab-b96f-b60ca42326f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcda38ce-d178-418a-9f3e-66c0df7ecdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Raw Token Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b1f725-60ba-4c28-b19e-21107de2b062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1) Databricks LLM + Embeddings\n",
    "# -----------------------\n",
    "# Make sure your Databricks auth is configured (e.g., DATABRICKS_HOST + DATABRICKS_TOKEN)\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"  # <-- change to your embedding endpoint name\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b03608-cbeb-476e-8ee0-28e4db57563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Sentence split + sentence-window splitter (LlamaIndex-like)\n",
    "# -----------------------\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[。！？!?])\\s+|\\n+\")\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def sentence_window_splitter(\n",
    "    documents: List[Document],\n",
    "    window_size: int = 2,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Mimics LlamaIndex SentenceWindowNodeParser:\n",
    "    - Each chunk is one core sentence\n",
    "    - metadata includes:\n",
    "        - original_text: core sentence\n",
    "        - window: context window (core +/- window_size sentences)\n",
    "    \"\"\"\n",
    "    out: List[Document] = []\n",
    "    for doc in documents:\n",
    "        sents = split_sentences(doc.page_content)\n",
    "        for i, core in enumerate(sents):\n",
    "            lo = max(0, i - window_size)\n",
    "            hi = min(len(sents), i + window_size + 1)\n",
    "            window_text = \" \".join(sents[lo:hi])\n",
    "\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=core,\n",
    "                    metadata={\n",
    "                        **(doc.metadata or {}),\n",
    "                        \"original_text\": core,\n",
    "                        \"window\": window_text,\n",
    "                        \"sent_index\": i,\n",
    "                        \"window_size\": window_size,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a46cdd-8ca8-421c-acc2-b26138b51c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Prompt + QA\n",
    "# -----------------------\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful technical assistant. Answer using ONLY the provided context. \"\n",
    "     \"If the context is insufficient, say what is missing.\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nAnswer in English:\")\n",
    "])\n",
    "\n",
    "def answer_with_retrieval(\n",
    "    docs: List[Document],\n",
    "    question: str,\n",
    "    top_k: int = 5,\n",
    "    use_window_metadata: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build FAISS index, retrieve top_k chunks, print retrieved chunks, then ask the Databricks LLM.\n",
    "    If use_window_metadata=True, feed metadata['window'] as context (Sentence Window style).\n",
    "    \"\"\"\n",
    "    vs = FAISS.from_documents(docs, embeddings)\n",
    "    retrieved = vs.similarity_search_with_score(question, k=top_k)\n",
    "\n",
    "    print(\"\\n--- Top retrieved chunks ---\")\n",
    "    context_blocks = []\n",
    "    for rank, (d, score) in enumerate(retrieved, 1):\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            window = d.metadata.get(\"window\", d.page_content)\n",
    "            core = d.metadata.get(\"original_text\", d.page_content)\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(f\"Core: {core}\")\n",
    "            print(f\"Window:\\n{window}\")\n",
    "            context_blocks.append(window)\n",
    "        else:\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(d.page_content)\n",
    "            context_blocks.append(d.page_content)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    msg = QA_PROMPT.format_messages(question=question, context=context)\n",
    "    resp = llm.invoke(msg)\n",
    "\n",
    "    print(\"\\n--- LLM Answer ---\")\n",
    "    print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65d9762-ddc9-48d9-9d54-09f8a1931f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4) Runner (prints raw chunks + retrieval behavior)\n",
    "# -----------------------\n",
    "def evaluate_splitter(\n",
    "    splitter_name: str,\n",
    "    split_fn: Callable[[], List[Document]],\n",
    "    question: str,\n",
    "    use_window_metadata: bool = False,\n",
    "    top_k: int = 5,\n",
    "    max_print_chunks: int = 50,\n",
    ") -> None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing splitter: {splitter_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    chunks = split_fn()\n",
    "\n",
    "    print(f\"\\n[Raw chunks generated] total={len(chunks)}\")\n",
    "    for i, d in enumerate(chunks[:max_print_chunks], 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            print(f\"Core: {d.metadata.get('original_text')}\")\n",
    "            print(f\"Window: {d.metadata.get('window')}\")\n",
    "        else:\n",
    "            print(d.page_content)\n",
    "\n",
    "    if len(chunks) > max_print_chunks:\n",
    "        print(f\"\\n... (only printed first {max_print_chunks} chunks)\")\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer_with_retrieval(\n",
    "        docs=chunks,\n",
    "        question=question,\n",
    "        top_k=top_k,\n",
    "        use_window_metadata=use_window_metadata,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{splitter_name} done.\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1906460-b1a6-4bb1-b7a8-b294fd615fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 5) Example document + question (English version)\n",
    "# -----------------------\n",
    "documents = [\n",
    "    Document(page_content=\"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
    "LLM-powered applications that combine external knowledge retrieval with\n",
    "text generation.\n",
    "Instead of relying solely on a model’s internal parameters, RAG systems\n",
    "retrieve relevant information from external data sources and use it as\n",
    "context during answer generation.\n",
    "This approach is widely used for applications such as question answering,\n",
    "knowledge assistants, and domain-specific search.\n",
    "\n",
    "A typical RAG pipeline consists of several core components, including\n",
    "document ingestion, text chunking, embedding generation, vector indexing,\n",
    "retrieval, and response synthesis.\n",
    "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
    "are first ingested and preprocessed.\n",
    "They are then split into smaller units, often referred to as chunks,\n",
    "to make retrieval more precise and efficient.\n",
    "\n",
    "Each chunk is converted into a vector representation using an embedding model\n",
    "and stored in a vector index.\n",
    "During inference, a user query is embedded and compared against the indexed\n",
    "vectors to retrieve the most relevant chunks.\n",
    "These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
    "allowing the model to generate answers that are grounded in external knowledge.\n",
    "\n",
    "--- The following content is less directly related to the RAG topic ---\n",
    "\n",
    "In addition, Python, as a general-purpose programming language, is widely used\n",
    "in the AI field due to its simplicity and rich ecosystem.\n",
    "For example, NumPy and Pandas are foundational tools for data processing,\n",
    "providing powerful capabilities for numerical computation and structured data.\n",
    "Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
    "for tasks such as classification, regression, and clustering.\n",
    "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
    "enabling efficient development and deployment of complex AI models.\n",
    "\n",
    "--- The following is another related but conceptually independent section ---\n",
    "\n",
    "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
    "contains a target sentence along with a configurable number of surrounding\n",
    "“window” sentences as context.\n",
    "This approach aims to provide rich local context to the LLM during retrieval,\n",
    "thereby improving the coherence and factual consistency of generated answers.\n",
    "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
    "content rather than relying solely on fixed character counts or sentence boundaries.\n",
    "It leverages embedding models to compute semantic similarity between sentences\n",
    "or phrases and identify natural breakpoints where topics or meanings shift.\n",
    "Both advanced methods can significantly improve retrieval quality and\n",
    "downstream generation performance in RAG applications.\n",
    "Choosing the right chunking strategy typically depends on the characteristics\n",
    "of the data and the expected query types.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "question = (\n",
    "    \"What are the main components of a Retrieval-Augmented Generation (RAG) system, \"\n",
    "    \"and how do sentence window chunking and semantic chunking differ?\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Run splitters\n",
    "# -----------------------\n",
    "# Token-based split (chunk_size=30, overlap=0)\n",
    "splitter_a = TokenTextSplitter(chunk_size=30, chunk_overlap=0)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=0)\",\n",
    "    split_fn=lambda: splitter_a.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Token-based split (chunk_size=30, overlap=10)\n",
    "splitter_b = TokenTextSplitter(chunk_size=30, chunk_overlap=10)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=10)\",\n",
    "    split_fn=lambda: splitter_b.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Sentence-window split (window_size=2)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Sentence Window Split (window_size=2)\",\n",
    "    split_fn=lambda: sentence_window_splitter(documents, window_size=2),\n",
    "    question=question,\n",
    "    use_window_metadata=True,   # feed metadata['window'] into LLM context\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981572fb-afc9-4af8-891d-4fcc08ff1c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Token Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9013498-c13b-4207-b3a0-6aada21b7290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "\n",
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG pipeline with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 1: Split documents\n",
    "    # -----------------------\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[{splitter_name}] Generated document chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(chunk.page_content.strip())\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 2: Build vector store\n",
    "    # -----------------------\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 3: Build RAG chain\n",
    "    # -----------------------\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 4: Ask question\n",
    "    # -----------------------\n",
    "    print(f\"\\nQuestion:\\n{question}\\n\")\n",
    "    print(\"Model Answer:\\n\")\n",
    "\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 5: Show retrieved context\n",
    "    # -----------------------\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        print(f\"\\n--- Source Document {i} ---\")\n",
    "        print(doc.page_content.strip())\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG pipeline with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=sentence_splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"RecursiveCharacterTextSplitter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "286e5107-183c-47af-a280-e5e3fedb193a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Sentence Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d162956-a99a-4eb3-abcb-8ebe80c177f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SentenceWindowTextSplitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size: int = 3,\n",
    "        window_metadata_key: str = \"window\",\n",
    "        original_text_metadata_key: str = \"original_text\",\n",
    "    ):\n",
    "        self.window_size = window_size\n",
    "        self.window_metadata_key = window_metadata_key\n",
    "        self.original_text_metadata_key = original_text_metadata_key\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks = []\n",
    "\n",
    "        for doc in documents:\n",
    "            sentences = [\n",
    "                s.strip()\n",
    "                for s in re.split(r\"(?<=[.!?])\\s+\", doc.page_content)\n",
    "                if s.strip()\n",
    "            ]\n",
    "\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(sentences), i + self.window_size + 1)\n",
    "\n",
    "                window_sentences = sentences[start:end]\n",
    "                window_text = \" \".join(window_sentences)\n",
    "\n",
    "                chunks.append(\n",
    "                    Document(\n",
    "                        page_content=window_text,\n",
    "                        metadata={\n",
    "                            self.window_metadata_key: window_text,\n",
    "                            self.original_text_metadata_key: sentence,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # --- Split documents ---\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[{splitter_name}] Generated chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if \"original_text\" in chunk.metadata:\n",
    "            print(\"Center sentence:\")\n",
    "            print(chunk.metadata[\"original_text\"])\n",
    "            print(\"\\nWindow context:\")\n",
    "            print(chunk.metadata[\"window\"])\n",
    "        else:\n",
    "            print(chunk.page_content)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # --- Vector store ---\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # --- RAG chain ---\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # --- Query ---\n",
    "    print(f\"\\nQuestion:\\n{question}\\n\")\n",
    "    print(\"Model Answer:\\n\")\n",
    "\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # --- Retrieved context ---\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        print(f\"\\n--- Source Document {i} ---\")\n",
    "        if \"original_text\" in doc.metadata:\n",
    "            print(\"Center sentence:\")\n",
    "            print(doc.metadata[\"original_text\"])\n",
    "            print(\"\\nWindow context:\")\n",
    "            print(doc.metadata[\"window\"])\n",
    "        else:\n",
    "            print(doc.page_content)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd771461-9f4a-4c5c-b4f1-ed904ec7a544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5. Run Sentence Window Chunking (LlamaIndex-style)\n",
    "# =========================================================\n",
    "\n",
    "sentence_window_splitter = SentenceWindowTextSplitter(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=sentence_window_splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Sentence Window (LangChain)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "301d1eee-f56f-4969-a5d6-8da95ed0a278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Sliding Window Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec04500b-838c-414f-96ba-d7146e9521b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SlidingWindowTextSplitter:\n",
    "    \"\"\"\n",
    "    Sliding window over sentences:\n",
    "    - window_size: number of sentences per chunk\n",
    "    - stride: how many sentences to move forward each step\n",
    "      (stride < window_size => overlap, stride == window_size => no overlap)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size: int = 5,\n",
    "        stride: int = 2,\n",
    "        window_metadata_key: str = \"window\",\n",
    "        start_idx_metadata_key: str = \"start_sentence_idx\",\n",
    "        end_idx_metadata_key: str = \"end_sentence_idx\",\n",
    "    ):\n",
    "        if window_size <= 0:\n",
    "            raise ValueError(\"window_size must be > 0\")\n",
    "        if stride <= 0:\n",
    "            raise ValueError(\"stride must be > 0\")\n",
    "        if stride > window_size:\n",
    "            # Allowed, but usually not what you want (gaps between windows)\n",
    "            pass\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.window_metadata_key = window_metadata_key\n",
    "        self.start_idx_metadata_key = start_idx_metadata_key\n",
    "        self.end_idx_metadata_key = end_idx_metadata_key\n",
    "\n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        # Same splitting logic as your sentence window splitter\n",
    "        return [\n",
    "            s.strip()\n",
    "            for s in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "            if s.strip()\n",
    "        ]\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks: List[Document] = []\n",
    "\n",
    "        for doc in documents:\n",
    "            sentences = self._split_sentences(doc.page_content)\n",
    "            n = len(sentences)\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            # Sliding windows\n",
    "            for start in range(0, n, self.stride):\n",
    "                end = min(start + self.window_size, n)\n",
    "                window_sentences = sentences[start:end]\n",
    "                window_text = \" \".join(window_sentences)\n",
    "\n",
    "                chunks.append(\n",
    "                    Document(\n",
    "                        page_content=window_text,\n",
    "                        metadata={\n",
    "                            self.window_metadata_key: window_text,\n",
    "                            self.start_idx_metadata_key: start,\n",
    "                            self.end_idx_metadata_key: end - 1,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                if end == n:\n",
    "                    break  # reached the end\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fcc1481-73ee-4edd-80a0-45ea3ee1bb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "splitter = SlidingWindowTextSplitter(window_size=5, stride=2)\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Sliding Window (size=5, stride=2)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6d2794-278c-4eda-bc03-4a4394dca565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Semantic Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf2c75e-cad7-4ea8-99a1-0562967be063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bf9c66-afca-450a-b07b-ab400dd6fa46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Analysis helpers (chunk stats + retrieval scores)\n",
    "# -----------------------\n",
    "def _approx_token_count(text: str) -> int:\n",
    "    # Cheap proxy; good enough for comparing splitters\n",
    "    return max(1, len(text.split()))\n",
    "\n",
    "\n",
    "def analyze_chunks(chunks: List[Document], name: str) -> Dict[str, Any]:\n",
    "    lengths = [_approx_token_count(c.page_content) for c in chunks]\n",
    "    lengths_sorted = sorted(lengths)\n",
    "    n = len(lengths_sorted)\n",
    "\n",
    "    def _p(pct: float) -> int:\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        idx = min(n - 1, int(round((pct / 100.0) * (n - 1))))\n",
    "        return lengths_sorted[idx]\n",
    "\n",
    "    out = {\n",
    "        \"splitter\": name,\n",
    "        \"num_chunks\": n,\n",
    "        \"min_tokens\": min(lengths) if lengths else 0,\n",
    "        \"p50_tokens\": _p(50),\n",
    "        \"p90_tokens\": _p(90),\n",
    "        \"max_tokens\": max(lengths) if lengths else 0,\n",
    "        \"avg_tokens\": round(sum(lengths) / n, 2) if n else 0,\n",
    "        \"exact_duplicate_chunks\": n - len({c.page_content for c in chunks}),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def retrieve_with_scores(vectorstore: FAISS, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "    # FAISS supports similarity_search_with_score\n",
    "    return vectorstore.similarity_search_with_score(query, k=k)\n",
    "\n",
    "\n",
    "def analyze_retrieval(\n",
    "    retrieved: List[Tuple[Document, float]],\n",
    "    keywords: Optional[List[str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    scores = [s for _, s in retrieved]\n",
    "    out = {\n",
    "        \"k\": len(retrieved),\n",
    "        \"best_score\": float(min(scores)) if scores else None,\n",
    "        \"worst_score\": float(max(scores)) if scores else None,\n",
    "        \"avg_score\": float(sum(scores) / len(scores)) if scores else None,\n",
    "    }\n",
    "\n",
    "    if keywords:\n",
    "        hits = 0\n",
    "        for doc, _ in retrieved:\n",
    "            t = doc.page_content.lower()\n",
    "            if any(kw.lower() in t for kw in keywords):\n",
    "                hits += 1\n",
    "        out[\"keyword_hits\"] = hits\n",
    "        out[\"keywords\"] = keywords\n",
    "    return out\n",
    "\n",
    "\n",
    "def print_analysis(chunk_stats: Dict[str, Any], retrieval_stats: Dict[str, Any]) -> None:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"Chunk stats:\")\n",
    "    for k, v in chunk_stats.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\n",
    "    print(\"\\nRetrieval stats (FAISS similarity_search_with_score):\")\n",
    "    for k, v in retrieval_stats.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\n",
    "    print(\"=\" * 70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4993922-0ed8-4728-95ca-9fb193b75de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Semantic Splitter (LangChain version)\n",
    "#    - Sentence tokenize\n",
    "#    - Embed each sentence\n",
    "#    - Compute adjacent cosine similarity\n",
    "#    - Break at low-similarity points by percentile threshold\n",
    "#    - Optional buffer_size to include neighbors around boundaries\n",
    "# -----------------------\n",
    "def sentence_tokenize_en(text: str) -> List[str]:\n",
    "    # Simple sentence splitter; you can swap with nltk/spacy if you prefer\n",
    "    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
    "    return sents\n",
    "\n",
    "\n",
    "def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "\n",
    "class SemanticTextSplitter:\n",
    "    \"\"\"\n",
    "    LangChain-style semantic chunker inspired by LlamaIndex SemanticSplitterNodeParser.\n",
    "\n",
    "    breakpoint_percentile_threshold:\n",
    "        If 95: break on the lowest 5% similarities (conservative, fewer breaks)\n",
    "        If 5:  break on the lowest 95% similarities (aggressive, many breaks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder,\n",
    "        sentence_splitter=sentence_tokenize_en,\n",
    "        breakpoint_percentile_threshold: float = 95,\n",
    "        buffer_size: int = 0,\n",
    "        min_sentences_per_chunk: int = 1,\n",
    "        max_sentences_per_chunk: Optional[int] = None,\n",
    "    ):\n",
    "        self.embedder = embedder\n",
    "        self.sentence_splitter = sentence_splitter\n",
    "        self.breakpoint_percentile_threshold = breakpoint_percentile_threshold\n",
    "        self.buffer_size = buffer_size\n",
    "        self.min_sentences_per_chunk = min_sentences_per_chunk\n",
    "        self.max_sentences_per_chunk = max_sentences_per_chunk\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        out_chunks: List[Document] = []\n",
    "\n",
    "        for doc in documents:\n",
    "            sentences = self.sentence_splitter(doc.page_content)\n",
    "            if len(sentences) == 0:\n",
    "                continue\n",
    "            if len(sentences) == 1:\n",
    "                out_chunks.append(Document(page_content=sentences[0], metadata=dict(doc.metadata or {})))\n",
    "                continue\n",
    "\n",
    "            # Embed sentences (Databricks embeddings returns list[list[float]])\n",
    "            sent_vecs = self.embedder.embed_documents(sentences)\n",
    "            sent_vecs = [np.array(v, dtype=np.float32) for v in sent_vecs]\n",
    "\n",
    "            sims = [_cosine_sim(sent_vecs[i], sent_vecs[i + 1]) for i in range(len(sent_vecs) - 1)]\n",
    "            # Breakpoints are where similarity is BELOW threshold value\n",
    "            threshold_val = float(np.percentile(sims, self.breakpoint_percentile_threshold))\n",
    "\n",
    "            break_idxs = [i for i, sim in enumerate(sims) if sim < threshold_val]\n",
    "            # i means \"between sentence i and i+1\" -> boundary at i+1\n",
    "            boundaries = set([0, len(sentences)])\n",
    "            for i in break_idxs:\n",
    "                boundaries.add(i + 1)\n",
    "\n",
    "            boundaries = sorted(boundaries)\n",
    "\n",
    "            # Create initial chunks from boundaries\n",
    "            chunks_sent_ranges: List[Tuple[int, int]] = []\n",
    "            for a, b in zip(boundaries[:-1], boundaries[1:]):\n",
    "                if a < b:\n",
    "                    chunks_sent_ranges.append((a, b))\n",
    "\n",
    "            # Apply buffer_size (expand ranges)\n",
    "            if self.buffer_size > 0:\n",
    "                expanded: List[Tuple[int, int]] = []\n",
    "                for a, b in chunks_sent_ranges:\n",
    "                    aa = max(0, a - self.buffer_size)\n",
    "                    bb = min(len(sentences), b + self.buffer_size)\n",
    "                    expanded.append((aa, bb))\n",
    "                # Merge overlaps after expansion\n",
    "                expanded.sort()\n",
    "                merged: List[Tuple[int, int]] = []\n",
    "                for a, b in expanded:\n",
    "                    if not merged or a > merged[-1][1]:\n",
    "                        merged.append((a, b))\n",
    "                    else:\n",
    "                        merged[-1] = (merged[-1][0], max(merged[-1][1], b))\n",
    "                chunks_sent_ranges = merged\n",
    "\n",
    "            # Enforce min/max sentences per chunk (optional)\n",
    "            final_ranges: List[Tuple[int, int]] = []\n",
    "            for a, b in chunks_sent_ranges:\n",
    "                # Ensure minimum sentences by merging forward if needed\n",
    "                if (b - a) < self.min_sentences_per_chunk and final_ranges:\n",
    "                    prev_a, prev_b = final_ranges.pop()\n",
    "                    final_ranges.append((prev_a, b))\n",
    "                else:\n",
    "                    final_ranges.append((a, b))\n",
    "\n",
    "            if self.max_sentences_per_chunk is not None:\n",
    "                # split overly large chunks\n",
    "                limited: List[Tuple[int, int]] = []\n",
    "                for a, b in final_ranges:\n",
    "                    cur = a\n",
    "                    while cur < b:\n",
    "                        nxt = min(b, cur + self.max_sentences_per_chunk)\n",
    "                        limited.append((cur, nxt))\n",
    "                        cur = nxt\n",
    "                final_ranges = limited\n",
    "\n",
    "            # Build Documents\n",
    "            base_meta = dict(doc.metadata or {})\n",
    "            for a, b in final_ranges:\n",
    "                text = \" \".join(sentences[a:b]).strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                meta = dict(base_meta)\n",
    "                # optional debug metadata\n",
    "                meta.update(\n",
    "                    {\n",
    "                        \"semantic_threshold_val\": threshold_val,\n",
    "                        \"semantic_percentile\": self.breakpoint_percentile_threshold,\n",
    "                        \"semantic_buffer_size\": self.buffer_size,\n",
    "                        \"sentence_range\": (a, b),\n",
    "                    }\n",
    "                )\n",
    "                out_chunks.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "        return out_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74128e68-5d7f-4bc5-bab4-a15c65806ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "    top_k: int = 5,\n",
    "    print_raw_chunks: bool = True,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # --- Split documents ---\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    if print_raw_chunks:\n",
    "        print(f\"[{splitter_name}] Generated chunks: total={len(chunks)}\")\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\n--- Chunk {i} ---\")\n",
    "            print(chunk.page_content)\n",
    "            if chunk.metadata:\n",
    "                # show only the semantic fields for readability\n",
    "                semantic_keys = [\"sentence_range\", \"semantic_percentile\", \"semantic_threshold_val\", \"semantic_buffer_size\"]\n",
    "                shown = {k: chunk.metadata.get(k) for k in semantic_keys if k in chunk.metadata}\n",
    "                if shown:\n",
    "                    print(f\"\\n[metadata] {shown}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    # --- Vector store ---\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    # --- Analysis: chunk stats + retrieval scores ---\n",
    "    chunk_stats = analyze_chunks(chunks, splitter_name)\n",
    "    retrieved = retrieve_with_scores(vectorstore, question, k=top_k)\n",
    "    keywords = [\"retrieval\", \"chunk\", \"chunking\", \"semantic\", \"embedding\", \"vector\", \"index\", \"rag\"]\n",
    "    retrieval_stats = analyze_retrieval(retrieved, keywords=keywords)\n",
    "    print_analysis(chunk_stats, retrieval_stats)\n",
    "\n",
    "    # --- RAG chain (stuff) ---\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # --- Query ---\n",
    "    print(f\"Question:\\n{question}\\n\")\n",
    "    print(\"--- LLM Answer ---\")\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # --- Retrieved context (with scores) ---\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents (with scores):\")\n",
    "    for i, (doc, score) in enumerate(retrieved, 1):\n",
    "        print(f\"\\n--- Source Document {i} --- score={score:.4f}\")\n",
    "        print(doc.page_content)\n",
    "        # show semantic debug metadata if present\n",
    "        if doc.metadata:\n",
    "            semantic_keys = [\"sentence_range\", \"semantic_percentile\", \"semantic_threshold_val\", \"semantic_buffer_size\"]\n",
    "            shown = {k: doc.metadata.get(k) for k in semantic_keys if k in doc.metadata}\n",
    "            if shown:\n",
    "                print(f\"\\n[metadata] {shown}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594b6d35-ab05-448f-8455-07f6a065fba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 6) Run Semantic Splitter experiments (conservative vs aggressive)\n",
    "# -----------------------\n",
    "# Conservative: break at lowest 5% similarities (95th percentile threshold)\n",
    "semantic_splitter_conservative = SemanticTextSplitter(\n",
    "    embedder=embeddings,\n",
    "    sentence_splitter=sentence_tokenize_en,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    buffer_size=1,                 # include 1 neighboring sentence around boundaries\n",
    "    min_sentences_per_chunk=1,\n",
    ")\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=semantic_splitter_conservative,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Semantic Split (percentile=95, buffer=1) - conservative\",\n",
    "    print_raw_chunks=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57f6e15-8e45-418b-a6c8-0a81d14ef136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggressive: break at lowest 95% similarities (5th percentile threshold)\n",
    "semantic_splitter_aggressive = SemanticTextSplitter(\n",
    "    embedder=embeddings,\n",
    "    sentence_splitter=sentence_tokenize_en,\n",
    "    breakpoint_percentile_threshold=5,\n",
    "    buffer_size=1,\n",
    "    min_sentences_per_chunk=1,\n",
    ")\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=semantic_splitter_aggressive,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Semantic Split (percentile=5, buffer=1) - aggressive\",\n",
    "    top_k=5,\n",
    "    print_raw_chunks=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8ea7ede-a33d-40e4-9ad1-0376339940c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### HybridTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d956d9ff-2dd6-4e1e-a67b-de8e86731701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class HybridTextSplitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        primary_splitter,                 # e.g., SemanticTextSplitter\n",
    "        secondary_splitter,               # e.g., SlidingWindowTextSplitter\n",
    "        max_chunk_tokens: int = 300,\n",
    "        debug_print: bool = True,\n",
    "    ):\n",
    "        self.primary_splitter = primary_splitter\n",
    "        self.secondary_splitter = secondary_splitter\n",
    "        self.max_chunk_tokens = max_chunk_tokens\n",
    "        self.debug_print = debug_print\n",
    "\n",
    "    def _token_len(self, text: str) -> int:\n",
    "        # Uses the same approximate token proxy as the previous code\n",
    "        return max(1, len(text.split()))\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        if self.debug_print:\n",
    "            print(\"--- Starting HYBRID splitting ---\")\n",
    "            print(\"Step 1: semantic split...\")\n",
    "\n",
    "        primary_chunks = self.primary_splitter.split_documents(documents)\n",
    "\n",
    "        if self.debug_print:\n",
    "            print(f\"\\n{'='*25} Step 1 (Semantic) Output {'='*25}\")\n",
    "            print(f\"Produced {len(primary_chunks)} semantic chunks.\")\n",
    "            for i, c in enumerate(primary_chunks, 1):\n",
    "                print(f\"\\n[Semantic chunk {i}] tokens≈{self._token_len(c.page_content)}\")\n",
    "                print(\"-\" * 60)\n",
    "                print(c.page_content.strip())\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "            print(f\"\\n{'='*25} Step 2 (Check + Secondary Split) {'='*25}\")\n",
    "\n",
    "        final_chunks: List[Document] = []\n",
    "        for i, chunk in enumerate(primary_chunks, 1):\n",
    "            tlen = self._token_len(chunk.page_content)\n",
    "\n",
    "            if self.debug_print:\n",
    "                print(f\"\\n>>> Checking semantic chunk {i} tokens≈{tlen} ...\")\n",
    "\n",
    "            if tlen <= self.max_chunk_tokens:\n",
    "                if self.debug_print:\n",
    "                    print(f\"  └─ OK (<= {self.max_chunk_tokens}). Keep as-is.\")\n",
    "                final_chunks.append(chunk)\n",
    "            else:\n",
    "                if self.debug_print:\n",
    "                    print(f\"  └─ Too large (> {self.max_chunk_tokens}). Apply sliding window split.\")\n",
    "                    print(\"     [Original oversized chunk]\")\n",
    "                    print(\"     \" + \"-\" * 50)\n",
    "                    print(\"     \" + chunk.page_content.strip().replace(\"\\n\", \"\\n     \"))\n",
    "                    print(\"     \" + \"-\" * 50)\n",
    "\n",
    "                # Secondary split acts on the oversized chunk as a single Document\n",
    "                sub_docs = self.secondary_splitter.split_documents(\n",
    "                    [Document(page_content=chunk.page_content, metadata=dict(chunk.metadata or {}))]\n",
    "                )\n",
    "\n",
    "                if self.debug_print:\n",
    "                    print(f\"\\n     [Secondary split produced {len(sub_docs)} sub-chunks]\")\n",
    "                    for j, sd in enumerate(sub_docs, 1):\n",
    "                        print(f\"\\n     [Sub-chunk {i}.{j}] tokens≈{self._token_len(sd.page_content)}\")\n",
    "                        print(\"     \" + \"-\" * 40)\n",
    "                        print(\"     \" + sd.page_content.strip().replace(\"\\n\", \"\\n     \"))\n",
    "                        print(\"     \" + \"-\" * 40)\n",
    "\n",
    "                final_chunks.extend(sub_docs)\n",
    "\n",
    "        if self.debug_print:\n",
    "            print(\"\\n--- HYBRID splitting complete ---\")\n",
    "            print(f\"Final chunk count: {len(final_chunks)}\")\n",
    "\n",
    "        return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4890554e-2bd9-4b00-93ab-b1e0f89318ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "semantic_primary = SemanticTextSplitter(\n",
    "    embedder=embeddings,\n",
    "    sentence_splitter=sentence_tokenize_en,   # or your chinese tokenizer if your doc is Chinese\n",
    "    breakpoint_percentile_threshold=95,       # conservative\n",
    "    buffer_size=1,\n",
    "    min_sentences_per_chunk=1,\n",
    ")\n",
    "\n",
    "sliding_secondary = SlidingWindowTextSplitter(\n",
    "    window_size=256,\n",
    "    stride=2)\n",
    "\n",
    "hybrid_splitter = HybridTextSplitter(\n",
    "    primary_splitter=semantic_primary,\n",
    "    secondary_splitter=sliding_secondary,\n",
    "    max_chunk_tokens=300,      # align with your example's max_chunk_size idea\n",
    "    debug_print=True,\n",
    ")\n",
    "\n",
    "# Now run the exact same RAG pipeline you already have:\n",
    "run_rag_pipeline(\n",
    "    splitter=hybrid_splitter,\n",
    "    documents=documents,   # reuse your RAG document list\n",
    "    question=question,\n",
    "    splitter_name=\"Hybrid Split (Semantic -> Sliding Window fallback)\",\n",
    "    top_k=5,\n",
    "    print_raw_chunks=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5023ba3b-0b5b-4e9e-a4cc-223b418ec58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Tokens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
