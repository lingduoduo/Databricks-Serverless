{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8901e340-f23c-4263-8dcc-8e887f407be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n  Downloading langchain-1.2.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-databricks\n  Downloading langchain_databricks-0.1.2-py3-none-any.whl.metadata (3.3 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (7.6 kB)\nCollecting tiktoken\n  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (6.7 kB)\nCollecting langchain-core<2.0.0,>=1.2.1 (from langchain)\n  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\nCollecting langgraph<1.1.0,>=1.0.2 (from langchain)\n  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.10.6)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (9.5 kB)\nCollecting requests<3.0.0,>=2.32.5 (from langchain-community)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (6.0.2)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (8.1 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (9.0.0)\nCollecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n  Downloading langsmith-0.5.2-py3-none-any.whl.metadata (15 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (2.1.3)\nCollecting databricks-vectorsearch<0.41,>=0.40 (from langchain-databricks)\n  Downloading databricks_vectorsearch-0.40-py3-none-any.whl.metadata (2.8 kB)\nINFO: pip is looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-databricks\n  Downloading langchain_databricks-0.1.1-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_databricks-0.1.0-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\nINFO: pip is still looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting langchain\n  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting langchain-community\n  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\nCollecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n  Downloading SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.6-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\nCollecting langchain\n  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n  Downloading langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n  Downloading langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n  Downloading langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.19-py3-none-any.whl.metadata (7.9 kB)\n  Downloading langchain_community-0.0.18-py3-none-any.whl.metadata (7.9 kB)\n  Downloading langchain_community-0.0.17-py3-none-any.whl.metadata (7.9 kB)\n  Downloading langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\n  Downloading langchain_community-0.0.15-py3-none-any.whl.metadata (7.6 kB)\n  Downloading langchain_community-0.0.14-py3-none-any.whl.metadata (7.5 kB)\n  Downloading langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\n  Downloading langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n  Downloading langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\n  Downloading langchain_community-0.0.6-py3-none-any.whl.metadata (7.2 kB)\n  Downloading langchain_community-0.0.5-py3-none-any.whl.metadata (7.1 kB)\n  Downloading langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n  Downloading langchain_community-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n  Downloading langchain_community-0.0.2-py3-none-any.whl.metadata (5.8 kB)\n  Downloading langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain\n  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\nCollecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (24.1)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.12.2)\nCollecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n  Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (1.1 kB)\nCollecting langchain\n  Downloading langchain-1.1.2-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.1.1-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.7-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.4-py3-none-any.whl.metadata (4.9 kB)\n  Downloading langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n  Downloading langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n  Downloading langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n  Downloading langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n  Downloading langchain_core-0.3.81-py3-none-any.whl.metadata (3.2 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.32.3)\nCollecting mlflow>=2.16.0 (from langchain-databricks)\n  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: scipy>=1.11 in /databricks/python3/lib/python3.12/site-packages (from langchain-databricks) (1.15.1)\nCollecting regex>=2022.1.18 (from tiktoken)\n  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (75 kB)\nRequirement already satisfied: mlflow-skinny<3,>=2.11.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.22.0)\nCollecting protobuf<5,>=3.12.0 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\nCollecting deprecation>=2 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\nCollecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (41 kB)\nCollecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nINFO: pip is looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\nCollecting mlflow>=2.16.0 (from langchain-databricks)\n  Downloading mlflow-3.8.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.7.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n  Downloading mlflow-3.5.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.5.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.2-py3-none-any.whl.metadata (30 kB)\nINFO: pip is still looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\n  Downloading mlflow-3.3.1-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.3.0-py3-none-any.whl.metadata (30 kB)\n  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.3-py3-none-any.whl.metadata (29 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.1-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-3.0.0-py3-none-any.whl.metadata (29 kB)\n  Downloading mlflow-2.22.4-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-skinny<3,>=2.11.3 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n  Downloading mlflow_skinny-2.22.4-py3-none-any.whl.metadata (31 kB)\nCollecting Flask<4 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.1.5)\nCollecting alembic!=1.10.0,<2 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\nCollecting docker<8,>=4.0.0 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting markdown<4,>=3.3 (from mlflow>=2.16.0->langchain-databricks)\n  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.10.0)\nRequirement already satisfied: pandas!=2.3.0,<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (2.2.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (1.6.1)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.49.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.5.3)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.34.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.1.31)\nCollecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n  Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (4.1 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow>=2.16.0->langchain-databricks)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow>=2.16.0->langchain-databricks) (3.0.2)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.16.0->langchain-databricks)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow>=2.16.0->langchain-databricks) (2.9.0.post0)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.6.2)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (3.2.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (3.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.40.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.21.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.53b1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.16.0->langchain-databricks) (1.16.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.9.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.4.8)\nDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m25.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m77.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_core-0.3.81-py3-none-any.whl (457 kB)\nDownloading langchain_databricks-0.1.2-py3-none-any.whl (21 kB)\nDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (11.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/11.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.5/11.5 MB\u001B[0m \u001B[31m149.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m60.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m91.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_vectorsearch-0.40-py3-none-any.whl (12 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\nDownloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\nDownloading langsmith-0.5.2-py3-none-any.whl (283 kB)\nDownloading mlflow-2.22.4-py3-none-any.whl (29.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/29.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.0/29.0 MB\u001B[0m \u001B[31m185.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-2.22.4-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m129.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\nDownloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (798 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/798.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m798.6/798.6 kB\u001B[0m \u001B[31m34.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m120.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading alembic-1.17.2-py3-none-any.whl (248 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (243 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (597 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/597.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m597.3/597.3 kB\u001B[0m \u001B[31m27.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading markdown-3.10-py3-none-any.whl (107 kB)\nDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\nDownloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (258 kB)\nDownloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (132 kB)\nDownloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (225 kB)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl (294 kB)\nDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nDownloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (340 kB)\nDownloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (372 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nInstalling collected packages: werkzeug, uuid-utils, typing-inspection, typing-inspect, requests, regex, python-dotenv, protobuf, propcache, orjson, multidict, marshmallow, markdown, Mako, jsonpatch, itsdangerous, httpx-sse, gunicorn, greenlet, graphql-core, frozenlist, faiss-cpu, deprecation, blinker, aiohappyeyeballs, yarl, tiktoken, SQLAlchemy, requests-toolbelt, graphql-relay, Flask, docker, dataclasses-json, aiosignal, pydantic-settings, langsmith, graphene, alembic, aiohttp, langchain-core, mlflow-skinny, langchain-text-splitters, mlflow, langchain, databricks-vectorsearch, langchain-databricks, langchain-community\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1a08ac84-dc30-4a17-85de-604a62e62bb2\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1a08ac84-dc30-4a17-85de-604a62e62bb2\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.7.0\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1a08ac84-dc30-4a17-85de-604a62e62bb2\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.22.0\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1a08ac84-dc30-4a17-85de-604a62e62bb2\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngrpcio-status 1.67.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed Flask-3.1.2 Mako-1.3.10 SQLAlchemy-2.0.45 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 alembic-1.17.2 blinker-1.9.0 databricks-vectorsearch-0.40 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 faiss-cpu-1.13.2 frozenlist-1.8.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.0 gunicorn-23.0.0 httpx-sse-0.4.3 itsdangerous-2.2.0 jsonpatch-1.33 langchain-0.3.27 langchain-community-0.3.31 langchain-core-0.3.81 langchain-databricks-0.1.2 langchain-text-splitters-0.3.11 langsmith-0.5.2 markdown-3.10 marshmallow-3.26.2 mlflow-2.22.4 mlflow-skinny-2.22.4 multidict-6.7.0 orjson-3.11.5 propcache-0.4.1 protobuf-4.25.8 pydantic-settings-2.12.0 python-dotenv-1.2.1 regex-2025.11.3 requests-2.32.5 requests-toolbelt-1.0.0 tiktoken-0.12.0 typing-inspect-0.9.0 typing-inspection-0.4.2 uuid-utils-0.12.0 werkzeug-3.1.4 yarl-1.22.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain langchain-community langchain-databricks faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b641c7-0eb8-4e2f-8d17-20c47d967d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef54e8-4f3d-42ab-b96f-b60ca42326f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcda38ce-d178-418a-9f3e-66c0df7ecdcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Raw Token Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b1f725-60ba-4c28-b19e-21107de2b062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-1a08ac84-dc30-4a17-85de-60/.ipykernel/3611/command-8016179771135666-1259243950:8: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n/home/spark-1a08ac84-dc30-4a17-85de-60/.ipykernel/3611/command-8016179771135666-1259243950:9: LangChainDeprecationWarning: Use databricks_langchain.DatabricksEmbeddings\n  embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 1) Databricks LLM + Embeddings\n",
    "# -----------------------\n",
    "# Make sure your Databricks auth is configured (e.g., DATABRICKS_HOST + DATABRICKS_TOKEN)\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"  # <-- change to your embedding endpoint name\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b03608-cbeb-476e-8ee0-28e4db57563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Sentence split + sentence-window splitter (LlamaIndex-like)\n",
    "# -----------------------\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[。！？!?])\\s+|\\n+\")\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def sentence_window_splitter(\n",
    "    documents: List[Document],\n",
    "    window_size: int = 2,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Mimics LlamaIndex SentenceWindowNodeParser:\n",
    "    - Each chunk is one core sentence\n",
    "    - metadata includes:\n",
    "        - original_text: core sentence\n",
    "        - window: context window (core +/- window_size sentences)\n",
    "    \"\"\"\n",
    "    out: List[Document] = []\n",
    "    for doc in documents:\n",
    "        sents = split_sentences(doc.page_content)\n",
    "        for i, core in enumerate(sents):\n",
    "            lo = max(0, i - window_size)\n",
    "            hi = min(len(sents), i + window_size + 1)\n",
    "            window_text = \" \".join(sents[lo:hi])\n",
    "\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=core,\n",
    "                    metadata={\n",
    "                        **(doc.metadata or {}),\n",
    "                        \"original_text\": core,\n",
    "                        \"window\": window_text,\n",
    "                        \"sent_index\": i,\n",
    "                        \"window_size\": window_size,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a46cdd-8ca8-421c-acc2-b26138b51c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Prompt + QA\n",
    "# -----------------------\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful technical assistant. Answer using ONLY the provided context. \"\n",
    "     \"If the context is insufficient, say what is missing.\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nAnswer in English:\")\n",
    "])\n",
    "\n",
    "def answer_with_retrieval(\n",
    "    docs: List[Document],\n",
    "    question: str,\n",
    "    top_k: int = 5,\n",
    "    use_window_metadata: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build FAISS index, retrieve top_k chunks, print retrieved chunks, then ask the Databricks LLM.\n",
    "    If use_window_metadata=True, feed metadata['window'] as context (Sentence Window style).\n",
    "    \"\"\"\n",
    "    vs = FAISS.from_documents(docs, embeddings)\n",
    "    retrieved = vs.similarity_search_with_score(question, k=top_k)\n",
    "\n",
    "    print(\"\\n--- Top retrieved chunks ---\")\n",
    "    context_blocks = []\n",
    "    for rank, (d, score) in enumerate(retrieved, 1):\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            window = d.metadata.get(\"window\", d.page_content)\n",
    "            core = d.metadata.get(\"original_text\", d.page_content)\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(f\"Core: {core}\")\n",
    "            print(f\"Window:\\n{window}\")\n",
    "            context_blocks.append(window)\n",
    "        else:\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(d.page_content)\n",
    "            context_blocks.append(d.page_content)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    msg = QA_PROMPT.format_messages(question=question, context=context)\n",
    "    resp = llm.invoke(msg)\n",
    "\n",
    "    print(\"\\n--- LLM Answer ---\")\n",
    "    print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65d9762-ddc9-48d9-9d54-09f8a1931f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4) Runner (prints raw chunks + retrieval behavior)\n",
    "# -----------------------\n",
    "def evaluate_splitter(\n",
    "    splitter_name: str,\n",
    "    split_fn: Callable[[], List[Document]],\n",
    "    question: str,\n",
    "    use_window_metadata: bool = False,\n",
    "    top_k: int = 5,\n",
    "    max_print_chunks: int = 50,\n",
    ") -> None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing splitter: {splitter_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    chunks = split_fn()\n",
    "\n",
    "    print(f\"\\n[Raw chunks generated] total={len(chunks)}\")\n",
    "    for i, d in enumerate(chunks[:max_print_chunks], 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            print(f\"Core: {d.metadata.get('original_text')}\")\n",
    "            print(f\"Window: {d.metadata.get('window')}\")\n",
    "        else:\n",
    "            print(d.page_content)\n",
    "\n",
    "    if len(chunks) > max_print_chunks:\n",
    "        print(f\"\\n... (only printed first {max_print_chunks} chunks)\")\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer_with_retrieval(\n",
    "        docs=chunks,\n",
    "        question=question,\n",
    "        top_k=top_k,\n",
    "        use_window_metadata=use_window_metadata,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{splitter_name} done.\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1906460-b1a6-4bb1-b7a8-b294fd615fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nTesting splitter: Token Split (chunk_size=30, overlap=0)\n============================================================\n\n[Raw chunks generated] total=20\n\n--- Chunk 1 ---\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n--- Chunk 2 ---\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and\n\n--- Chunk 3 ---\n use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific\n\n--- Chunk 4 ---\n search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector index\n\n--- Chunk 5 ---\ning,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 6 ---\n\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise\n\n--- Chunk 7 ---\n and efficient.\n\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference,\n\n--- Chunk 8 ---\n a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language\n\n--- Chunk 9 ---\n Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is\n\n--- Chunk 10 ---\n less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the\n\n--- Chunk 11 ---\n AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful\n\n--- Chunk 12 ---\n capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression\n\n--- Chunk 13 ---\n, and clustering.\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of\n\n--- Chunk 14 ---\n complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking\n\n--- Chunk 15 ---\n strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n\n\n--- Chunk 16 ---\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\n\n\n--- Chunk 17 ---\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries\n\n--- Chunk 18 ---\n.\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth\n\n--- Chunk 19 ---\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n--- Chunk 20 ---\n characteristics\nof the data and the expected query types.\n\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3183\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n[2] score=0.3336\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n[3] score=0.3830\n search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector index\n\n[4] score=0.4250\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and\n\n[5] score=0.4327\n complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector index\n5. Text generation\n\nSentence window chunking and semantic chunking differ in that sentence window chunking is an advanced chunking technique that is not explicitly described in the provided context, whereas semantic chunking is not mentioned at all. However, based on the general concept of chunking in RAG systems, it can be inferred that semantic chunking likely involves dividing text into meaningful units based on their semantic relationships or meanings.\n\nToken Split (chunk_size=30, overlap=0) done.\n============================================================\n\n\n============================================================\nTesting splitter: Token Split (chunk_size=30, overlap=10)\n============================================================\n\n[Raw chunks generated] total=29\n\n--- Chunk 1 ---\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n--- Chunk 2 ---\n-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\n\n--- Chunk 3 ---\n model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\n\n\n--- Chunk 4 ---\n use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific\n\n--- Chunk 5 ---\n answering,\nknowledge assistants, and domain-specific search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion,\n\n--- Chunk 6 ---\n of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis\n\n--- Chunk 7 ---\ning,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 8 ---\ns, databases, APIs, or web pages—\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred\n\n--- Chunk 9 ---\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n\nEach chunk is converted into\n\n--- Chunk 10 ---\n and efficient.\n\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference,\n\n--- Chunk 11 ---\n stored in a vector index.\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\n\n--- Chunk 12 ---\n\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nall\n\n--- Chunk 13 ---\n Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is\n\n--- Chunk 14 ---\n external knowledge.\n\n--- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-\n\n--- Chunk 15 ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem.\n\n--- Chunk 16 ---\n AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful\n\n--- Chunk 17 ---\n foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine\n\n--- Chunk 18 ---\ncikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering.\nTogether, these tools\n\n--- Chunk 19 ---\n, and clustering.\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of\n\n--- Chunk 20 ---\n practitioners,\nenabling efficient development and deployment of complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\n\n\n--- Chunk 21 ---\n another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target\n\n--- Chunk 22 ---\n strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n\n\n--- Chunk 23 ---\n“window” sentences as context.\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the\n\n--- Chunk 24 ---\n LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand,\n\n--- Chunk 25 ---\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries\n\n--- Chunk 26 ---\n than relying solely on fixed character counts or sentence boundaries.\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural\n\n--- Chunk 27 ---\n semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth advanced methods can significantly improve retrieval quality and\ndown\n\n--- Chunk 28 ---\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n--- Chunk 29 ---\nosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3183\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n[2] score=0.3336\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n[3] score=0.3912\n LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand,\n\n[4] score=0.4275\n-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\n\n[5] score=0.4342\n another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. External knowledge retrieval\n2. Language Model (LLM)\n3. Chunking strategy (which can be either sentence window chunking or semantic chunking)\n\nSentence window chunking and semantic chunking differ in that:\n\n* Sentence window chunking involves dividing the input text into fixed-size chunks, typically containing a target sentence or a specific context.\n* Semantic chunking, on the other hand, involves dividing the input text into chunks based on semantic meaning or concepts, rather than fixed-size windows.\n\nToken Split (chunk_size=30, overlap=10) done.\n============================================================\n\n\n============================================================\nTesting splitter: Sentence Window Split (window_size=2)\n============================================================\n\n[Raw chunks generated] total=44\n\n--- Chunk 1 ---\nCore: Retrieval-Augmented Generation (RAG) is a common architecture for building\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n\n--- Chunk 2 ---\nCore: LLM-powered applications that combine external knowledge retrieval with\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems\n\n--- Chunk 3 ---\nCore: text generation.\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as\n\n--- Chunk 4 ---\nCore: Instead of relying solely on a model’s internal parameters, RAG systems\nWindow: LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation.\n\n--- Chunk 5 ---\nCore: retrieve relevant information from external data sources and use it as\nWindow: text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering,\n\n--- Chunk 6 ---\nCore: context during answer generation.\nWindow: Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search.\n\n--- Chunk 7 ---\nCore: This approach is widely used for applications such as question answering,\nWindow: retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\n\n--- Chunk 8 ---\nCore: knowledge assistants, and domain-specific search.\nWindow: context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing,\n\n--- Chunk 9 ---\nCore: A typical RAG pipeline consists of several core components, including\nWindow: This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis.\n\n--- Chunk 10 ---\nCore: document ingestion, text chunking, embedding generation, vector indexing,\nWindow: knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 11 ---\nCore: retrieval, and response synthesis.\nWindow: A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed.\n\n--- Chunk 12 ---\nCore: Documents from various sources—such as PDFs, databases, APIs, or web pages—\nWindow: document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\n\n--- Chunk 13 ---\nCore: are first ingested and preprocessed.\nWindow: retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient.\n\n--- Chunk 14 ---\nCore: They are then split into smaller units, often referred to as chunks,\nWindow: Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\n\n--- Chunk 15 ---\nCore: to make retrieval more precise and efficient.\nWindow: are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index.\n\n--- Chunk 16 ---\nCore: Each chunk is converted into a vector representation using an embedding model\nWindow: They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed\n\n--- Chunk 17 ---\nCore: and stored in a vector index.\nWindow: to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks.\n\n--- Chunk 18 ---\nCore: During inference, a user query is embedded and compared against the indexed\nWindow: Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\n\n--- Chunk 19 ---\nCore: vectors to retrieve the most relevant chunks.\nWindow: and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\n--- Chunk 20 ---\nCore: These retrieved chunks are provided to a Large Language Model (LLM) as context,\nWindow: During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\n--- Chunk 21 ---\nCore: allowing the model to generate answers that are grounded in external knowledge.\nWindow: vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used\n\n--- Chunk 22 ---\nCore: --- The following content is less directly related to the RAG topic ---\nWindow: These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem.\n\n--- Chunk 23 ---\nCore: In addition, Python, as a general-purpose programming language, is widely used\nWindow: allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\n\n--- Chunk 24 ---\nCore: in the AI field due to its simplicity and rich ecosystem.\nWindow: --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data.\n\n--- Chunk 25 ---\nCore: For example, NumPy and Pandas are foundational tools for data processing,\nWindow: In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\n\n--- Chunk 26 ---\nCore: providing powerful capabilities for numerical computation and structured data.\nWindow: in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering.\n\n--- Chunk 27 ---\nCore: Scikit-learn offers a comprehensive suite of machine learning algorithms\nWindow: For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n\n--- Chunk 28 ---\nCore: for tasks such as classification, regression, and clustering.\nWindow: providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models.\n\n--- Chunk 29 ---\nCore: Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nWindow: Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\n--- Chunk 30 ---\nCore: enabling efficient development and deployment of complex AI models.\nWindow: for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk\n\n--- Chunk 31 ---\nCore: --- The following is another related but conceptually independent section ---\nWindow: Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding\n\n--- Chunk 32 ---\nCore: Sentence window chunking is an advanced chunking strategy in which each chunk\nWindow: enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n\n--- Chunk 33 ---\nCore: contains a target sentence along with a configurable number of surrounding\nWindow: --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\n\n--- Chunk 34 ---\nCore: “window” sentences as context.\nWindow: Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers.\n\n--- Chunk 35 ---\nCore: This approach aims to provide rich local context to the LLM during retrieval,\nWindow: contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\n\n--- Chunk 36 ---\nCore: thereby improving the coherence and factual consistency of generated answers.\nWindow: “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries.\n\n--- Chunk 37 ---\nCore: Semantic chunking, on the other hand, attempts to split text based on semantic\nWindow: This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n\n--- Chunk 38 ---\nCore: content rather than relying solely on fixed character counts or sentence boundaries.\nWindow: thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\n--- Chunk 39 ---\nCore: It leverages embedding models to compute semantic similarity between sentences\nWindow: Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\n\n--- Chunk 40 ---\nCore: or phrases and identify natural breakpoints where topics or meanings shift.\nWindow: content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications.\n\n--- Chunk 41 ---\nCore: Both advanced methods can significantly improve retrieval quality and\nWindow: It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\n\n--- Chunk 42 ---\nCore: downstream generation performance in RAG applications.\nWindow: or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\n--- Chunk 43 ---\nCore: Choosing the right chunking strategy typically depends on the characteristics\nWindow: Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\n--- Chunk 44 ---\nCore: of the data and the expected query types.\nWindow: downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3029\nCore: Retrieval-Augmented Generation (RAG) is a common architecture for building\nWindow:\nRetrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n\n[2] score=0.3936\nCore: Sentence window chunking is an advanced chunking strategy in which each chunk\nWindow:\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n\n[3] score=0.4610\nCore: Semantic chunking, on the other hand, attempts to split text based on semantic\nWindow:\nThis approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n\n[4] score=0.4851\nCore: document ingestion, text chunking, embedding generation, vector indexing,\nWindow:\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n\n[5] score=0.5718\nCore: vectors to retrieve the most relevant chunks.\nWindow:\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion: This involves collecting and storing documents from various sources such as PDFs, databases, APIs, or web pages.\n2. Text chunking: This is the process of splitting the ingested documents into smaller chunks, which can be done using sentence window chunking or semantic chunking.\n3. Embedding generation: This involves generating vector representations of the chunked text, which are used for indexing and retrieval.\n4. Vector indexing: This is the process of storing the generated vectors in an index, which allows for efficient retrieval of relevant chunks.\n5. Retrieval: This involves comparing a user query against the indexed vectors to retrieve the most relevant chunks.\n6. Response synthesis: This is the final step, where the retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\nSentence window chunking and semantic chunking differ in their approach to splitting text into chunks:\n\n* Sentence window chunking involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context.\n* Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries, using embedding models to compute semantic similarity between sentences.\n\nSentence Window Split (window_size=2) done.\n============================================================\n\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 5) Example document + question (English version)\n",
    "# -----------------------\n",
    "documents = [\n",
    "    Document(page_content=\"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
    "LLM-powered applications that combine external knowledge retrieval with\n",
    "text generation.\n",
    "Instead of relying solely on a model’s internal parameters, RAG systems\n",
    "retrieve relevant information from external data sources and use it as\n",
    "context during answer generation.\n",
    "This approach is widely used for applications such as question answering,\n",
    "knowledge assistants, and domain-specific search.\n",
    "\n",
    "A typical RAG pipeline consists of several core components, including\n",
    "document ingestion, text chunking, embedding generation, vector indexing,\n",
    "retrieval, and response synthesis.\n",
    "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
    "are first ingested and preprocessed.\n",
    "They are then split into smaller units, often referred to as chunks,\n",
    "to make retrieval more precise and efficient.\n",
    "\n",
    "Each chunk is converted into a vector representation using an embedding model\n",
    "and stored in a vector index.\n",
    "During inference, a user query is embedded and compared against the indexed\n",
    "vectors to retrieve the most relevant chunks.\n",
    "These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
    "allowing the model to generate answers that are grounded in external knowledge.\n",
    "\n",
    "--- The following content is less directly related to the RAG topic ---\n",
    "\n",
    "In addition, Python, as a general-purpose programming language, is widely used\n",
    "in the AI field due to its simplicity and rich ecosystem.\n",
    "For example, NumPy and Pandas are foundational tools for data processing,\n",
    "providing powerful capabilities for numerical computation and structured data.\n",
    "Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
    "for tasks such as classification, regression, and clustering.\n",
    "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
    "enabling efficient development and deployment of complex AI models.\n",
    "\n",
    "--- The following is another related but conceptually independent section ---\n",
    "\n",
    "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
    "contains a target sentence along with a configurable number of surrounding\n",
    "“window” sentences as context.\n",
    "This approach aims to provide rich local context to the LLM during retrieval,\n",
    "thereby improving the coherence and factual consistency of generated answers.\n",
    "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
    "content rather than relying solely on fixed character counts or sentence boundaries.\n",
    "It leverages embedding models to compute semantic similarity between sentences\n",
    "or phrases and identify natural breakpoints where topics or meanings shift.\n",
    "Both advanced methods can significantly improve retrieval quality and\n",
    "downstream generation performance in RAG applications.\n",
    "Choosing the right chunking strategy typically depends on the characteristics\n",
    "of the data and the expected query types.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "question = (\n",
    "    \"What are the main components of a Retrieval-Augmented Generation (RAG) system, \"\n",
    "    \"and how do sentence window chunking and semantic chunking differ?\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Run splitters\n",
    "# -----------------------\n",
    "# Token-based split (chunk_size=30, overlap=0)\n",
    "splitter_a = TokenTextSplitter(chunk_size=30, chunk_overlap=0)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=0)\",\n",
    "    split_fn=lambda: splitter_a.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Token-based split (chunk_size=30, overlap=10)\n",
    "splitter_b = TokenTextSplitter(chunk_size=30, chunk_overlap=10)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=10)\",\n",
    "    split_fn=lambda: splitter_b.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Sentence-window split (window_size=2)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Sentence Window Split (window_size=2)\",\n",
    "    split_fn=lambda: sentence_window_splitter(documents, window_size=2),\n",
    "    question=question,\n",
    "    use_window_metadata=True,   # feed metadata['window'] into LLM context\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981572fb-afc9-4af8-891d-4fcc08ff1c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Token Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9013498-c13b-4207-b3a0-6aada21b7290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nRunning RAG pipeline with splitter: RecursiveCharacterTextSplitter\n======================================================================\n\n[RecursiveCharacterTextSplitter] Generated document chunks:\n\n--- Chunk 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search.\n----------------------------------------\n\n--- Chunk 2 ---\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n----------------------------------------\n\n--- Chunk 3 ---\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is less directly related to the RAG topic ---\n----------------------------------------\n\n--- Chunk 4 ---\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering.\n----------------------------------------\n\n--- Chunk 5 ---\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models.\n----------------------------------------\n\n--- Chunk 6 ---\n--- The following is another related but conceptually independent section ---\n----------------------------------------\n\n--- Chunk 7 ---\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries.\n----------------------------------------\n\n--- Chunk 8 ---\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\nBuilding vector store...\n\nQuestion:\nWhat are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\nModel Answer:\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-1a08ac84-dc30-4a17-85de-60/.ipykernel/3611/command-8016179771135672-3916514322:57: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  result = qa_chain(question)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector indexing\n5. Retrieval\n6. Response synthesis\n\nRegarding sentence window chunking and semantic chunking, here's how they differ:\n\n* Sentence window chunking: This approach involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context. This provides rich local context to the Large Language Model (LLM) during retrieval, aiming to improve the coherence and factual consistency of generated answers.\n* Semantic chunking: This method attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. Each chunk is converted into a vector representation using an embedding model and stored in a vector index.\n\n[RecursiveCharacterTextSplitter] Retrieved source documents:\n\n--- Source Document 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search.\n------------------------------------------------------------\n\n--- Source Document 2 ---\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\n--- Source Document 3 ---\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries.\n------------------------------------------------------------\n\n--- Source Document 4 ---\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is less directly related to the RAG topic ---\n------------------------------------------------------------\n\n--- Source Document 5 ---\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n------------------------------------------------------------\n\nFinished RAG pipeline with RecursiveCharacterTextSplitter\n======================================================================\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "\n",
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG pipeline with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 1: Split documents\n",
    "    # -----------------------\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[{splitter_name}] Generated document chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(chunk.page_content.strip())\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 2: Build vector store\n",
    "    # -----------------------\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 3: Build RAG chain\n",
    "    # -----------------------\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 4: Ask question\n",
    "    # -----------------------\n",
    "    print(f\"\\nQuestion:\\n{question}\\n\")\n",
    "    print(\"Model Answer:\\n\")\n",
    "\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 5: Show retrieved context\n",
    "    # -----------------------\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        print(f\"\\n--- Source Document {i} ---\")\n",
    "        print(doc.page_content.strip())\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG pipeline with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=sentence_splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"RecursiveCharacterTextSplitter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "286e5107-183c-47af-a280-e5e3fedb193a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Sentence Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d162956-a99a-4eb3-abcb-8ebe80c177f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SentenceWindowTextSplitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size: int = 3,\n",
    "        window_metadata_key: str = \"window\",\n",
    "        original_text_metadata_key: str = \"original_text\",\n",
    "    ):\n",
    "        self.window_size = window_size\n",
    "        self.window_metadata_key = window_metadata_key\n",
    "        self.original_text_metadata_key = original_text_metadata_key\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks = []\n",
    "\n",
    "        for doc in documents:\n",
    "            sentences = [\n",
    "                s.strip()\n",
    "                for s in re.split(r\"(?<=[.!?])\\s+\", doc.page_content)\n",
    "                if s.strip()\n",
    "            ]\n",
    "\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(sentences), i + self.window_size + 1)\n",
    "\n",
    "                window_sentences = sentences[start:end]\n",
    "                window_text = \" \".join(window_sentences)\n",
    "\n",
    "                chunks.append(\n",
    "                    Document(\n",
    "                        page_content=window_text,\n",
    "                        metadata={\n",
    "                            self.window_metadata_key: window_text,\n",
    "                            self.original_text_metadata_key: sentence,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # --- Split documents ---\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[{splitter_name}] Generated chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if \"original_text\" in chunk.metadata:\n",
    "            print(\"Center sentence:\")\n",
    "            print(chunk.metadata[\"original_text\"])\n",
    "            print(\"\\nWindow context:\")\n",
    "            print(chunk.metadata[\"window\"])\n",
    "        else:\n",
    "            print(chunk.page_content)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # --- Vector store ---\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # --- RAG chain ---\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # --- Query ---\n",
    "    print(f\"\\nQuestion:\\n{question}\\n\")\n",
    "    print(\"Model Answer:\\n\")\n",
    "\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # --- Retrieved context ---\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        print(f\"\\n--- Source Document {i} ---\")\n",
    "        if \"original_text\" in doc.metadata:\n",
    "            print(\"Center sentence:\")\n",
    "            print(doc.metadata[\"original_text\"])\n",
    "            print(\"\\nWindow context:\")\n",
    "            print(doc.metadata[\"window\"])\n",
    "        else:\n",
    "            print(doc.page_content)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd771461-9f4a-4c5c-b4f1-ed904ec7a544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nRunning RAG with splitter: Sentence Window (LangChain)\n======================================================================\n\n[Sentence Window (LangChain)] Generated chunks:\n\n--- Chunk 1 ---\nCenter sentence:\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation.\n\nWindow context:\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis.\n----------------------------------------\n\n--- Chunk 2 ---\nCenter sentence:\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\n\nWindow context:\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\n----------------------------------------\n\n--- Chunk 3 ---\nCenter sentence:\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search.\n\nWindow context:\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n----------------------------------------\n\n--- Chunk 4 ---\nCenter sentence:\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis.\n\nWindow context:\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\n----------------------------------------\n\n--- Chunk 5 ---\nCenter sentence:\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\n\nWindow context:\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\n----------------------------------------\n\n--- Chunk 6 ---\nCenter sentence:\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n\nWindow context:\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n----------------------------------------\n\n--- Chunk 7 ---\nCenter sentence:\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\n\nWindow context:\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem.\n----------------------------------------\n\n--- Chunk 8 ---\nCenter sentence:\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\n\nWindow context:\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\n----------------------------------------\n\n--- Chunk 9 ---\nCenter sentence:\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\nWindow context:\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering.\n----------------------------------------\n\n--- Chunk 10 ---\nCenter sentence:\n--- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem.\n\nWindow context:\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models.\n----------------------------------------\n\n--- Chunk 11 ---\nCenter sentence:\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\n\nWindow context:\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n----------------------------------------\n\n--- Chunk 12 ---\nCenter sentence:\nScikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering.\n\nWindow context:\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\n----------------------------------------\n\n--- Chunk 13 ---\nCenter sentence:\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models.\n\nWindow context:\n--- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries.\n----------------------------------------\n\n--- Chunk 14 ---\nCenter sentence:\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n\nWindow context:\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\n----------------------------------------\n\n--- Chunk 15 ---\nCenter sentence:\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\n\nWindow context:\nScikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\n----------------------------------------\n\n--- Chunk 16 ---\nCenter sentence:\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries.\n\nWindow context:\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\n--- Chunk 17 ---\nCenter sentence:\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\n\nWindow context:\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\n--- Chunk 18 ---\nCenter sentence:\nBoth advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\n\nWindow context:\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\n--- Chunk 19 ---\nCenter sentence:\nChoosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\nWindow context:\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\nBuilding vector store...\n\nQuestion:\nWhat are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\nModel Answer:\n\nAccording to the provided context, the main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion: This involves ingesting documents from various sources, such as PDFs, databases, APIs, or web pages.\n2. Text chunking: This involves splitting the ingested documents into smaller units, often referred to as chunks, to make retrieval more precise and efficient.\n3. Embedding generation: Each chunk is converted into a vector representation using an embedding model.\n4. Vector indexing: The vector representations are stored in a vector index.\n5. Retrieval: Relevant information is retrieved from the vector index.\n6. Response synthesis: The retrieved information is used to generate a response.\n\nRegarding sentence window chunking and semantic chunking, they are both advanced chunking strategies used in RAG systems. The main differences between them are:\n\n* Sentence window chunking: This approach involves creating chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context. This provides rich local context to the LLM during retrieval, improving the coherence and factual consistency of generated answers.\n* Semantic chunking: This approach attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\n[Sentence Window (LangChain)] Retrieved source documents:\n\n--- Source Document 1 ---\nCenter sentence:\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\n\nWindow context:\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\n--- Source Document 2 ---\nCenter sentence:\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search.\n\nWindow context:\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n------------------------------------------------------------\n\n--- Source Document 3 ---\nCenter sentence:\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries.\n\nWindow context:\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\n--- Source Document 4 ---\nCenter sentence:\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis.\n\nWindow context:\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\n------------------------------------------------------------\n\n--- Source Document 5 ---\nCenter sentence:\nBoth advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\n\nWindow context:\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\nFinished RAG with Sentence Window (LangChain)\n======================================================================\n\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 5. Run Sentence Window Chunking (LlamaIndex-style)\n",
    "# =========================================================\n",
    "\n",
    "sentence_window_splitter = SentenceWindowTextSplitter(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=sentence_window_splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Sentence Window (LangChain)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "301d1eee-f56f-4969-a5d6-8da95ed0a278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Sliding Window Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec04500b-838c-414f-96ba-d7146e9521b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SlidingWindowTextSplitter:\n",
    "    \"\"\"\n",
    "    Sliding window over sentences:\n",
    "    - window_size: number of sentences per chunk\n",
    "    - stride: how many sentences to move forward each step\n",
    "      (stride < window_size => overlap, stride == window_size => no overlap)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size: int = 5,\n",
    "        stride: int = 2,\n",
    "        window_metadata_key: str = \"window\",\n",
    "        start_idx_metadata_key: str = \"start_sentence_idx\",\n",
    "        end_idx_metadata_key: str = \"end_sentence_idx\",\n",
    "    ):\n",
    "        if window_size <= 0:\n",
    "            raise ValueError(\"window_size must be > 0\")\n",
    "        if stride <= 0:\n",
    "            raise ValueError(\"stride must be > 0\")\n",
    "        if stride > window_size:\n",
    "            # Allowed, but usually not what you want (gaps between windows)\n",
    "            pass\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.window_metadata_key = window_metadata_key\n",
    "        self.start_idx_metadata_key = start_idx_metadata_key\n",
    "        self.end_idx_metadata_key = end_idx_metadata_key\n",
    "\n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        # Same splitting logic as your sentence window splitter\n",
    "        return [\n",
    "            s.strip()\n",
    "            for s in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "            if s.strip()\n",
    "        ]\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        chunks: List[Document] = []\n",
    "\n",
    "        for doc in documents:\n",
    "            sentences = self._split_sentences(doc.page_content)\n",
    "            n = len(sentences)\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            # Sliding windows\n",
    "            for start in range(0, n, self.stride):\n",
    "                end = min(start + self.window_size, n)\n",
    "                window_sentences = sentences[start:end]\n",
    "                window_text = \" \".join(window_sentences)\n",
    "\n",
    "                chunks.append(\n",
    "                    Document(\n",
    "                        page_content=window_text,\n",
    "                        metadata={\n",
    "                            self.window_metadata_key: window_text,\n",
    "                            self.start_idx_metadata_key: start,\n",
    "                            self.end_idx_metadata_key: end - 1,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                if end == n:\n",
    "                    break  # reached the end\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fcc1481-73ee-4edd-80a0-45ea3ee1bb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nRunning RAG with splitter: Sliding Window (size=5, stride=2)\n======================================================================\n\n[Sliding Window (size=5, stride=2)] Generated chunks:\n\n--- Chunk 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\n----------------------------------------\n\n--- Chunk 2 ---\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\n----------------------------------------\n\n--- Chunk 3 ---\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n----------------------------------------\n\n--- Chunk 4 ---\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\n----------------------------------------\n\n--- Chunk 5 ---\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models.\n----------------------------------------\n\n--- Chunk 6 ---\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\n----------------------------------------\n\n--- Chunk 7 ---\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\n----------------------------------------\n\n--- Chunk 8 ---\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\nBuilding vector store...\n\nQuestion:\nWhat are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\nModel Answer:\n\nAccording to the provided context, the main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector indexing\n5. Retrieval\n6. Response synthesis\n\nRegarding text chunking, the two advanced methods mentioned are:\n\n1. **Sentence window chunking**: This approach involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context. This provides rich local context to the LLM during retrieval, improving the coherence and factual consistency of generated answers.\n2. **Semantic chunking**: This method attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\nIn summary, sentence window chunking focuses on providing local context to the LLM, while semantic chunking focuses on splitting text based on semantic content.\n\n[Sliding Window (size=5, stride=2)] Retrieved source documents:\n\n--- Source Document 1 ---\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\n--- Source Document 2 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\n------------------------------------------------------------\n\n--- Source Document 3 ---\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\n------------------------------------------------------------\n\n--- Source Document 4 ---\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\n------------------------------------------------------------\n\n--- Source Document 5 ---\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n------------------------------------------------------------\n\nFinished RAG with Sliding Window (size=5, stride=2)\n======================================================================\n\n"
     ]
    }
   ],
   "source": [
    "splitter = SlidingWindowTextSplitter(window_size=5, stride=2)\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Sliding Window (size=5, stride=2)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6d2794-278c-4eda-bc03-4a4394dca565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Semantic Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf2c75e-cad7-4ea8-99a1-0562967be063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bf9c66-afca-450a-b07b-ab400dd6fa46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Analysis helpers (chunk stats + retrieval scores)\n",
    "# -----------------------\n",
    "def _approx_token_count(text: str) -> int:\n",
    "    # Cheap proxy; good enough for comparing splitters\n",
    "    return max(1, len(text.split()))\n",
    "\n",
    "\n",
    "def analyze_chunks(chunks: List[Document], name: str) -> Dict[str, Any]:\n",
    "    lengths = [_approx_token_count(c.page_content) for c in chunks]\n",
    "    lengths_sorted = sorted(lengths)\n",
    "    n = len(lengths_sorted)\n",
    "\n",
    "    def _p(pct: float) -> int:\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        idx = min(n - 1, int(round((pct / 100.0) * (n - 1))))\n",
    "        return lengths_sorted[idx]\n",
    "\n",
    "    out = {\n",
    "        \"splitter\": name,\n",
    "        \"num_chunks\": n,\n",
    "        \"min_tokens\": min(lengths) if lengths else 0,\n",
    "        \"p50_tokens\": _p(50),\n",
    "        \"p90_tokens\": _p(90),\n",
    "        \"max_tokens\": max(lengths) if lengths else 0,\n",
    "        \"avg_tokens\": round(sum(lengths) / n, 2) if n else 0,\n",
    "        \"exact_duplicate_chunks\": n - len({c.page_content for c in chunks}),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def retrieve_with_scores(vectorstore: FAISS, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "    # FAISS supports similarity_search_with_score\n",
    "    return vectorstore.similarity_search_with_score(query, k=k)\n",
    "\n",
    "\n",
    "def analyze_retrieval(\n",
    "    retrieved: List[Tuple[Document, float]],\n",
    "    keywords: Optional[List[str]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    scores = [s for _, s in retrieved]\n",
    "    out = {\n",
    "        \"k\": len(retrieved),\n",
    "        \"best_score\": float(min(scores)) if scores else None,\n",
    "        \"worst_score\": float(max(scores)) if scores else None,\n",
    "        \"avg_score\": float(sum(scores) / len(scores)) if scores else None,\n",
    "    }\n",
    "\n",
    "    if keywords:\n",
    "        hits = 0\n",
    "        for doc, _ in retrieved:\n",
    "            t = doc.page_content.lower()\n",
    "            if any(kw.lower() in t for kw in keywords):\n",
    "                hits += 1\n",
    "        out[\"keyword_hits\"] = hits\n",
    "        out[\"keywords\"] = keywords\n",
    "    return out\n",
    "\n",
    "\n",
    "def print_analysis(chunk_stats: Dict[str, Any], retrieval_stats: Dict[str, Any]) -> None:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"Chunk stats:\")\n",
    "    for k, v in chunk_stats.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\n",
    "    print(\"\\nRetrieval stats (FAISS similarity_search_with_score):\")\n",
    "    for k, v in retrieval_stats.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\n",
    "    print(\"=\" * 70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4993922-0ed8-4728-95ca-9fb193b75de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Semantic Splitter (LangChain version)\n",
    "#    - Sentence tokenize\n",
    "#    - Embed each sentence\n",
    "#    - Compute adjacent cosine similarity\n",
    "#    - Break at low-similarity points by percentile threshold\n",
    "#    - Optional buffer_size to include neighbors around boundaries\n",
    "# -----------------------\n",
    "def sentence_tokenize_en(text: str) -> List[str]:\n",
    "    # Simple sentence splitter; you can swap with nltk/spacy if you prefer\n",
    "    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
    "    return sents\n",
    "\n",
    "\n",
    "def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "\n",
    "class SemanticTextSplitter:\n",
    "    \"\"\"\n",
    "    LangChain-style semantic chunker inspired by LlamaIndex SemanticSplitterNodeParser.\n",
    "\n",
    "    breakpoint_percentile_threshold:\n",
    "        If 95: break on the lowest 5% similarities (conservative, fewer breaks)\n",
    "        If 5:  break on the lowest 95% similarities (aggressive, many breaks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder,\n",
    "        sentence_splitter=sentence_tokenize_en,\n",
    "        breakpoint_percentile_threshold: float = 95,\n",
    "        buffer_size: int = 0,\n",
    "        min_sentences_per_chunk: int = 1,\n",
    "        max_sentences_per_chunk: Optional[int] = None,\n",
    "    ):\n",
    "        self.embedder = embedder\n",
    "        self.sentence_splitter = sentence_splitter\n",
    "        self.breakpoint_percentile_threshold = breakpoint_percentile_threshold\n",
    "        self.buffer_size = buffer_size\n",
    "        self.min_sentences_per_chunk = min_sentences_per_chunk\n",
    "        self.max_sentences_per_chunk = max_sentences_per_chunk\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        out_chunks: List[Document] = []\n",
    "\n",
    "        for doc in documents:\n",
    "            sentences = self.sentence_splitter(doc.page_content)\n",
    "            if len(sentences) == 0:\n",
    "                continue\n",
    "            if len(sentences) == 1:\n",
    "                out_chunks.append(Document(page_content=sentences[0], metadata=dict(doc.metadata or {})))\n",
    "                continue\n",
    "\n",
    "            # Embed sentences (Databricks embeddings returns list[list[float]])\n",
    "            sent_vecs = self.embedder.embed_documents(sentences)\n",
    "            sent_vecs = [np.array(v, dtype=np.float32) for v in sent_vecs]\n",
    "\n",
    "            sims = [_cosine_sim(sent_vecs[i], sent_vecs[i + 1]) for i in range(len(sent_vecs) - 1)]\n",
    "            # Breakpoints are where similarity is BELOW threshold value\n",
    "            threshold_val = float(np.percentile(sims, self.breakpoint_percentile_threshold))\n",
    "\n",
    "            break_idxs = [i for i, sim in enumerate(sims) if sim < threshold_val]\n",
    "            # i means \"between sentence i and i+1\" -> boundary at i+1\n",
    "            boundaries = set([0, len(sentences)])\n",
    "            for i in break_idxs:\n",
    "                boundaries.add(i + 1)\n",
    "\n",
    "            boundaries = sorted(boundaries)\n",
    "\n",
    "            # Create initial chunks from boundaries\n",
    "            chunks_sent_ranges: List[Tuple[int, int]] = []\n",
    "            for a, b in zip(boundaries[:-1], boundaries[1:]):\n",
    "                if a < b:\n",
    "                    chunks_sent_ranges.append((a, b))\n",
    "\n",
    "            # Apply buffer_size (expand ranges)\n",
    "            if self.buffer_size > 0:\n",
    "                expanded: List[Tuple[int, int]] = []\n",
    "                for a, b in chunks_sent_ranges:\n",
    "                    aa = max(0, a - self.buffer_size)\n",
    "                    bb = min(len(sentences), b + self.buffer_size)\n",
    "                    expanded.append((aa, bb))\n",
    "                # Merge overlaps after expansion\n",
    "                expanded.sort()\n",
    "                merged: List[Tuple[int, int]] = []\n",
    "                for a, b in expanded:\n",
    "                    if not merged or a > merged[-1][1]:\n",
    "                        merged.append((a, b))\n",
    "                    else:\n",
    "                        merged[-1] = (merged[-1][0], max(merged[-1][1], b))\n",
    "                chunks_sent_ranges = merged\n",
    "\n",
    "            # Enforce min/max sentences per chunk (optional)\n",
    "            final_ranges: List[Tuple[int, int]] = []\n",
    "            for a, b in chunks_sent_ranges:\n",
    "                # Ensure minimum sentences by merging forward if needed\n",
    "                if (b - a) < self.min_sentences_per_chunk and final_ranges:\n",
    "                    prev_a, prev_b = final_ranges.pop()\n",
    "                    final_ranges.append((prev_a, b))\n",
    "                else:\n",
    "                    final_ranges.append((a, b))\n",
    "\n",
    "            if self.max_sentences_per_chunk is not None:\n",
    "                # split overly large chunks\n",
    "                limited: List[Tuple[int, int]] = []\n",
    "                for a, b in final_ranges:\n",
    "                    cur = a\n",
    "                    while cur < b:\n",
    "                        nxt = min(b, cur + self.max_sentences_per_chunk)\n",
    "                        limited.append((cur, nxt))\n",
    "                        cur = nxt\n",
    "                final_ranges = limited\n",
    "\n",
    "            # Build Documents\n",
    "            base_meta = dict(doc.metadata or {})\n",
    "            for a, b in final_ranges:\n",
    "                text = \" \".join(sentences[a:b]).strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                meta = dict(base_meta)\n",
    "                # optional debug metadata\n",
    "                meta.update(\n",
    "                    {\n",
    "                        \"semantic_threshold_val\": threshold_val,\n",
    "                        \"semantic_percentile\": self.breakpoint_percentile_threshold,\n",
    "                        \"semantic_buffer_size\": self.buffer_size,\n",
    "                        \"sentence_range\": (a, b),\n",
    "                    }\n",
    "                )\n",
    "                out_chunks.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "        return out_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74128e68-5d7f-4bc5-bab4-a15c65806ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "    top_k: int = 5,\n",
    "    print_raw_chunks: bool = True,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # --- Split documents ---\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    if print_raw_chunks:\n",
    "        print(f\"[{splitter_name}] Generated chunks: total={len(chunks)}\")\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\n--- Chunk {i} ---\")\n",
    "            print(chunk.page_content)\n",
    "            if chunk.metadata:\n",
    "                # show only the semantic fields for readability\n",
    "                semantic_keys = [\"sentence_range\", \"semantic_percentile\", \"semantic_threshold_val\", \"semantic_buffer_size\"]\n",
    "                shown = {k: chunk.metadata.get(k) for k in semantic_keys if k in chunk.metadata}\n",
    "                if shown:\n",
    "                    print(f\"\\n[metadata] {shown}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    # --- Vector store ---\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    # --- Analysis: chunk stats + retrieval scores ---\n",
    "    chunk_stats = analyze_chunks(chunks, splitter_name)\n",
    "    retrieved = retrieve_with_scores(vectorstore, question, k=top_k)\n",
    "    keywords = [\"retrieval\", \"chunk\", \"chunking\", \"semantic\", \"embedding\", \"vector\", \"index\", \"rag\"]\n",
    "    retrieval_stats = analyze_retrieval(retrieved, keywords=keywords)\n",
    "    print_analysis(chunk_stats, retrieval_stats)\n",
    "\n",
    "    # --- RAG chain (stuff) ---\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # --- Query ---\n",
    "    print(f\"Question:\\n{question}\\n\")\n",
    "    print(\"--- LLM Answer ---\")\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # --- Retrieved context (with scores) ---\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents (with scores):\")\n",
    "    for i, (doc, score) in enumerate(retrieved, 1):\n",
    "        print(f\"\\n--- Source Document {i} --- score={score:.4f}\")\n",
    "        print(doc.page_content)\n",
    "        # show semantic debug metadata if present\n",
    "        if doc.metadata:\n",
    "            semantic_keys = [\"sentence_range\", \"semantic_percentile\", \"semantic_threshold_val\", \"semantic_buffer_size\"]\n",
    "            shown = {k: doc.metadata.get(k) for k in semantic_keys if k in doc.metadata}\n",
    "            if shown:\n",
    "                print(f\"\\n[metadata] {shown}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594b6d35-ab05-448f-8455-07f6a065fba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nRunning RAG with splitter: Semantic Split (percentile=95, buffer=1) - conservative\n======================================================================\n\n[Semantic Split (percentile=95, buffer=1) - conservative] Generated chunks: total=1\n\n--- Chunk 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\n[metadata] {'sentence_range': (0, 19), 'semantic_percentile': 95, 'semantic_threshold_val': 0.7079526960849761, 'semantic_buffer_size': 1}\n----------------------------------------\n\nBuilding vector store...\n\n======================================================================\nANALYSIS\n======================================================================\nChunk stats:\n  - splitter: Semantic Split (percentile=95, buffer=1) - conservative\n  - num_chunks: 1\n  - min_tokens: 414\n  - p50_tokens: 414\n  - p90_tokens: 414\n  - max_tokens: 414\n  - avg_tokens: 414.0\n  - exact_duplicate_chunks: 0\n\nRetrieval stats (FAISS similarity_search_with_score):\n  - k: 1\n  - best_score: 0.18783670663833618\n  - worst_score: 0.18783670663833618\n  - avg_score: 0.18783670663833618\n  - keyword_hits: 1\n  - keywords: ['retrieval', 'chunk', 'chunking', 'semantic', 'embedding', 'vector', 'index', 'rag']\n======================================================================\n\nQuestion:\nWhat are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- LLM Answer ---\nAccording to the provided context, the main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector indexing\n5. Retrieval\n6. Response synthesis\n\nRegarding sentence window chunking and semantic chunking, the context explains that:\n\n* Sentence window chunking involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context. This approach aims to provide rich local context to the LLM during retrieval.\n* Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\nIn summary, sentence window chunking focuses on providing local context to the LLM, while semantic chunking focuses on identifying meaningful breakpoints in the text based on semantic similarity.\n\n[Semantic Split (percentile=95, buffer=1) - conservative] Retrieved source documents (with scores):\n\n--- Source Document 1 --- score=0.1878\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\n[metadata] {'sentence_range': (0, 19), 'semantic_percentile': 95, 'semantic_threshold_val': 0.7079526960849761, 'semantic_buffer_size': 1}\n------------------------------------------------------------\n\nFinished RAG with Semantic Split (percentile=95, buffer=1) - conservative\n======================================================================\n\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 6) Run Semantic Splitter experiments (conservative vs aggressive)\n",
    "# -----------------------\n",
    "# Conservative: break at lowest 5% similarities (95th percentile threshold)\n",
    "semantic_splitter_conservative = SemanticTextSplitter(\n",
    "    embedder=embeddings,\n",
    "    sentence_splitter=sentence_tokenize_en,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    buffer_size=1,                 # include 1 neighboring sentence around boundaries\n",
    "    min_sentences_per_chunk=1,\n",
    ")\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=semantic_splitter_conservative,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Semantic Split (percentile=95, buffer=1) - conservative\",\n",
    "    print_raw_chunks=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57f6e15-8e45-418b-a6c8-0a81d14ef136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nRunning RAG with splitter: Semantic Split (percentile=5, buffer=1) - aggressive\n======================================================================\n\n[Semantic Split (percentile=5, buffer=1) - aggressive] Generated chunks: total=1\n\n--- Chunk 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\n[metadata] {'sentence_range': (0, 19), 'semantic_percentile': 5, 'semantic_threshold_val': 0.4696271479129791, 'semantic_buffer_size': 1}\n----------------------------------------\n\nBuilding vector store...\n\n======================================================================\nANALYSIS\n======================================================================\nChunk stats:\n  - splitter: Semantic Split (percentile=5, buffer=1) - aggressive\n  - num_chunks: 1\n  - min_tokens: 414\n  - p50_tokens: 414\n  - p90_tokens: 414\n  - max_tokens: 414\n  - avg_tokens: 414.0\n  - exact_duplicate_chunks: 0\n\nRetrieval stats (FAISS similarity_search_with_score):\n  - k: 1\n  - best_score: 0.18783670663833618\n  - worst_score: 0.18783670663833618\n  - avg_score: 0.18783670663833618\n  - keyword_hits: 1\n  - keywords: ['retrieval', 'chunk', 'chunking', 'semantic', 'embedding', 'vector', 'index', 'rag']\n======================================================================\n\nQuestion:\nWhat are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- LLM Answer ---\nAccording to the provided context, the main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector indexing\n5. Retrieval\n6. Response synthesis\n\nText chunking is a crucial component, and there are different chunking strategies, including:\n\n1. **Sentence window chunking**: This approach involves creating chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context. This aims to provide rich local context to the LLM during retrieval, improving the coherence and factual consistency of generated answers.\n2. **Semantic chunking**: This method attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\nThe key differences between sentence window chunking and semantic chunking are:\n\n* **Context provision**: Sentence window chunking provides local context by including surrounding sentences, while semantic chunking focuses on identifying natural breakpoints in the text based on semantic content.\n* **Chunking approach**: Sentence window chunking uses a fixed window size, whereas semantic chunking uses embedding models to compute semantic similarity and identify breakpoints.\n* **Purpose**: Both methods aim to improve retrieval quality and downstream generation performance, but sentence window chunking is designed to provide rich local context, while semantic chunking is focused on identifying meaningful breakpoints in the text.\n\n[Semantic Split (percentile=5, buffer=1) - aggressive] Retrieved source documents (with scores):\n\n--- Source Document 1 --- score=0.1878\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\n[metadata] {'sentence_range': (0, 19), 'semantic_percentile': 5, 'semantic_threshold_val': 0.4696271479129791, 'semantic_buffer_size': 1}\n------------------------------------------------------------\n\nFinished RAG with Semantic Split (percentile=5, buffer=1) - aggressive\n======================================================================\n\n"
     ]
    }
   ],
   "source": [
    "# Aggressive: break at lowest 95% similarities (5th percentile threshold)\n",
    "semantic_splitter_aggressive = SemanticTextSplitter(\n",
    "    embedder=embeddings,\n",
    "    sentence_splitter=sentence_tokenize_en,\n",
    "    breakpoint_percentile_threshold=5,\n",
    "    buffer_size=1,\n",
    "    min_sentences_per_chunk=1,\n",
    ")\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=semantic_splitter_aggressive,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"Semantic Split (percentile=5, buffer=1) - aggressive\",\n",
    "    top_k=5,\n",
    "    print_raw_chunks=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8ea7ede-a33d-40e4-9ad1-0376339940c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### HybridTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d956d9ff-2dd6-4e1e-a67b-de8e86731701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class HybridTextSplitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        primary_splitter,                 # e.g., SemanticTextSplitter\n",
    "        secondary_splitter,               # e.g., SlidingWindowTextSplitter\n",
    "        max_chunk_tokens: int = 300,\n",
    "        debug_print: bool = True,\n",
    "    ):\n",
    "        self.primary_splitter = primary_splitter\n",
    "        self.secondary_splitter = secondary_splitter\n",
    "        self.max_chunk_tokens = max_chunk_tokens\n",
    "        self.debug_print = debug_print\n",
    "\n",
    "    def _token_len(self, text: str) -> int:\n",
    "        # Uses the same approximate token proxy as the previous code\n",
    "        return max(1, len(text.split()))\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        if self.debug_print:\n",
    "            print(\"--- Starting HYBRID splitting ---\")\n",
    "            print(\"Step 1: semantic split...\")\n",
    "\n",
    "        primary_chunks = self.primary_splitter.split_documents(documents)\n",
    "\n",
    "        if self.debug_print:\n",
    "            print(f\"\\n{'='*25} Step 1 (Semantic) Output {'='*25}\")\n",
    "            print(f\"Produced {len(primary_chunks)} semantic chunks.\")\n",
    "            for i, c in enumerate(primary_chunks, 1):\n",
    "                print(f\"\\n[Semantic chunk {i}] tokens≈{self._token_len(c.page_content)}\")\n",
    "                print(\"-\" * 60)\n",
    "                print(c.page_content.strip())\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "            print(f\"\\n{'='*25} Step 2 (Check + Secondary Split) {'='*25}\")\n",
    "\n",
    "        final_chunks: List[Document] = []\n",
    "        for i, chunk in enumerate(primary_chunks, 1):\n",
    "            tlen = self._token_len(chunk.page_content)\n",
    "\n",
    "            if self.debug_print:\n",
    "                print(f\"\\n>>> Checking semantic chunk {i} tokens≈{tlen} ...\")\n",
    "\n",
    "            if tlen <= self.max_chunk_tokens:\n",
    "                if self.debug_print:\n",
    "                    print(f\"  └─ OK (<= {self.max_chunk_tokens}). Keep as-is.\")\n",
    "                final_chunks.append(chunk)\n",
    "            else:\n",
    "                if self.debug_print:\n",
    "                    print(f\"  └─ Too large (> {self.max_chunk_tokens}). Apply sliding window split.\")\n",
    "                    print(\"     [Original oversized chunk]\")\n",
    "                    print(\"     \" + \"-\" * 50)\n",
    "                    print(\"     \" + chunk.page_content.strip().replace(\"\\n\", \"\\n     \"))\n",
    "                    print(\"     \" + \"-\" * 50)\n",
    "\n",
    "                # Secondary split acts on the oversized chunk as a single Document\n",
    "                sub_docs = self.secondary_splitter.split_documents(\n",
    "                    [Document(page_content=chunk.page_content, metadata=dict(chunk.metadata or {}))]\n",
    "                )\n",
    "\n",
    "                if self.debug_print:\n",
    "                    print(f\"\\n     [Secondary split produced {len(sub_docs)} sub-chunks]\")\n",
    "                    for j, sd in enumerate(sub_docs, 1):\n",
    "                        print(f\"\\n     [Sub-chunk {i}.{j}] tokens≈{self._token_len(sd.page_content)}\")\n",
    "                        print(\"     \" + \"-\" * 40)\n",
    "                        print(\"     \" + sd.page_content.strip().replace(\"\\n\", \"\\n     \"))\n",
    "                        print(\"     \" + \"-\" * 40)\n",
    "\n",
    "                final_chunks.extend(sub_docs)\n",
    "\n",
    "        if self.debug_print:\n",
    "            print(\"\\n--- HYBRID splitting complete ---\")\n",
    "            print(f\"Final chunk count: {len(final_chunks)}\")\n",
    "\n",
    "        return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4890554e-2bd9-4b00-93ab-b1e0f89318ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nRunning RAG with splitter: Hybrid Split (Semantic -> Sliding Window fallback)\n======================================================================\n\n--- Starting HYBRID splitting ---\nStep 1: semantic split...\n\n========================= Step 1 (Semantic) Output =========================\nProduced 1 semantic chunks.\n\n[Semantic chunk 1] tokens≈414\n------------------------------------------------------------\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\n========================= Step 2 (Check + Secondary Split) =========================\n\n>>> Checking semantic chunk 1 tokens≈414 ...\n  └─ Too large (> 300). Apply sliding window split.\n     [Original oversized chunk]\n     --------------------------------------------------\n     Retrieval-Augmented Generation (RAG) is a common architecture for building\n     LLM-powered applications that combine external knowledge retrieval with\n     text generation. Instead of relying solely on a model’s internal parameters, RAG systems\n     retrieve relevant information from external data sources and use it as\n     context during answer generation. This approach is widely used for applications such as question answering,\n     knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\n     document ingestion, text chunking, embedding generation, vector indexing,\n     retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n     are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\n     to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\n     and stored in a vector index. During inference, a user query is embedded and compared against the indexed\n     vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\n     allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n     \n     In addition, Python, as a general-purpose programming language, is widely used\n     in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\n     providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\n     for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n     enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n     \n     Sentence window chunking is an advanced chunking strategy in which each chunk\n     contains a target sentence along with a configurable number of surrounding\n     “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\n     thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\n     content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n     or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\n     downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\n     of the data and the expected query types.\n     --------------------------------------------------\n\n     [Secondary split produced 1 sub-chunks]\n\n     [Sub-chunk 1.1] tokens≈414\n     ----------------------------------------\n     Retrieval-Augmented Generation (RAG) is a common architecture for building\n     LLM-powered applications that combine external knowledge retrieval with\n     text generation. Instead of relying solely on a model’s internal parameters, RAG systems\n     retrieve relevant information from external data sources and use it as\n     context during answer generation. This approach is widely used for applications such as question answering,\n     knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\n     document ingestion, text chunking, embedding generation, vector indexing,\n     retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n     are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\n     to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\n     and stored in a vector index. During inference, a user query is embedded and compared against the indexed\n     vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\n     allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n     \n     In addition, Python, as a general-purpose programming language, is widely used\n     in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\n     providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\n     for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n     enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n     \n     Sentence window chunking is an advanced chunking strategy in which each chunk\n     contains a target sentence along with a configurable number of surrounding\n     “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\n     thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\n     content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n     or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\n     downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\n     of the data and the expected query types.\n     ----------------------------------------\n\n--- HYBRID splitting complete ---\nFinal chunk count: 1\n[Hybrid Split (Semantic -> Sliding Window fallback)] Generated chunks: total=1\n\n--- Chunk 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\nBuilding vector store...\n\n======================================================================\nANALYSIS\n======================================================================\nChunk stats:\n  - splitter: Hybrid Split (Semantic -> Sliding Window fallback)\n  - num_chunks: 1\n  - min_tokens: 414\n  - p50_tokens: 414\n  - p90_tokens: 414\n  - max_tokens: 414\n  - avg_tokens: 414.0\n  - exact_duplicate_chunks: 0\n\nRetrieval stats (FAISS similarity_search_with_score):\n  - k: 1\n  - best_score: 0.18783670663833618\n  - worst_score: 0.18783670663833618\n  - avg_score: 0.18783670663833618\n  - keyword_hits: 1\n  - keywords: ['retrieval', 'chunk', 'chunking', 'semantic', 'embedding', 'vector', 'index', 'rag']\n======================================================================\n\nQuestion:\nWhat are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- LLM Answer ---\nAccording to the provided context, the main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector indexing\n5. Retrieval\n6. Response synthesis\n\nRegarding chunking strategies, the context mentions two advanced methods:\n\n1. **Sentence window chunking**: This approach involves creating chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context. The goal is to provide rich local context to the Large Language Model (LLM) during retrieval, which can improve the coherence and factual consistency of generated answers.\n2. **Semantic chunking**: This method attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It uses embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\nThe key differences between these two chunking strategies are:\n\n* Sentence window chunking focuses on providing local context to the LLM, whereas semantic chunking focuses on identifying semantic breaks in the text.\n* Sentence window chunking is more concerned with the proximity of sentences, whereas semantic chunking is concerned with the meaning and content of the text.\n\nThe choice of chunking strategy depends on the characteristics of the data and the expected query types.\n\n[Hybrid Split (Semantic -> Sliding Window fallback)] Retrieved source documents (with scores):\n\n--- Source Document 1 --- score=0.1878\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation. Instead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation. This approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\nFinished RAG with Hybrid Split (Semantic -> Sliding Window fallback)\n======================================================================\n\n"
     ]
    }
   ],
   "source": [
    "semantic_primary = SemanticTextSplitter(\n",
    "    embedder=embeddings,\n",
    "    sentence_splitter=sentence_tokenize_en,   # or your chinese tokenizer if your doc is Chinese\n",
    "    breakpoint_percentile_threshold=95,       # conservative\n",
    "    buffer_size=1,\n",
    "    min_sentences_per_chunk=1,\n",
    ")\n",
    "\n",
    "sliding_secondary = SlidingWindowTextSplitter(\n",
    "    window_size=256,\n",
    "    stride=2)\n",
    "\n",
    "hybrid_splitter = HybridTextSplitter(\n",
    "    primary_splitter=semantic_primary,\n",
    "    secondary_splitter=sliding_secondary,\n",
    "    max_chunk_tokens=300,      # align with your example's max_chunk_size idea\n",
    "    debug_print=True,\n",
    ")\n",
    "\n",
    "# Now run the exact same RAG pipeline you already have:\n",
    "run_rag_pipeline(\n",
    "    splitter=hybrid_splitter,\n",
    "    documents=documents,   # reuse your RAG document list\n",
    "    question=question,\n",
    "    splitter_name=\"Hybrid Split (Semantic -> Sliding Window fallback)\",\n",
    "    top_k=5,\n",
    "    print_raw_chunks=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5023ba3b-0b5b-4e9e-a4cc-223b418ec58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Tokens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}