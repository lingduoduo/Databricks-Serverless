{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8901e340-f23c-4263-8dcc-8e887f407be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (0.3.27)\nCollecting langchain\n  Using cached langchain-1.2.0-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: langchain-community in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (0.3.31)\nCollecting langchain-community\n  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: langchain-databricks in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (0.1.2)\nRequirement already satisfied: faiss-cpu in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (1.13.2)\nRequirement already satisfied: tiktoken in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (0.12.0)\nCollecting langchain-core<2.0.0,>=1.2.1 (from langchain)\n  Using cached langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\nCollecting langgraph<1.1.0,>=1.0.2 (from langchain)\n  Using cached langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.10.6)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n  Using cached langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-community) (2.0.45)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-community) (3.13.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-community) (2.12.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-community) (0.5.2)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-community) (0.4.3)\nRequirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (2.1.3)\nRequirement already satisfied: databricks-vectorsearch<0.41,>=0.40 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-databricks) (0.40)\nINFO: pip is looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-databricks\n  Using cached langchain_databricks-0.1.2-py3-none-any.whl.metadata (3.3 kB)\n  Using cached langchain_databricks-0.1.1-py3-none-any.whl.metadata (5.9 kB)\n  Using cached langchain_databricks-0.1.0-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-community\n  Using cached langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\nINFO: pip is still looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Using cached langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n  Using cached langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n  Using cached langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Using cached langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n  Using cached langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n  Using cached langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n  Using cached langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Using cached langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n  Using cached langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n  Using cached langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n  Using cached langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n  Using cached langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Using cached langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\nCollecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n  Using cached SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\nCollecting langchain-community\n  Using cached langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.6-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n  Using cached langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n  Using cached langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n  Using cached langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n  Using cached langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\nCollecting langchain\n  Using cached langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community\n  Using cached langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n  Using cached langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n  Using cached langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n  Using cached langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n  Using cached langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n  Using cached langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n  Using cached langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n  Using cached langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n  Using cached langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n  Using cached langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n  Using cached langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n  Using cached langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n  Using cached langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n  Using cached langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n  Using cached langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n  Using cached langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n  Using cached langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n  Using cached langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n  Using cached langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n  Using cached langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n  Using cached langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n  Using cached langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n  Using cached langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n  Using cached langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n  Using cached langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n  Using cached langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n  Using cached langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n  Using cached langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n  Using cached langchain_community-0.0.19-py3-none-any.whl.metadata (7.9 kB)\n  Using cached langchain_community-0.0.18-py3-none-any.whl.metadata (7.9 kB)\n  Using cached langchain_community-0.0.17-py3-none-any.whl.metadata (7.9 kB)\n  Using cached langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\n  Using cached langchain_community-0.0.15-py3-none-any.whl.metadata (7.6 kB)\n  Using cached langchain_community-0.0.14-py3-none-any.whl.metadata (7.5 kB)\n  Using cached langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\n  Using cached langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n  Using cached langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n  Using cached langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n  Using cached langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n  Using cached langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\n  Using cached langchain_community-0.0.6-py3-none-any.whl.metadata (7.2 kB)\n  Using cached langchain_community-0.0.5-py3-none-any.whl.metadata (7.1 kB)\n  Using cached langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n  Using cached langchain_community-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n  Using cached langchain_community-0.0.2-py3-none-any.whl.metadata (5.8 kB)\n  Using cached langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\nCollecting langchain\n  Using cached langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (24.1)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.12.2)\nRequirement already satisfied: uuid-utils<1.0,>=0.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n  Using cached langchain-1.1.2-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.1.1-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.0.7-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.0.4-py3-none-any.whl.metadata (4.9 kB)\n  Using cached langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n  Using cached langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n  Using cached langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n  Using cached langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain) (0.3.81)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain) (0.3.11)\nRequirement already satisfied: mlflow>=2.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langchain-databricks) (2.22.4)\nRequirement already satisfied: scipy>=1.11 in /databricks/python3/lib/python3.12/site-packages (from langchain-databricks) (1.15.1)\nRequirement already satisfied: regex>=2022.1.18 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from tiktoken) (2025.11.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\nRequirement already satisfied: mlflow-skinny<3,>=2.11.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.22.4)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.25.8)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.1.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\nRequirement already satisfied: orjson>=3.9.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.1.2)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.1.5)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (1.17.2)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (7.1.0)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (23.0.0)\nRequirement already satisfied: markdown<4,>=3.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.10)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.10.0)\nRequirement already satisfied: pandas!=2.3.0,<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (2.2.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (1.6.1)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.49.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.5.3)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.34.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\nRequirement already satisfied: python-dotenv>=0.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.1.31)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow>=2.16.0->langchain-databricks) (1.3.10)\nRequirement already satisfied: blinker>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from Flask<4->mlflow>=2.16.0->langchain-databricks) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from Flask<4->mlflow>=2.16.0->langchain-databricks) (2.2.0)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow>=2.16.0->langchain-databricks) (3.0.2)\nRequirement already satisfied: werkzeug>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from Flask<4->mlflow>=2.16.0->langchain-databricks) (3.1.4)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from graphene<4->mlflow>=2.16.0->langchain-databricks) (3.2.7)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-492a8fe2-4a95-4d1c-b451-e4baec8d43a0/lib/python3.12/site-packages (from graphene<4->mlflow>=2.16.0->langchain-databricks) (3.2.0)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow>=2.16.0->langchain-databricks) (2.9.0.post0)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.6.2)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (3.2.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (3.5.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.40.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.21.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.53b1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.16.0->langchain-databricks) (1.16.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.9.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.4.8)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain langchain-community langchain-databricks faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b641c7-0eb8-4e2f-8d17-20c47d967d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef54e8-4f3d-42ab-b96f-b60ca42326f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b1f725-60ba-4c28-b19e-21107de2b062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-492a8fe2-4a95-4d1c-b451-e4/.ipykernel/4079/command-8016179771135635-1259243950:8: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n/home/spark-492a8fe2-4a95-4d1c-b451-e4/.ipykernel/4079/command-8016179771135635-1259243950:9: LangChainDeprecationWarning: Use databricks_langchain.DatabricksEmbeddings\n  embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 1) Databricks LLM + Embeddings\n",
    "# -----------------------\n",
    "# Make sure your Databricks auth is configured (e.g., DATABRICKS_HOST + DATABRICKS_TOKEN)\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"  # <-- change to your embedding endpoint name\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b03608-cbeb-476e-8ee0-28e4db57563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Sentence split + sentence-window splitter (LlamaIndex-like)\n",
    "# -----------------------\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[。！？!?])\\s+|\\n+\")\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def sentence_window_splitter(\n",
    "    documents: List[Document],\n",
    "    window_size: int = 2,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Mimics LlamaIndex SentenceWindowNodeParser:\n",
    "    - Each chunk is one core sentence\n",
    "    - metadata includes:\n",
    "        - original_text: core sentence\n",
    "        - window: context window (core +/- window_size sentences)\n",
    "    \"\"\"\n",
    "    out: List[Document] = []\n",
    "    for doc in documents:\n",
    "        sents = split_sentences(doc.page_content)\n",
    "        for i, core in enumerate(sents):\n",
    "            lo = max(0, i - window_size)\n",
    "            hi = min(len(sents), i + window_size + 1)\n",
    "            window_text = \" \".join(sents[lo:hi])\n",
    "\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=core,\n",
    "                    metadata={\n",
    "                        **(doc.metadata or {}),\n",
    "                        \"original_text\": core,\n",
    "                        \"window\": window_text,\n",
    "                        \"sent_index\": i,\n",
    "                        \"window_size\": window_size,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a46cdd-8ca8-421c-acc2-b26138b51c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Prompt + QA\n",
    "# -----------------------\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful technical assistant. Answer using ONLY the provided context. \"\n",
    "     \"If the context is insufficient, say what is missing.\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nAnswer in English:\")\n",
    "])\n",
    "\n",
    "def answer_with_retrieval(\n",
    "    docs: List[Document],\n",
    "    question: str,\n",
    "    top_k: int = 5,\n",
    "    use_window_metadata: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build FAISS index, retrieve top_k chunks, print retrieved chunks, then ask the Databricks LLM.\n",
    "    If use_window_metadata=True, feed metadata['window'] as context (Sentence Window style).\n",
    "    \"\"\"\n",
    "    vs = FAISS.from_documents(docs, embeddings)\n",
    "    retrieved = vs.similarity_search_with_score(question, k=top_k)\n",
    "\n",
    "    print(\"\\n--- Top retrieved chunks ---\")\n",
    "    context_blocks = []\n",
    "    for rank, (d, score) in enumerate(retrieved, 1):\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            window = d.metadata.get(\"window\", d.page_content)\n",
    "            core = d.metadata.get(\"original_text\", d.page_content)\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(f\"Core: {core}\")\n",
    "            print(f\"Window:\\n{window}\")\n",
    "            context_blocks.append(window)\n",
    "        else:\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(d.page_content)\n",
    "            context_blocks.append(d.page_content)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    msg = QA_PROMPT.format_messages(question=question, context=context)\n",
    "    resp = llm.invoke(msg)\n",
    "\n",
    "    print(\"\\n--- LLM Answer ---\")\n",
    "    print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65d9762-ddc9-48d9-9d54-09f8a1931f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4) Runner (prints raw chunks + retrieval behavior)\n",
    "# -----------------------\n",
    "def evaluate_splitter(\n",
    "    splitter_name: str,\n",
    "    split_fn: Callable[[], List[Document]],\n",
    "    question: str,\n",
    "    use_window_metadata: bool = False,\n",
    "    top_k: int = 5,\n",
    "    max_print_chunks: int = 50,\n",
    ") -> None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing splitter: {splitter_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    chunks = split_fn()\n",
    "\n",
    "    print(f\"\\n[Raw chunks generated] total={len(chunks)}\")\n",
    "    for i, d in enumerate(chunks[:max_print_chunks], 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            print(f\"Core: {d.metadata.get('original_text')}\")\n",
    "            print(f\"Window: {d.metadata.get('window')}\")\n",
    "        else:\n",
    "            print(d.page_content)\n",
    "\n",
    "    if len(chunks) > max_print_chunks:\n",
    "        print(f\"\\n... (only printed first {max_print_chunks} chunks)\")\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer_with_retrieval(\n",
    "        docs=chunks,\n",
    "        question=question,\n",
    "        top_k=top_k,\n",
    "        use_window_metadata=use_window_metadata,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{splitter_name} done.\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1906460-b1a6-4bb1-b7a8-b294fd615fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nTesting splitter: Token Split (chunk_size=30, overlap=0)\n============================================================\n\n[Raw chunks generated] total=20\n\n--- Chunk 1 ---\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n--- Chunk 2 ---\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and\n\n--- Chunk 3 ---\n use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific\n\n--- Chunk 4 ---\n search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector index\n\n--- Chunk 5 ---\ning,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 6 ---\n\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise\n\n--- Chunk 7 ---\n and efficient.\n\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference,\n\n--- Chunk 8 ---\n a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language\n\n--- Chunk 9 ---\n Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is\n\n--- Chunk 10 ---\n less directly related to the RAG topic ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the\n\n--- Chunk 11 ---\n AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful\n\n--- Chunk 12 ---\n capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression\n\n--- Chunk 13 ---\n, and clustering.\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of\n\n--- Chunk 14 ---\n complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking\n\n--- Chunk 15 ---\n strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n\n\n--- Chunk 16 ---\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\n\n\n--- Chunk 17 ---\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries\n\n--- Chunk 18 ---\n.\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth\n\n--- Chunk 19 ---\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n--- Chunk 20 ---\n characteristics\nof the data and the expected query types.\n\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3183\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n[2] score=0.3336\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n[3] score=0.3830\n search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector index\n\n[4] score=0.4250\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and\n\n[5] score=0.4327\n complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector index\n5. Text generation\n\nAs for the difference between sentence window chunking and semantic chunking, the context only mentions sentence window chunking as an advanced chunking strategy, but does not provide information on semantic chunking. Therefore, I am unable to provide a comparison between the two.\n\nToken Split (chunk_size=30, overlap=0) done.\n============================================================\n\n\n============================================================\nTesting splitter: Token Split (chunk_size=30, overlap=10)\n============================================================\n\n[Raw chunks generated] total=29\n\n--- Chunk 1 ---\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n--- Chunk 2 ---\n-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\n\n--- Chunk 3 ---\n model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\n\n\n--- Chunk 4 ---\n use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific\n\n--- Chunk 5 ---\n answering,\nknowledge assistants, and domain-specific search.\n\nA typical RAG pipeline consists of several core components, including\ndocument ingestion,\n\n--- Chunk 6 ---\n of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis\n\n--- Chunk 7 ---\ning,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 8 ---\ns, databases, APIs, or web pages—\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred\n\n--- Chunk 9 ---\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n\nEach chunk is converted into\n\n--- Chunk 10 ---\n and efficient.\n\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference,\n\n--- Chunk 11 ---\n stored in a vector index.\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\n\n--- Chunk 12 ---\n\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nall\n\n--- Chunk 13 ---\n Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is\n\n--- Chunk 14 ---\n external knowledge.\n\n--- The following content is less directly related to the RAG topic ---\n\nIn addition, Python, as a general-\n\n--- Chunk 15 ---\n\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem.\n\n--- Chunk 16 ---\n AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful\n\n--- Chunk 17 ---\n foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine\n\n--- Chunk 18 ---\ncikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering.\nTogether, these tools\n\n--- Chunk 19 ---\n, and clustering.\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of\n\n--- Chunk 20 ---\n practitioners,\nenabling efficient development and deployment of complex AI models.\n\n--- The following is another related but conceptually independent section ---\n\n\n\n--- Chunk 21 ---\n another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target\n\n--- Chunk 22 ---\n strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\n\n\n--- Chunk 23 ---\n“window” sentences as context.\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the\n\n--- Chunk 24 ---\n LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand,\n\n--- Chunk 25 ---\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries\n\n--- Chunk 26 ---\n than relying solely on fixed character counts or sentence boundaries.\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural\n\n--- Chunk 27 ---\n semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth advanced methods can significantly improve retrieval quality and\ndown\n\n--- Chunk 28 ---\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n--- Chunk 29 ---\nosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3183\n advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the\n\n[2] score=0.3336\n\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\n\n\n[3] score=0.3912\n LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand,\n\n[4] score=0.4275\n-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\n\n[5] score=0.4342\n another related but conceptually independent section ---\n\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. External knowledge retrieval\n2. Language Model (LLM)\n3. Chunking strategy (which can be either sentence window chunking or semantic chunking)\n\nSentence window chunking and semantic chunking differ in that:\n\n* Sentence window chunking involves dividing the input text into chunks, where each chunk contains a target sentence and a surrounding context (window) of sentences.\n* Semantic chunking, on the other hand, involves dividing the input text into chunks based on semantic meaning, rather than syntactic structure.\n\nToken Split (chunk_size=30, overlap=10) done.\n============================================================\n\n\n============================================================\nTesting splitter: Sentence Window Split (window_size=2)\n============================================================\n\n[Raw chunks generated] total=44\n\n--- Chunk 1 ---\nCore: Retrieval-Augmented Generation (RAG) is a common architecture for building\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n\n--- Chunk 2 ---\nCore: LLM-powered applications that combine external knowledge retrieval with\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems\n\n--- Chunk 3 ---\nCore: text generation.\nWindow: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as\n\n--- Chunk 4 ---\nCore: Instead of relying solely on a model’s internal parameters, RAG systems\nWindow: LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation.\n\n--- Chunk 5 ---\nCore: retrieve relevant information from external data sources and use it as\nWindow: text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering,\n\n--- Chunk 6 ---\nCore: context during answer generation.\nWindow: Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search.\n\n--- Chunk 7 ---\nCore: This approach is widely used for applications such as question answering,\nWindow: retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\n\n--- Chunk 8 ---\nCore: knowledge assistants, and domain-specific search.\nWindow: context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing,\n\n--- Chunk 9 ---\nCore: A typical RAG pipeline consists of several core components, including\nWindow: This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis.\n\n--- Chunk 10 ---\nCore: document ingestion, text chunking, embedding generation, vector indexing,\nWindow: knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n\n--- Chunk 11 ---\nCore: retrieval, and response synthesis.\nWindow: A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed.\n\n--- Chunk 12 ---\nCore: Documents from various sources—such as PDFs, databases, APIs, or web pages—\nWindow: document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\n\n--- Chunk 13 ---\nCore: are first ingested and preprocessed.\nWindow: retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient.\n\n--- Chunk 14 ---\nCore: They are then split into smaller units, often referred to as chunks,\nWindow: Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\n\n--- Chunk 15 ---\nCore: to make retrieval more precise and efficient.\nWindow: are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index.\n\n--- Chunk 16 ---\nCore: Each chunk is converted into a vector representation using an embedding model\nWindow: They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed\n\n--- Chunk 17 ---\nCore: and stored in a vector index.\nWindow: to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks.\n\n--- Chunk 18 ---\nCore: During inference, a user query is embedded and compared against the indexed\nWindow: Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\n\n--- Chunk 19 ---\nCore: vectors to retrieve the most relevant chunks.\nWindow: and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\n--- Chunk 20 ---\nCore: These retrieved chunks are provided to a Large Language Model (LLM) as context,\nWindow: During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n\n--- Chunk 21 ---\nCore: allowing the model to generate answers that are grounded in external knowledge.\nWindow: vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used\n\n--- Chunk 22 ---\nCore: --- The following content is less directly related to the RAG topic ---\nWindow: These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem.\n\n--- Chunk 23 ---\nCore: In addition, Python, as a general-purpose programming language, is widely used\nWindow: allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\n\n--- Chunk 24 ---\nCore: in the AI field due to its simplicity and rich ecosystem.\nWindow: --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data.\n\n--- Chunk 25 ---\nCore: For example, NumPy and Pandas are foundational tools for data processing,\nWindow: In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\n\n--- Chunk 26 ---\nCore: providing powerful capabilities for numerical computation and structured data.\nWindow: in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering.\n\n--- Chunk 27 ---\nCore: Scikit-learn offers a comprehensive suite of machine learning algorithms\nWindow: For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n\n--- Chunk 28 ---\nCore: for tasks such as classification, regression, and clustering.\nWindow: providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models.\n\n--- Chunk 29 ---\nCore: Together, these tools form a powerful toolbox for data scientists and AI practitioners,\nWindow: Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n\n--- Chunk 30 ---\nCore: enabling efficient development and deployment of complex AI models.\nWindow: for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk\n\n--- Chunk 31 ---\nCore: --- The following is another related but conceptually independent section ---\nWindow: Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding\n\n--- Chunk 32 ---\nCore: Sentence window chunking is an advanced chunking strategy in which each chunk\nWindow: enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n\n--- Chunk 33 ---\nCore: contains a target sentence along with a configurable number of surrounding\nWindow: --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\n\n--- Chunk 34 ---\nCore: “window” sentences as context.\nWindow: Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers.\n\n--- Chunk 35 ---\nCore: This approach aims to provide rich local context to the LLM during retrieval,\nWindow: contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\n\n--- Chunk 36 ---\nCore: thereby improving the coherence and factual consistency of generated answers.\nWindow: “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries.\n\n--- Chunk 37 ---\nCore: Semantic chunking, on the other hand, attempts to split text based on semantic\nWindow: This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n\n--- Chunk 38 ---\nCore: content rather than relying solely on fixed character counts or sentence boundaries.\nWindow: thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n\n--- Chunk 39 ---\nCore: It leverages embedding models to compute semantic similarity between sentences\nWindow: Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\n\n--- Chunk 40 ---\nCore: or phrases and identify natural breakpoints where topics or meanings shift.\nWindow: content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications.\n\n--- Chunk 41 ---\nCore: Both advanced methods can significantly improve retrieval quality and\nWindow: It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\n\n--- Chunk 42 ---\nCore: downstream generation performance in RAG applications.\nWindow: or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\n--- Chunk 43 ---\nCore: Choosing the right chunking strategy typically depends on the characteristics\nWindow: Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\n--- Chunk 44 ---\nCore: of the data and the expected query types.\nWindow: downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n\nQuestion: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\n--- Top retrieved chunks ---\n\n[1] score=0.3029\nCore: Retrieval-Augmented Generation (RAG) is a common architecture for building\nWindow:\nRetrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n\n[2] score=0.3936\nCore: Sentence window chunking is an advanced chunking strategy in which each chunk\nWindow:\nenabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n\n[3] score=0.4610\nCore: Semantic chunking, on the other hand, attempts to split text based on semantic\nWindow:\nThis approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n\n[4] score=0.4851\nCore: document ingestion, text chunking, embedding generation, vector indexing,\nWindow:\nknowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n\n[5] score=0.5718\nCore: vectors to retrieve the most relevant chunks.\nWindow:\nand stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\n--- LLM Answer ---\nThe main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion: This involves collecting and storing documents from various sources such as PDFs, databases, APIs, or web pages.\n2. Text chunking: This is the process of splitting the ingested documents into smaller chunks, which can be done using different strategies such as sentence window chunking or semantic chunking.\n3. Embedding generation: This involves converting the text chunks into numerical vectors that can be used for indexing and retrieval.\n4. Vector indexing: This is the process of storing the generated vectors in an index that allows for efficient retrieval of relevant chunks.\n5. Retrieval: This involves comparing a user query with the indexed vectors to retrieve the most relevant chunks.\n6. Response synthesis: This is the final step where the retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n\nSentence window chunking and semantic chunking differ in their approach to splitting text into chunks:\n\n* Sentence window chunking involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context.\n* Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries, using embedding models to compute semantic similarity between sentences.\n\nSentence Window Split (window_size=2) done.\n============================================================\n\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 5) Example document + question (English version)\n",
    "# -----------------------\n",
    "documents = [\n",
    "    Document(page_content=\"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
    "LLM-powered applications that combine external knowledge retrieval with\n",
    "text generation.\n",
    "Instead of relying solely on a model’s internal parameters, RAG systems\n",
    "retrieve relevant information from external data sources and use it as\n",
    "context during answer generation.\n",
    "This approach is widely used for applications such as question answering,\n",
    "knowledge assistants, and domain-specific search.\n",
    "\n",
    "A typical RAG pipeline consists of several core components, including\n",
    "document ingestion, text chunking, embedding generation, vector indexing,\n",
    "retrieval, and response synthesis.\n",
    "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
    "are first ingested and preprocessed.\n",
    "They are then split into smaller units, often referred to as chunks,\n",
    "to make retrieval more precise and efficient.\n",
    "\n",
    "Each chunk is converted into a vector representation using an embedding model\n",
    "and stored in a vector index.\n",
    "During inference, a user query is embedded and compared against the indexed\n",
    "vectors to retrieve the most relevant chunks.\n",
    "These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
    "allowing the model to generate answers that are grounded in external knowledge.\n",
    "\n",
    "--- The following content is less directly related to the RAG topic ---\n",
    "\n",
    "In addition, Python, as a general-purpose programming language, is widely used\n",
    "in the AI field due to its simplicity and rich ecosystem.\n",
    "For example, NumPy and Pandas are foundational tools for data processing,\n",
    "providing powerful capabilities for numerical computation and structured data.\n",
    "Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
    "for tasks such as classification, regression, and clustering.\n",
    "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
    "enabling efficient development and deployment of complex AI models.\n",
    "\n",
    "--- The following is another related but conceptually independent section ---\n",
    "\n",
    "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
    "contains a target sentence along with a configurable number of surrounding\n",
    "“window” sentences as context.\n",
    "This approach aims to provide rich local context to the LLM during retrieval,\n",
    "thereby improving the coherence and factual consistency of generated answers.\n",
    "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
    "content rather than relying solely on fixed character counts or sentence boundaries.\n",
    "It leverages embedding models to compute semantic similarity between sentences\n",
    "or phrases and identify natural breakpoints where topics or meanings shift.\n",
    "Both advanced methods can significantly improve retrieval quality and\n",
    "downstream generation performance in RAG applications.\n",
    "Choosing the right chunking strategy typically depends on the characteristics\n",
    "of the data and the expected query types.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "question = (\n",
    "    \"What are the main components of a Retrieval-Augmented Generation (RAG) system, \"\n",
    "    \"and how do sentence window chunking and semantic chunking differ?\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Run splitters\n",
    "# -----------------------\n",
    "# Token-based split (chunk_size=30, overlap=0)\n",
    "splitter_a = TokenTextSplitter(chunk_size=30, chunk_overlap=0)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=0)\",\n",
    "    split_fn=lambda: splitter_a.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Token-based split (chunk_size=30, overlap=10)\n",
    "splitter_b = TokenTextSplitter(chunk_size=30, chunk_overlap=10)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=10)\",\n",
    "    split_fn=lambda: splitter_b.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Sentence-window split (window_size=2)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Sentence Window Split (window_size=2)\",\n",
    "    split_fn=lambda: sentence_window_splitter(documents, window_size=2),\n",
    "    question=question,\n",
    "    use_window_metadata=True,   # feed metadata['window'] into LLM context\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981572fb-afc9-4af8-891d-4fcc08ff1c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Token Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9013498-c13b-4207-b3a0-6aada21b7290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nRunning RAG pipeline with splitter: RecursiveCharacterTextSplitter\n======================================================================\n\n[RecursiveCharacterTextSplitter] Generated document chunks:\n\n--- Chunk 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search.\n----------------------------------------\n\n--- Chunk 2 ---\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n----------------------------------------\n\n--- Chunk 3 ---\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is less directly related to the RAG topic ---\n----------------------------------------\n\n--- Chunk 4 ---\nIn addition, Python, as a general-purpose programming language, is widely used\nin the AI field due to its simplicity and rich ecosystem.\nFor example, NumPy and Pandas are foundational tools for data processing,\nproviding powerful capabilities for numerical computation and structured data.\nScikit-learn offers a comprehensive suite of machine learning algorithms\nfor tasks such as classification, regression, and clustering.\n----------------------------------------\n\n--- Chunk 5 ---\nTogether, these tools form a powerful toolbox for data scientists and AI practitioners,\nenabling efficient development and deployment of complex AI models.\n----------------------------------------\n\n--- Chunk 6 ---\n--- The following is another related but conceptually independent section ---\n----------------------------------------\n\n--- Chunk 7 ---\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries.\n----------------------------------------\n\n--- Chunk 8 ---\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n----------------------------------------\n\nBuilding vector store...\n\nQuestion:\nWhat are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n\nModel Answer:\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-492a8fe2-4a95-4d1c-b451-e4/.ipykernel/4079/command-8016179771135642-3916514322:57: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  result = qa_chain(question)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the main components of a Retrieval-Augmented Generation (RAG) system are:\n\n1. Document ingestion\n2. Text chunking\n3. Embedding generation\n4. Vector indexing\n5. Retrieval\n6. Response synthesis\n\nRegarding sentence window chunking and semantic chunking, the context states that:\n\n* Sentence window chunking involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding \"window\" sentences as context. This approach aims to provide rich local context to the LLM during retrieval, improving the coherence and factual consistency of generated answers.\n* Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. Each chunk is converted into a vector representation using an embedding model and stored in a vector index.\n\nIn summary, the main difference between sentence window chunking and semantic chunking is the approach used to split the text into chunks. Sentence window chunking focuses on providing local context around a target sentence, while semantic chunking focuses on splitting text based on its semantic meaning.\n\n[RecursiveCharacterTextSplitter] Retrieved source documents:\n\n--- Source Document 1 ---\nRetrieval-Augmented Generation (RAG) is a common architecture for building\nLLM-powered applications that combine external knowledge retrieval with\ntext generation.\nInstead of relying solely on a model’s internal parameters, RAG systems\nretrieve relevant information from external data sources and use it as\ncontext during answer generation.\nThis approach is widely used for applications such as question answering,\nknowledge assistants, and domain-specific search.\n------------------------------------------------------------\n\n--- Source Document 2 ---\nIt leverages embedding models to compute semantic similarity between sentences\nor phrases and identify natural breakpoints where topics or meanings shift.\nBoth advanced methods can significantly improve retrieval quality and\ndownstream generation performance in RAG applications.\nChoosing the right chunking strategy typically depends on the characteristics\nof the data and the expected query types.\n------------------------------------------------------------\n\n--- Source Document 3 ---\nSentence window chunking is an advanced chunking strategy in which each chunk\ncontains a target sentence along with a configurable number of surrounding\n“window” sentences as context.\nThis approach aims to provide rich local context to the LLM during retrieval,\nthereby improving the coherence and factual consistency of generated answers.\nSemantic chunking, on the other hand, attempts to split text based on semantic\ncontent rather than relying solely on fixed character counts or sentence boundaries.\n------------------------------------------------------------\n\n--- Source Document 4 ---\nEach chunk is converted into a vector representation using an embedding model\nand stored in a vector index.\nDuring inference, a user query is embedded and compared against the indexed\nvectors to retrieve the most relevant chunks.\nThese retrieved chunks are provided to a Large Language Model (LLM) as context,\nallowing the model to generate answers that are grounded in external knowledge.\n\n--- The following content is less directly related to the RAG topic ---\n------------------------------------------------------------\n\n--- Source Document 5 ---\nA typical RAG pipeline consists of several core components, including\ndocument ingestion, text chunking, embedding generation, vector indexing,\nretrieval, and response synthesis.\nDocuments from various sources—such as PDFs, databases, APIs, or web pages—\nare first ingested and preprocessed.\nThey are then split into smaller units, often referred to as chunks,\nto make retrieval more precise and efficient.\n------------------------------------------------------------\n\nFinished RAG pipeline with RecursiveCharacterTextSplitter\n======================================================================\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "\n",
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG pipeline with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 1: Split documents\n",
    "    # -----------------------\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[{splitter_name}] Generated document chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(chunk.page_content.strip())\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 2: Build vector store\n",
    "    # -----------------------\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 3: Build RAG chain\n",
    "    # -----------------------\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 4: Ask question\n",
    "    # -----------------------\n",
    "    print(f\"\\nQuestion:\\n{question}\\n\")\n",
    "    print(\"Model Answer:\\n\")\n",
    "\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 5: Show retrieved context\n",
    "    # -----------------------\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        print(f\"\\n--- Source Document {i} ---\")\n",
    "        print(doc.page_content.strip())\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG pipeline with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=sentence_splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"RecursiveCharacterTextSplitter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d162956-a99a-4eb3-abcb-8ebe80c177f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Tokens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}