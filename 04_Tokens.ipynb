{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8901e340-f23c-4263-8dcc-8e887f407be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-databricks\n",
      "  Downloading langchain_databricks-0.1.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (7.6 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (6.7 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.2.1 (from langchain)\n",
      "  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.10.6)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (9.5 kB)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (2.1.3)\n",
      "Collecting databricks-vectorsearch<0.41,>=0.40 (from langchain-databricks)\n",
      "  Downloading databricks_vectorsearch-0.40-py3-none-any.whl.metadata (2.8 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-databricks\n",
      "  Downloading langchain_databricks-0.1.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_databricks-0.1.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-databricks to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n",
      "  Downloading SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
      "  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
      "  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading langchain_community-0.0.18-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading langchain_community-0.0.17-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain_community-0.0.15-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading langchain_community-0.0.14-py3-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.6-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading langchain_community-0.0.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading langchain_community-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading langchain_community-0.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Downloading langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.12.2)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (1.1 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-1.1.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.1.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.0.7-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.0.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Downloading langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Downloading langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Downloading langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Downloading langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.81-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Collecting mlflow>=2.16.0 (from langchain-databricks)\n",
      "  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: scipy>=1.11 in /databricks/python3/lib/python3.12/site-packages (from langchain-databricks) (1.15.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: mlflow-skinny<3,>=2.11.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.22.0)\n",
      "Collecting protobuf<5,>=3.12.0 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl.metadata (541 bytes)\n",
      "Collecting deprecation>=2 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\n",
      "INFO: pip is looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mlflow>=2.16.0 (from langchain-databricks)\n",
      "  Downloading mlflow-3.8.0-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading mlflow-3.7.0-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading mlflow-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading mlflow-3.5.0-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading mlflow-3.3.2-py3-none-any.whl.metadata (30 kB)\n",
      "INFO: pip is still looking at multiple versions of mlflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading mlflow-3.3.1-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading mlflow-3.3.0-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading mlflow-3.1.3-py3-none-any.whl.metadata (29 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading mlflow-3.0.1-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading mlflow-3.0.0-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading mlflow-2.22.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting mlflow-skinny<3,>=2.11.3 (from databricks-vectorsearch<0.41,>=0.40->langchain-databricks)\n",
      "  Downloading mlflow_skinny-2.22.4-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting Flask<4 (from mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.1.5)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (3.10.0)\n",
      "Requirement already satisfied: pandas!=2.3.0,<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<20,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (19.0.1)\n",
      "Requirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=2.16.0->langchain-databricks) (1.6.1)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.0.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.49.0)\n",
      "Requirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.115.12)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.1.43)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (6.6.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.32.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.5.3)\n",
      "Requirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.34.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.1.31)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n",
      "  Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (4.1 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow>=2.16.0->langchain-databricks) (3.0.2)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.16.0->langchain-databricks)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow>=2.16.0->langchain-databricks) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.16.0->langchain-databricks) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas!=2.3.0,<3->mlflow>=2.16.0->langchain-databricks) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.16.0->langchain-databricks) (3.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (2.40.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.46.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.2.13)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.53b1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.16.0->langchain-databricks) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (1.17.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (4.9.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch<0.41,>=0.40->langchain-databricks) (0.4.8)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.81-py3-none-any.whl (457 kB)\n",
      "Downloading langchain_databricks-0.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (11.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/11.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m155.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl (1.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (1.7 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_vectorsearch-0.40-py3-none-any.whl (12 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.5.2-py3-none-any.whl (283 kB)\n",
      "Downloading mlflow-2.22.4-py3-none-any.whl (29.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/29.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m188.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-2.22.4-py3-none-any.whl (6.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m150.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (798 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (3.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (243 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (597 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/597.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m597.3/597.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (258 kB)\n",
      "Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (132 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (225 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_aarch64.whl (294 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (340 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (372 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: werkzeug, uuid-utils, typing-inspection, typing-inspect, requests, regex, python-dotenv, protobuf, propcache, orjson, multidict, marshmallow, markdown, Mako, jsonpatch, itsdangerous, httpx-sse, gunicorn, greenlet, graphql-core, frozenlist, faiss-cpu, deprecation, blinker, aiohappyeyeballs, yarl, tiktoken, SQLAlchemy, requests-toolbelt, graphql-relay, Flask, docker, dataclasses-json, aiosignal, pydantic-settings, langsmith, graphene, alembic, aiohttp, langchain-core, mlflow-skinny, langchain-text-splitters, mlflow, langchain, databricks-vectorsearch, langchain-databricks, langchain-community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Not uninstalling protobuf at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n",
      "    Can't uninstall 'protobuf'. No files were found to uninstall.\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.7.0\n",
      "    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n",
      "    Can't uninstall 'blinker'. No files were found to uninstall.\n",
      "  Attempting uninstall: mlflow-skinny\n",
      "    Found existing installation: mlflow-skinny 2.22.0\n",
      "    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c4b8dbc-4a66-4826-9d56-206110279dcf\n",
      "    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.67.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.1.2 Mako-1.3.10 SQLAlchemy-2.0.45 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 alembic-1.17.2 blinker-1.9.0 databricks-vectorsearch-0.40 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 faiss-cpu-1.13.2 frozenlist-1.8.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.0 gunicorn-23.0.0 httpx-sse-0.4.3 itsdangerous-2.2.0 jsonpatch-1.33 langchain-0.3.27 langchain-community-0.3.31 langchain-core-0.3.81 langchain-databricks-0.1.2 langchain-text-splitters-0.3.11 langsmith-0.5.2 markdown-3.10 marshmallow-3.26.2 mlflow-2.22.4 mlflow-skinny-2.22.4 multidict-6.7.0 orjson-3.11.5 propcache-0.4.1 protobuf-4.25.8 pydantic-settings-2.12.0 python-dotenv-1.2.1 regex-2025.11.3 requests-2.32.5 requests-toolbelt-1.0.0 tiktoken-0.12.0 typing-inspect-0.9.0 typing-inspection-0.4.2 uuid-utils-0.12.0 werkzeug-3.1.4 yarl-1.22.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain langchain-community langchain-databricks faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b641c7-0eb8-4e2f-8d17-20c47d967d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef54e8-4f3d-42ab-b96f-b60ca42326f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b1f725-60ba-4c28-b19e-21107de2b062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-0c4b8dbc-4a66-4826-9d56-20/.ipykernel/3944/command-5829496254952718-1259243950:8: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n",
      "  llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
      "/home/spark-0c4b8dbc-4a66-4826-9d56-20/.ipykernel/3944/command-5829496254952718-1259243950:9: LangChainDeprecationWarning: Use databricks_langchain.DatabricksEmbeddings\n",
      "  embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 1) Databricks LLM + Embeddings\n",
    "# -----------------------\n",
    "# Make sure your Databricks auth is configured (e.g., DATABRICKS_HOST + DATABRICKS_TOKEN)\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"  # <-- change to your embedding endpoint name\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b03608-cbeb-476e-8ee0-28e4db57563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Sentence split + sentence-window splitter (LlamaIndex-like)\n",
    "# -----------------------\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[。！？!?])\\s+|\\n+\")\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def sentence_window_splitter(\n",
    "    documents: List[Document],\n",
    "    window_size: int = 2,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Mimics LlamaIndex SentenceWindowNodeParser:\n",
    "    - Each chunk is one core sentence\n",
    "    - metadata includes:\n",
    "        - original_text: core sentence\n",
    "        - window: context window (core +/- window_size sentences)\n",
    "    \"\"\"\n",
    "    out: List[Document] = []\n",
    "    for doc in documents:\n",
    "        sents = split_sentences(doc.page_content)\n",
    "        for i, core in enumerate(sents):\n",
    "            lo = max(0, i - window_size)\n",
    "            hi = min(len(sents), i + window_size + 1)\n",
    "            window_text = \" \".join(sents[lo:hi])\n",
    "\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=core,\n",
    "                    metadata={\n",
    "                        **(doc.metadata or {}),\n",
    "                        \"original_text\": core,\n",
    "                        \"window\": window_text,\n",
    "                        \"sent_index\": i,\n",
    "                        \"window_size\": window_size,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a46cdd-8ca8-421c-acc2-b26138b51c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Prompt + QA\n",
    "# -----------------------\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful technical assistant. Answer using ONLY the provided context. \"\n",
    "     \"If the context is insufficient, say what is missing.\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nAnswer in English:\")\n",
    "])\n",
    "\n",
    "def answer_with_retrieval(\n",
    "    docs: List[Document],\n",
    "    question: str,\n",
    "    top_k: int = 5,\n",
    "    use_window_metadata: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build FAISS index, retrieve top_k chunks, print retrieved chunks, then ask the Databricks LLM.\n",
    "    If use_window_metadata=True, feed metadata['window'] as context (Sentence Window style).\n",
    "    \"\"\"\n",
    "    vs = FAISS.from_documents(docs, embeddings)\n",
    "    retrieved = vs.similarity_search_with_score(question, k=top_k)\n",
    "\n",
    "    print(\"\\n--- Top retrieved chunks ---\")\n",
    "    context_blocks = []\n",
    "    for rank, (d, score) in enumerate(retrieved, 1):\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            window = d.metadata.get(\"window\", d.page_content)\n",
    "            core = d.metadata.get(\"original_text\", d.page_content)\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(f\"Core: {core}\")\n",
    "            print(f\"Window:\\n{window}\")\n",
    "            context_blocks.append(window)\n",
    "        else:\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(d.page_content)\n",
    "            context_blocks.append(d.page_content)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    msg = QA_PROMPT.format_messages(question=question, context=context)\n",
    "    resp = llm.invoke(msg)\n",
    "\n",
    "    print(\"\\n--- LLM Answer ---\")\n",
    "    print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65d9762-ddc9-48d9-9d54-09f8a1931f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4) Runner (prints raw chunks + retrieval behavior)\n",
    "# -----------------------\n",
    "def evaluate_splitter(\n",
    "    splitter_name: str,\n",
    "    split_fn: Callable[[], List[Document]],\n",
    "    question: str,\n",
    "    use_window_metadata: bool = False,\n",
    "    top_k: int = 5,\n",
    "    max_print_chunks: int = 50,\n",
    ") -> None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing splitter: {splitter_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    chunks = split_fn()\n",
    "\n",
    "    print(f\"\\n[Raw chunks generated] total={len(chunks)}\")\n",
    "    for i, d in enumerate(chunks[:max_print_chunks], 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            print(f\"Core: {d.metadata.get('original_text')}\")\n",
    "            print(f\"Window: {d.metadata.get('window')}\")\n",
    "        else:\n",
    "            print(d.page_content)\n",
    "\n",
    "    if len(chunks) > max_print_chunks:\n",
    "        print(f\"\\n... (only printed first {max_print_chunks} chunks)\")\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer_with_retrieval(\n",
    "        docs=chunks,\n",
    "        question=question,\n",
    "        top_k=top_k,\n",
    "        use_window_metadata=use_window_metadata,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{splitter_name} done.\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1906460-b1a6-4bb1-b7a8-b294fd615fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing splitter: Token Split (chunk_size=30, overlap=0)\n",
      "============================================================\n",
      "\n",
      "[Raw chunks generated] total=20\n",
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
      "LLM-powered applications that combine external knowledge retrieval with\n",
      "\n",
      "\n",
      "--- Chunk 2 ---\n",
      "text generation.\n",
      "Instead of relying solely on a model’s internal parameters, RAG systems\n",
      "retrieve relevant information from external data sources and\n",
      "\n",
      "--- Chunk 3 ---\n",
      " use it as\n",
      "context during answer generation.\n",
      "This approach is widely used for applications such as question answering,\n",
      "knowledge assistants, and domain-specific\n",
      "\n",
      "--- Chunk 4 ---\n",
      " search.\n",
      "\n",
      "A typical RAG pipeline consists of several core components, including\n",
      "document ingestion, text chunking, embedding generation, vector index\n",
      "\n",
      "--- Chunk 5 ---\n",
      "ing,\n",
      "retrieval, and response synthesis.\n",
      "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
      "\n",
      "--- Chunk 6 ---\n",
      "\n",
      "are first ingested and preprocessed.\n",
      "They are then split into smaller units, often referred to as chunks,\n",
      "to make retrieval more precise\n",
      "\n",
      "--- Chunk 7 ---\n",
      " and efficient.\n",
      "\n",
      "Each chunk is converted into a vector representation using an embedding model\n",
      "and stored in a vector index.\n",
      "During inference,\n",
      "\n",
      "--- Chunk 8 ---\n",
      " a user query is embedded and compared against the indexed\n",
      "vectors to retrieve the most relevant chunks.\n",
      "These retrieved chunks are provided to a Large Language\n",
      "\n",
      "--- Chunk 9 ---\n",
      " Model (LLM) as context,\n",
      "allowing the model to generate answers that are grounded in external knowledge.\n",
      "\n",
      "--- The following content is\n",
      "\n",
      "--- Chunk 10 ---\n",
      " less directly related to the RAG topic ---\n",
      "\n",
      "In addition, Python, as a general-purpose programming language, is widely used\n",
      "in the\n",
      "\n",
      "--- Chunk 11 ---\n",
      " AI field due to its simplicity and rich ecosystem.\n",
      "For example, NumPy and Pandas are foundational tools for data processing,\n",
      "providing powerful\n",
      "\n",
      "--- Chunk 12 ---\n",
      " capabilities for numerical computation and structured data.\n",
      "Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
      "for tasks such as classification, regression\n",
      "\n",
      "--- Chunk 13 ---\n",
      ", and clustering.\n",
      "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
      "enabling efficient development and deployment of\n",
      "\n",
      "--- Chunk 14 ---\n",
      " complex AI models.\n",
      "\n",
      "--- The following is another related but conceptually independent section ---\n",
      "\n",
      "Sentence window chunking is an advanced chunking\n",
      "\n",
      "--- Chunk 15 ---\n",
      " strategy in which each chunk\n",
      "contains a target sentence along with a configurable number of surrounding\n",
      "“window” sentences as context.\n",
      "\n",
      "\n",
      "--- Chunk 16 ---\n",
      "This approach aims to provide rich local context to the LLM during retrieval,\n",
      "thereby improving the coherence and factual consistency of generated answers.\n",
      "\n",
      "\n",
      "--- Chunk 17 ---\n",
      "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
      "content rather than relying solely on fixed character counts or sentence boundaries\n",
      "\n",
      "--- Chunk 18 ---\n",
      ".\n",
      "It leverages embedding models to compute semantic similarity between sentences\n",
      "or phrases and identify natural breakpoints where topics or meanings shift.\n",
      "Both\n",
      "\n",
      "--- Chunk 19 ---\n",
      " advanced methods can significantly improve retrieval quality and\n",
      "downstream generation performance in RAG applications.\n",
      "Choosing the right chunking strategy typically depends on the\n",
      "\n",
      "--- Chunk 20 ---\n",
      " characteristics\n",
      "of the data and the expected query types.\n",
      "\n",
      "\n",
      "Question: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n",
      "\n",
      "--- Top retrieved chunks ---\n",
      "\n",
      "[1] score=0.3183\n",
      " advanced methods can significantly improve retrieval quality and\n",
      "downstream generation performance in RAG applications.\n",
      "Choosing the right chunking strategy typically depends on the\n",
      "\n",
      "[2] score=0.3336\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
      "LLM-powered applications that combine external knowledge retrieval with\n",
      "\n",
      "\n",
      "[3] score=0.3830\n",
      " search.\n",
      "\n",
      "A typical RAG pipeline consists of several core components, including\n",
      "document ingestion, text chunking, embedding generation, vector index\n",
      "\n",
      "[4] score=0.4250\n",
      "text generation.\n",
      "Instead of relying solely on a model’s internal parameters, RAG systems\n",
      "retrieve relevant information from external data sources and\n",
      "\n",
      "[5] score=0.4327\n",
      " complex AI models.\n",
      "\n",
      "--- The following is another related but conceptually independent section ---\n",
      "\n",
      "Sentence window chunking is an advanced chunking\n",
      "\n",
      "--- LLM Answer ---\n",
      "The main components of a Retrieval-Augmented Generation (RAG) system are:\n",
      "\n",
      "1. Document ingestion\n",
      "2. Text chunking\n",
      "3. Embedding generation\n",
      "4. Vector index\n",
      "5. Text generation\n",
      "\n",
      "Sentence window chunking and semantic chunking differ in that sentence window chunking is an advanced chunking strategy that is not explicitly described in the provided context, whereas semantic chunking is not mentioned at all. However, based on the general concept of chunking, it can be inferred that semantic chunking likely involves dividing text into meaningful units based on their semantic relationships or meanings.\n",
      "\n",
      "Token Split (chunk_size=30, overlap=0) done.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing splitter: Token Split (chunk_size=30, overlap=10)\n",
      "============================================================\n",
      "\n",
      "[Raw chunks generated] total=29\n",
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
      "LLM-powered applications that combine external knowledge retrieval with\n",
      "\n",
      "\n",
      "--- Chunk 2 ---\n",
      "-powered applications that combine external knowledge retrieval with\n",
      "text generation.\n",
      "Instead of relying solely on a model’s internal parameters, RAG systems\n",
      "\n",
      "--- Chunk 3 ---\n",
      " model’s internal parameters, RAG systems\n",
      "retrieve relevant information from external data sources and use it as\n",
      "context during answer generation.\n",
      "\n",
      "\n",
      "--- Chunk 4 ---\n",
      " use it as\n",
      "context during answer generation.\n",
      "This approach is widely used for applications such as question answering,\n",
      "knowledge assistants, and domain-specific\n",
      "\n",
      "--- Chunk 5 ---\n",
      " answering,\n",
      "knowledge assistants, and domain-specific search.\n",
      "\n",
      "A typical RAG pipeline consists of several core components, including\n",
      "document ingestion,\n",
      "\n",
      "--- Chunk 6 ---\n",
      " of several core components, including\n",
      "document ingestion, text chunking, embedding generation, vector indexing,\n",
      "retrieval, and response synthesis\n",
      "\n",
      "--- Chunk 7 ---\n",
      "ing,\n",
      "retrieval, and response synthesis.\n",
      "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
      "\n",
      "--- Chunk 8 ---\n",
      "s, databases, APIs, or web pages—\n",
      "are first ingested and preprocessed.\n",
      "They are then split into smaller units, often referred\n",
      "\n",
      "--- Chunk 9 ---\n",
      "They are then split into smaller units, often referred to as chunks,\n",
      "to make retrieval more precise and efficient.\n",
      "\n",
      "Each chunk is converted into\n",
      "\n",
      "--- Chunk 10 ---\n",
      " and efficient.\n",
      "\n",
      "Each chunk is converted into a vector representation using an embedding model\n",
      "and stored in a vector index.\n",
      "During inference,\n",
      "\n",
      "--- Chunk 11 ---\n",
      " stored in a vector index.\n",
      "During inference, a user query is embedded and compared against the indexed\n",
      "vectors to retrieve the most relevant chunks.\n",
      "\n",
      "--- Chunk 12 ---\n",
      "\n",
      "vectors to retrieve the most relevant chunks.\n",
      "These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
      "all\n",
      "\n",
      "--- Chunk 13 ---\n",
      " Model (LLM) as context,\n",
      "allowing the model to generate answers that are grounded in external knowledge.\n",
      "\n",
      "--- The following content is\n",
      "\n",
      "--- Chunk 14 ---\n",
      " external knowledge.\n",
      "\n",
      "--- The following content is less directly related to the RAG topic ---\n",
      "\n",
      "In addition, Python, as a general-\n",
      "\n",
      "--- Chunk 15 ---\n",
      "\n",
      "In addition, Python, as a general-purpose programming language, is widely used\n",
      "in the AI field due to its simplicity and rich ecosystem.\n",
      "\n",
      "--- Chunk 16 ---\n",
      " AI field due to its simplicity and rich ecosystem.\n",
      "For example, NumPy and Pandas are foundational tools for data processing,\n",
      "providing powerful\n",
      "\n",
      "--- Chunk 17 ---\n",
      " foundational tools for data processing,\n",
      "providing powerful capabilities for numerical computation and structured data.\n",
      "Scikit-learn offers a comprehensive suite of machine\n",
      "\n",
      "--- Chunk 18 ---\n",
      "cikit-learn offers a comprehensive suite of machine learning algorithms\n",
      "for tasks such as classification, regression, and clustering.\n",
      "Together, these tools\n",
      "\n",
      "--- Chunk 19 ---\n",
      ", and clustering.\n",
      "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
      "enabling efficient development and deployment of\n",
      "\n",
      "--- Chunk 20 ---\n",
      " practitioners,\n",
      "enabling efficient development and deployment of complex AI models.\n",
      "\n",
      "--- The following is another related but conceptually independent section ---\n",
      "\n",
      "\n",
      "\n",
      "--- Chunk 21 ---\n",
      " another related but conceptually independent section ---\n",
      "\n",
      "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
      "contains a target\n",
      "\n",
      "--- Chunk 22 ---\n",
      " strategy in which each chunk\n",
      "contains a target sentence along with a configurable number of surrounding\n",
      "“window” sentences as context.\n",
      "\n",
      "\n",
      "--- Chunk 23 ---\n",
      "“window” sentences as context.\n",
      "This approach aims to provide rich local context to the LLM during retrieval,\n",
      "thereby improving the\n",
      "\n",
      "--- Chunk 24 ---\n",
      " LLM during retrieval,\n",
      "thereby improving the coherence and factual consistency of generated answers.\n",
      "Semantic chunking, on the other hand,\n",
      "\n",
      "--- Chunk 25 ---\n",
      "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
      "content rather than relying solely on fixed character counts or sentence boundaries\n",
      "\n",
      "--- Chunk 26 ---\n",
      " than relying solely on fixed character counts or sentence boundaries.\n",
      "It leverages embedding models to compute semantic similarity between sentences\n",
      "or phrases and identify natural\n",
      "\n",
      "--- Chunk 27 ---\n",
      " semantic similarity between sentences\n",
      "or phrases and identify natural breakpoints where topics or meanings shift.\n",
      "Both advanced methods can significantly improve retrieval quality and\n",
      "down\n",
      "\n",
      "--- Chunk 28 ---\n",
      " advanced methods can significantly improve retrieval quality and\n",
      "downstream generation performance in RAG applications.\n",
      "Choosing the right chunking strategy typically depends on the\n",
      "\n",
      "--- Chunk 29 ---\n",
      "osing the right chunking strategy typically depends on the characteristics\n",
      "of the data and the expected query types.\n",
      "\n",
      "\n",
      "Question: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n",
      "\n",
      "--- Top retrieved chunks ---\n",
      "\n",
      "[1] score=0.3183\n",
      " advanced methods can significantly improve retrieval quality and\n",
      "downstream generation performance in RAG applications.\n",
      "Choosing the right chunking strategy typically depends on the\n",
      "\n",
      "[2] score=0.3336\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
      "LLM-powered applications that combine external knowledge retrieval with\n",
      "\n",
      "\n",
      "[3] score=0.3912\n",
      " LLM during retrieval,\n",
      "thereby improving the coherence and factual consistency of generated answers.\n",
      "Semantic chunking, on the other hand,\n",
      "\n",
      "[4] score=0.4275\n",
      "-powered applications that combine external knowledge retrieval with\n",
      "text generation.\n",
      "Instead of relying solely on a model’s internal parameters, RAG systems\n",
      "\n",
      "[5] score=0.4342\n",
      " another related but conceptually independent section ---\n",
      "\n",
      "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
      "contains a target\n",
      "\n",
      "--- LLM Answer ---\n",
      "The main components of a Retrieval-Augmented Generation (RAG) system are:\n",
      "\n",
      "1. External knowledge retrieval\n",
      "2. Language Model (LLM)\n",
      "3. Chunking strategy (which can be either sentence window chunking or semantic chunking)\n",
      "\n",
      "Sentence window chunking and semantic chunking differ in that:\n",
      "\n",
      "* Sentence window chunking involves dividing the input text into chunks, where each chunk contains a target sentence and its surrounding context.\n",
      "* Semantic chunking, on the other hand, involves dividing the input text into chunks based on semantic meaning, rather than just syntactic structure.\n",
      "\n",
      "Token Split (chunk_size=30, overlap=10) done.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing splitter: Sentence Window Split (window_size=2)\n",
      "============================================================\n",
      "\n",
      "[Raw chunks generated] total=44\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Core: Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
      "Window: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Core: LLM-powered applications that combine external knowledge retrieval with\n",
      "Window: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Core: text generation.\n",
      "Window: Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Core: Instead of relying solely on a model’s internal parameters, RAG systems\n",
      "Window: LLM-powered applications that combine external knowledge retrieval with text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation.\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Core: retrieve relevant information from external data sources and use it as\n",
      "Window: text generation. Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering,\n",
      "\n",
      "--- Chunk 6 ---\n",
      "Core: context during answer generation.\n",
      "Window: Instead of relying solely on a model’s internal parameters, RAG systems retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search.\n",
      "\n",
      "--- Chunk 7 ---\n",
      "Core: This approach is widely used for applications such as question answering,\n",
      "Window: retrieve relevant information from external data sources and use it as context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including\n",
      "\n",
      "--- Chunk 8 ---\n",
      "Core: knowledge assistants, and domain-specific search.\n",
      "Window: context during answer generation. This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing,\n",
      "\n",
      "--- Chunk 9 ---\n",
      "Core: A typical RAG pipeline consists of several core components, including\n",
      "Window: This approach is widely used for applications such as question answering, knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis.\n",
      "\n",
      "--- Chunk 10 ---\n",
      "Core: document ingestion, text chunking, embedding generation, vector indexing,\n",
      "Window: knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
      "\n",
      "--- Chunk 11 ---\n",
      "Core: retrieval, and response synthesis.\n",
      "Window: A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed.\n",
      "\n",
      "--- Chunk 12 ---\n",
      "Core: Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
      "Window: document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks,\n",
      "\n",
      "--- Chunk 13 ---\n",
      "Core: are first ingested and preprocessed.\n",
      "Window: retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient.\n",
      "\n",
      "--- Chunk 14 ---\n",
      "Core: They are then split into smaller units, often referred to as chunks,\n",
      "Window: Documents from various sources—such as PDFs, databases, APIs, or web pages— are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model\n",
      "\n",
      "--- Chunk 15 ---\n",
      "Core: to make retrieval more precise and efficient.\n",
      "Window: are first ingested and preprocessed. They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index.\n",
      "\n",
      "--- Chunk 16 ---\n",
      "Core: Each chunk is converted into a vector representation using an embedding model\n",
      "Window: They are then split into smaller units, often referred to as chunks, to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed\n",
      "\n",
      "--- Chunk 17 ---\n",
      "Core: and stored in a vector index.\n",
      "Window: to make retrieval more precise and efficient. Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks.\n",
      "\n",
      "--- Chunk 18 ---\n",
      "Core: During inference, a user query is embedded and compared against the indexed\n",
      "Window: Each chunk is converted into a vector representation using an embedding model and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
      "\n",
      "--- Chunk 19 ---\n",
      "Core: vectors to retrieve the most relevant chunks.\n",
      "Window: and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n",
      "\n",
      "--- Chunk 20 ---\n",
      "Core: These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
      "Window: During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic ---\n",
      "\n",
      "--- Chunk 21 ---\n",
      "Core: allowing the model to generate answers that are grounded in external knowledge.\n",
      "Window: vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used\n",
      "\n",
      "--- Chunk 22 ---\n",
      "Core: --- The following content is less directly related to the RAG topic ---\n",
      "Window: These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem.\n",
      "\n",
      "--- Chunk 23 ---\n",
      "Core: In addition, Python, as a general-purpose programming language, is widely used\n",
      "Window: allowing the model to generate answers that are grounded in external knowledge. --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing,\n",
      "\n",
      "--- Chunk 24 ---\n",
      "Core: in the AI field due to its simplicity and rich ecosystem.\n",
      "Window: --- The following content is less directly related to the RAG topic --- In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data.\n",
      "\n",
      "--- Chunk 25 ---\n",
      "Core: For example, NumPy and Pandas are foundational tools for data processing,\n",
      "Window: In addition, Python, as a general-purpose programming language, is widely used in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
      "\n",
      "--- Chunk 26 ---\n",
      "Core: providing powerful capabilities for numerical computation and structured data.\n",
      "Window: in the AI field due to its simplicity and rich ecosystem. For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering.\n",
      "\n",
      "--- Chunk 27 ---\n",
      "Core: Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
      "Window: For example, NumPy and Pandas are foundational tools for data processing, providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
      "\n",
      "--- Chunk 28 ---\n",
      "Core: for tasks such as classification, regression, and clustering.\n",
      "Window: providing powerful capabilities for numerical computation and structured data. Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models.\n",
      "\n",
      "--- Chunk 29 ---\n",
      "Core: Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
      "Window: Scikit-learn offers a comprehensive suite of machine learning algorithms for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section ---\n",
      "\n",
      "--- Chunk 30 ---\n",
      "Core: enabling efficient development and deployment of complex AI models.\n",
      "Window: for tasks such as classification, regression, and clustering. Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk\n",
      "\n",
      "--- Chunk 31 ---\n",
      "Core: --- The following is another related but conceptually independent section ---\n",
      "Window: Together, these tools form a powerful toolbox for data scientists and AI practitioners, enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding\n",
      "\n",
      "--- Chunk 32 ---\n",
      "Core: Sentence window chunking is an advanced chunking strategy in which each chunk\n",
      "Window: enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n",
      "\n",
      "--- Chunk 33 ---\n",
      "Core: contains a target sentence along with a configurable number of surrounding\n",
      "Window: --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval,\n",
      "\n",
      "--- Chunk 34 ---\n",
      "Core: “window” sentences as context.\n",
      "Window: Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers.\n",
      "\n",
      "--- Chunk 35 ---\n",
      "Core: This approach aims to provide rich local context to the LLM during retrieval,\n",
      "Window: contains a target sentence along with a configurable number of surrounding “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic\n",
      "\n",
      "--- Chunk 36 ---\n",
      "Core: thereby improving the coherence and factual consistency of generated answers.\n",
      "Window: “window” sentences as context. This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries.\n",
      "\n",
      "--- Chunk 37 ---\n",
      "Core: Semantic chunking, on the other hand, attempts to split text based on semantic\n",
      "Window: This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n",
      "\n",
      "--- Chunk 38 ---\n",
      "Core: content rather than relying solely on fixed character counts or sentence boundaries.\n",
      "Window: thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift.\n",
      "\n",
      "--- Chunk 39 ---\n",
      "Core: It leverages embedding models to compute semantic similarity between sentences\n",
      "Window: Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and\n",
      "\n",
      "--- Chunk 40 ---\n",
      "Core: or phrases and identify natural breakpoints where topics or meanings shift.\n",
      "Window: content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications.\n",
      "\n",
      "--- Chunk 41 ---\n",
      "Core: Both advanced methods can significantly improve retrieval quality and\n",
      "Window: It leverages embedding models to compute semantic similarity between sentences or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics\n",
      "\n",
      "--- Chunk 42 ---\n",
      "Core: downstream generation performance in RAG applications.\n",
      "Window: or phrases and identify natural breakpoints where topics or meanings shift. Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n",
      "\n",
      "--- Chunk 43 ---\n",
      "Core: Choosing the right chunking strategy typically depends on the characteristics\n",
      "Window: Both advanced methods can significantly improve retrieval quality and downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n",
      "\n",
      "--- Chunk 44 ---\n",
      "Core: of the data and the expected query types.\n",
      "Window: downstream generation performance in RAG applications. Choosing the right chunking strategy typically depends on the characteristics of the data and the expected query types.\n",
      "\n",
      "Question: What are the main components of a Retrieval-Augmented Generation (RAG) system, and how do sentence window chunking and semantic chunking differ?\n",
      "\n",
      "--- Top retrieved chunks ---\n",
      "\n",
      "[1] score=0.3029\n",
      "Core: Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
      "Window:\n",
      "Retrieval-Augmented Generation (RAG) is a common architecture for building LLM-powered applications that combine external knowledge retrieval with text generation.\n",
      "\n",
      "[2] score=0.3936\n",
      "Core: Sentence window chunking is an advanced chunking strategy in which each chunk\n",
      "Window:\n",
      "enabling efficient development and deployment of complex AI models. --- The following is another related but conceptually independent section --- Sentence window chunking is an advanced chunking strategy in which each chunk contains a target sentence along with a configurable number of surrounding “window” sentences as context.\n",
      "\n",
      "[3] score=0.4610\n",
      "Core: Semantic chunking, on the other hand, attempts to split text based on semantic\n",
      "Window:\n",
      "This approach aims to provide rich local context to the LLM during retrieval, thereby improving the coherence and factual consistency of generated answers. Semantic chunking, on the other hand, attempts to split text based on semantic content rather than relying solely on fixed character counts or sentence boundaries. It leverages embedding models to compute semantic similarity between sentences\n",
      "\n",
      "[4] score=0.4851\n",
      "Core: document ingestion, text chunking, embedding generation, vector indexing,\n",
      "Window:\n",
      "knowledge assistants, and domain-specific search. A typical RAG pipeline consists of several core components, including document ingestion, text chunking, embedding generation, vector indexing, retrieval, and response synthesis. Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
      "\n",
      "[5] score=0.5718\n",
      "Core: vectors to retrieve the most relevant chunks.\n",
      "Window:\n",
      "and stored in a vector index. During inference, a user query is embedded and compared against the indexed vectors to retrieve the most relevant chunks. These retrieved chunks are provided to a Large Language Model (LLM) as context, allowing the model to generate answers that are grounded in external knowledge.\n",
      "\n",
      "--- LLM Answer ---\n",
      "The main components of a Retrieval-Augmented Generation (RAG) system are:\n",
      "\n",
      "1. Document ingestion: This involves collecting and processing documents from various sources.\n",
      "2. Text chunking: This is the process of splitting text into smaller chunks, which can be done using different strategies such as sentence window chunking or semantic chunking.\n",
      "3. Embedding generation: This involves converting the text chunks into numerical vectors that can be used for comparison.\n",
      "4. Vector indexing: This involves storing the embedded vectors in an index for efficient retrieval.\n",
      "5. Retrieval: This involves comparing the user query with the indexed vectors to retrieve the most relevant chunks.\n",
      "6. Response synthesis: This involves using the retrieved chunks as context to generate answers with a Large Language Model (LLM).\n",
      "\n",
      "Sentence window chunking and semantic chunking differ in their approach to splitting text into chunks. Sentence window chunking involves splitting text into chunks that contain a target sentence along with a configurable number of surrounding sentences as context. Semantic chunking, on the other hand, involves splitting text based on semantic content, using embedding models to compute semantic similarity between sentences.\n",
      "\n",
      "Sentence Window Split (window_size=2) done.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 5) Example document + question (English version)\n",
    "# -----------------------\n",
    "documents = [\n",
    "    Document(page_content=\"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
    "LLM-powered applications that combine external knowledge retrieval with\n",
    "text generation.\n",
    "Instead of relying solely on a model’s internal parameters, RAG systems\n",
    "retrieve relevant information from external data sources and use it as\n",
    "context during answer generation.\n",
    "This approach is widely used for applications such as question answering,\n",
    "knowledge assistants, and domain-specific search.\n",
    "\n",
    "A typical RAG pipeline consists of several core components, including\n",
    "document ingestion, text chunking, embedding generation, vector indexing,\n",
    "retrieval, and response synthesis.\n",
    "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
    "are first ingested and preprocessed.\n",
    "They are then split into smaller units, often referred to as chunks,\n",
    "to make retrieval more precise and efficient.\n",
    "\n",
    "Each chunk is converted into a vector representation using an embedding model\n",
    "and stored in a vector index.\n",
    "During inference, a user query is embedded and compared against the indexed\n",
    "vectors to retrieve the most relevant chunks.\n",
    "These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
    "allowing the model to generate answers that are grounded in external knowledge.\n",
    "\n",
    "--- The following content is less directly related to the RAG topic ---\n",
    "\n",
    "In addition, Python, as a general-purpose programming language, is widely used\n",
    "in the AI field due to its simplicity and rich ecosystem.\n",
    "For example, NumPy and Pandas are foundational tools for data processing,\n",
    "providing powerful capabilities for numerical computation and structured data.\n",
    "Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
    "for tasks such as classification, regression, and clustering.\n",
    "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
    "enabling efficient development and deployment of complex AI models.\n",
    "\n",
    "--- The following is another related but conceptually independent section ---\n",
    "\n",
    "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
    "contains a target sentence along with a configurable number of surrounding\n",
    "“window” sentences as context.\n",
    "This approach aims to provide rich local context to the LLM during retrieval,\n",
    "thereby improving the coherence and factual consistency of generated answers.\n",
    "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
    "content rather than relying solely on fixed character counts or sentence boundaries.\n",
    "It leverages embedding models to compute semantic similarity between sentences\n",
    "or phrases and identify natural breakpoints where topics or meanings shift.\n",
    "Both advanced methods can significantly improve retrieval quality and\n",
    "downstream generation performance in RAG applications.\n",
    "Choosing the right chunking strategy typically depends on the characteristics\n",
    "of the data and the expected query types.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "question = (\n",
    "    \"What are the main components of a Retrieval-Augmented Generation (RAG) system, \"\n",
    "    \"and how do sentence window chunking and semantic chunking differ?\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Run splitters\n",
    "# -----------------------\n",
    "# Token-based split (chunk_size=30, overlap=0)\n",
    "splitter_a = TokenTextSplitter(chunk_size=30, chunk_overlap=0)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=0)\",\n",
    "    split_fn=lambda: splitter_a.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Token-based split (chunk_size=30, overlap=10)\n",
    "splitter_b = TokenTextSplitter(chunk_size=30, chunk_overlap=10)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=10)\",\n",
    "    split_fn=lambda: splitter_b.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Sentence-window split (window_size=2)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Sentence Window Split (window_size=2)\",\n",
    "    split_fn=lambda: sentence_window_splitter(documents, window_size=2),\n",
    "    question=question,\n",
    "    use_window_metadata=True,   # feed metadata['window'] into LLM context\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d162956-a99a-4eb3-abcb-8ebe80c177f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Tokens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
