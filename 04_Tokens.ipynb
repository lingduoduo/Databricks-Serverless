{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8901e340-f23c-4263-8dcc-8e887f407be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain langchain-community langchain-databricks faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b641c7-0eb8-4e2f-8d17-20c47d967d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acef54e8-4f3d-42ab-b96f-b60ca42326f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Callable\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_databricks import ChatDatabricks, DatabricksEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b1f725-60ba-4c28-b19e-21107de2b062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1) Databricks LLM + Embeddings\n",
    "# -----------------------\n",
    "# Make sure your Databricks auth is configured (e.g., DATABRICKS_HOST + DATABRICKS_TOKEN)\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "EMBEDDING_ENDPOINT_NAME = \"databricks-bge-large-en\"  # <-- change to your embedding endpoint name\n",
    "\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=0.1)\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b03608-cbeb-476e-8ee0-28e4db57563d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Sentence split + sentence-window splitter (LlamaIndex-like)\n",
    "# -----------------------\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[。！？!?])\\s+|\\n+\")\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def sentence_window_splitter(\n",
    "    documents: List[Document],\n",
    "    window_size: int = 2,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Mimics LlamaIndex SentenceWindowNodeParser:\n",
    "    - Each chunk is one core sentence\n",
    "    - metadata includes:\n",
    "        - original_text: core sentence\n",
    "        - window: context window (core +/- window_size sentences)\n",
    "    \"\"\"\n",
    "    out: List[Document] = []\n",
    "    for doc in documents:\n",
    "        sents = split_sentences(doc.page_content)\n",
    "        for i, core in enumerate(sents):\n",
    "            lo = max(0, i - window_size)\n",
    "            hi = min(len(sents), i + window_size + 1)\n",
    "            window_text = \" \".join(sents[lo:hi])\n",
    "\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=core,\n",
    "                    metadata={\n",
    "                        **(doc.metadata or {}),\n",
    "                        \"original_text\": core,\n",
    "                        \"window\": window_text,\n",
    "                        \"sent_index\": i,\n",
    "                        \"window_size\": window_size,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a46cdd-8ca8-421c-acc2-b26138b51c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3) Prompt + QA\n",
    "# -----------------------\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful technical assistant. Answer using ONLY the provided context. \"\n",
    "     \"If the context is insufficient, say what is missing.\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nAnswer in English:\")\n",
    "])\n",
    "\n",
    "def answer_with_retrieval(\n",
    "    docs: List[Document],\n",
    "    question: str,\n",
    "    top_k: int = 5,\n",
    "    use_window_metadata: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build FAISS index, retrieve top_k chunks, print retrieved chunks, then ask the Databricks LLM.\n",
    "    If use_window_metadata=True, feed metadata['window'] as context (Sentence Window style).\n",
    "    \"\"\"\n",
    "    vs = FAISS.from_documents(docs, embeddings)\n",
    "    retrieved = vs.similarity_search_with_score(question, k=top_k)\n",
    "\n",
    "    print(\"\\n--- Top retrieved chunks ---\")\n",
    "    context_blocks = []\n",
    "    for rank, (d, score) in enumerate(retrieved, 1):\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            window = d.metadata.get(\"window\", d.page_content)\n",
    "            core = d.metadata.get(\"original_text\", d.page_content)\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(f\"Core: {core}\")\n",
    "            print(f\"Window:\\n{window}\")\n",
    "            context_blocks.append(window)\n",
    "        else:\n",
    "            print(f\"\\n[{rank}] score={score:.4f}\")\n",
    "            print(d.page_content)\n",
    "            context_blocks.append(d.page_content)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    msg = QA_PROMPT.format_messages(question=question, context=context)\n",
    "    resp = llm.invoke(msg)\n",
    "\n",
    "    print(\"\\n--- LLM Answer ---\")\n",
    "    print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65d9762-ddc9-48d9-9d54-09f8a1931f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4) Runner (prints raw chunks + retrieval behavior)\n",
    "# -----------------------\n",
    "def evaluate_splitter(\n",
    "    splitter_name: str,\n",
    "    split_fn: Callable[[], List[Document]],\n",
    "    question: str,\n",
    "    use_window_metadata: bool = False,\n",
    "    top_k: int = 5,\n",
    "    max_print_chunks: int = 50,\n",
    ") -> None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing splitter: {splitter_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    chunks = split_fn()\n",
    "\n",
    "    print(f\"\\n[Raw chunks generated] total={len(chunks)}\")\n",
    "    for i, d in enumerate(chunks[:max_print_chunks], 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        if use_window_metadata and \"window\" in (d.metadata or {}):\n",
    "            print(f\"Core: {d.metadata.get('original_text')}\")\n",
    "            print(f\"Window: {d.metadata.get('window')}\")\n",
    "        else:\n",
    "            print(d.page_content)\n",
    "\n",
    "    if len(chunks) > max_print_chunks:\n",
    "        print(f\"\\n... (only printed first {max_print_chunks} chunks)\")\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer_with_retrieval(\n",
    "        docs=chunks,\n",
    "        question=question,\n",
    "        top_k=top_k,\n",
    "        use_window_metadata=use_window_metadata,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{splitter_name} done.\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1906460-b1a6-4bb1-b7a8-b294fd615fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 5) Example document + question (English version)\n",
    "# -----------------------\n",
    "documents = [\n",
    "    Document(page_content=\"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is a common architecture for building\n",
    "LLM-powered applications that combine external knowledge retrieval with\n",
    "text generation.\n",
    "Instead of relying solely on a model’s internal parameters, RAG systems\n",
    "retrieve relevant information from external data sources and use it as\n",
    "context during answer generation.\n",
    "This approach is widely used for applications such as question answering,\n",
    "knowledge assistants, and domain-specific search.\n",
    "\n",
    "A typical RAG pipeline consists of several core components, including\n",
    "document ingestion, text chunking, embedding generation, vector indexing,\n",
    "retrieval, and response synthesis.\n",
    "Documents from various sources—such as PDFs, databases, APIs, or web pages—\n",
    "are first ingested and preprocessed.\n",
    "They are then split into smaller units, often referred to as chunks,\n",
    "to make retrieval more precise and efficient.\n",
    "\n",
    "Each chunk is converted into a vector representation using an embedding model\n",
    "and stored in a vector index.\n",
    "During inference, a user query is embedded and compared against the indexed\n",
    "vectors to retrieve the most relevant chunks.\n",
    "These retrieved chunks are provided to a Large Language Model (LLM) as context,\n",
    "allowing the model to generate answers that are grounded in external knowledge.\n",
    "\n",
    "--- The following content is less directly related to the RAG topic ---\n",
    "\n",
    "In addition, Python, as a general-purpose programming language, is widely used\n",
    "in the AI field due to its simplicity and rich ecosystem.\n",
    "For example, NumPy and Pandas are foundational tools for data processing,\n",
    "providing powerful capabilities for numerical computation and structured data.\n",
    "Scikit-learn offers a comprehensive suite of machine learning algorithms\n",
    "for tasks such as classification, regression, and clustering.\n",
    "Together, these tools form a powerful toolbox for data scientists and AI practitioners,\n",
    "enabling efficient development and deployment of complex AI models.\n",
    "\n",
    "--- The following is another related but conceptually independent section ---\n",
    "\n",
    "Sentence window chunking is an advanced chunking strategy in which each chunk\n",
    "contains a target sentence along with a configurable number of surrounding\n",
    "“window” sentences as context.\n",
    "This approach aims to provide rich local context to the LLM during retrieval,\n",
    "thereby improving the coherence and factual consistency of generated answers.\n",
    "Semantic chunking, on the other hand, attempts to split text based on semantic\n",
    "content rather than relying solely on fixed character counts or sentence boundaries.\n",
    "It leverages embedding models to compute semantic similarity between sentences\n",
    "or phrases and identify natural breakpoints where topics or meanings shift.\n",
    "Both advanced methods can significantly improve retrieval quality and\n",
    "downstream generation performance in RAG applications.\n",
    "Choosing the right chunking strategy typically depends on the characteristics\n",
    "of the data and the expected query types.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "question = (\n",
    "    \"What are the main components of a Retrieval-Augmented Generation (RAG) system, \"\n",
    "    \"and how do sentence window chunking and semantic chunking differ?\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Run splitters\n",
    "# -----------------------\n",
    "# Token-based split (chunk_size=30, overlap=0)\n",
    "splitter_a = TokenTextSplitter(chunk_size=30, chunk_overlap=0)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=0)\",\n",
    "    split_fn=lambda: splitter_a.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Token-based split (chunk_size=30, overlap=10)\n",
    "splitter_b = TokenTextSplitter(chunk_size=30, chunk_overlap=10)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Token Split (chunk_size=30, overlap=10)\",\n",
    "    split_fn=lambda: splitter_b.split_documents(documents),\n",
    "    question=question,\n",
    "    use_window_metadata=False,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Sentence-window split (window_size=2)\n",
    "evaluate_splitter(\n",
    "    splitter_name=\"Sentence Window Split (window_size=2)\",\n",
    "    split_fn=lambda: sentence_window_splitter(documents, window_size=2),\n",
    "    question=question,\n",
    "    use_window_metadata=True,   # feed metadata['window'] into LLM context\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981572fb-afc9-4af8-891d-4fcc08ff1c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate Token Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9013498-c13b-4207-b3a0-6aada21b7290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "\n",
    "def run_rag_pipeline(\n",
    "    splitter,\n",
    "    documents: List[Document],\n",
    "    question: str,\n",
    "    splitter_name: str,\n",
    "):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Running RAG pipeline with splitter: {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 1: Split documents\n",
    "    # -----------------------\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"[{splitter_name}] Generated document chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(chunk.page_content.strip())\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 2: Build vector store\n",
    "    # -----------------------\n",
    "    print(\"\\nBuilding vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 3: Build RAG chain\n",
    "    # -----------------------\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 4: Ask question\n",
    "    # -----------------------\n",
    "    print(f\"\\nQuestion:\\n{question}\\n\")\n",
    "    print(\"Model Answer:\\n\")\n",
    "\n",
    "    result = qa_chain(question)\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # -----------------------\n",
    "    # Step 5: Show retrieved context\n",
    "    # -----------------------\n",
    "    print(f\"\\n[{splitter_name}] Retrieved source documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        print(f\"\\n--- Source Document {i} ---\")\n",
    "        print(doc.page_content.strip())\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinished RAG pipeline with {splitter_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "\n",
    "run_rag_pipeline(\n",
    "    splitter=sentence_splitter,\n",
    "    documents=documents,\n",
    "    question=question,\n",
    "    splitter_name=\"RecursiveCharacterTextSplitter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d162956-a99a-4eb3-abcb-8ebe80c177f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Tokens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
